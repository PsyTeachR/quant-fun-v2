[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"Book Name: Fundamentals Quantitative AnalysisSummary: Materials University Glasgow School Psychology & Neuroscience Research Methods modules MSc students.Authors: Emily Nordmann & Phil McAleerAim: course covers data skills R Markdown, data wrangling tidyverse, data visualisation ggplot2. also introduces statistical concepts Null Hypothesis Significance Testing (NHST), alpha, power, effect size, sample size, well demonstrating perform numerous analyses based around general linear model including correlations, t-tests, ANOVAs Regression.Contact: book living document regularly checked updated improvements. issues using book queries, please contact Phil McAleer.R Version: book written R version 4.1.1 (2021-08-10)","code":""},{"path":"foreword.html","id":"foreword","chapter":"Foreword","heading":"Foreword","text":"","code":""},{"path":"foreword.html","id":"welcome-to-the-fundamentals-of-quantitative-analysis","chapter":"Foreword","heading":"Welcome to the Fundamentals of Quantitative Analysis","text":"book written designed help learn core data skills R RStudio. skills lead able manipulate analyse quantitative data - key component accredited Psychology programme. addition book team support helpful skills demonstration videos, encourage use Teams ask questions .ability work quantitative data key skill Psychologists using R tool, working data, can also promote reproducible research practices. Although first may seem like writing programming script time-consuming point--click approaches, case! written script need , can easily re-run analysis without go step manually ) easier b) less likely result errors something slightly different forget one steps.Crucially, analysis script researchers can also see got raw data statistics report final paper. Sharing analysis scripts online sites Open Science Framework now seen important open science practice. Even continue quantitative research, future, skills develop course allow evaluate quantitative research understand goes behind scenes data conclusions presented, allowing become much confident competent consumers users research.","code":""},{"path":"foreword.html","id":"how-to-use-this-book-and-the-accompanying-videos","chapter":"Foreword","heading":"How to use this book and the accompanying videos","text":"Within book , many initial chapters, provide code need use. always strongly encourage type code , good practice learning code, remember can copy paste book need . Typing code seem much slower first make errors, lots , learn much quickly way try write code can.also provide solutions many activities. -one going check whether tried figured activity rather going straight solution remember , copy paste without thinking, learn nothing. Learning data skills knowledge underpins skills much like learning language - practice use , better become.Additionally, number chapters book associated video videos. videos support get comfortable data skills. However, important use wisely. always try work chapter book (prefer activity) first, watch video get stuck, extra information.Finally, book living document. means occasion make updates book fixing typos including additional detail activities. substantial changes made, create new support materials accompanying video. However, impossible record new video every time make minor change activity, therefore, sometimes may slight differences videos content book. differences book video, book always considered definitive version.","code":""},{"path":"foreword.html","id":"intended-learning-outcomes","chapter":"Foreword","heading":"Intended Learning Outcomes","text":"end course students able :Clean wrangle data appropriate forms analysisVisualise data using range plotsConduct interpret core set statistical tests (t-test, correlation, ANOVA, regression)","code":""},{"path":"programming-basics.html","id":"programming-basics","chapter":"1 Programming basics","heading":"1 Programming basics","text":"chapter, cover use R RStudio University Glasgow, well basic programming concepts terminology, common pitfalls, helpful hints, get help. programming experience find chapter particularly helpful, however, even used R , experience programming, may helpful hints tips please make sure read chapter moving .long chapter expect memorise information contained sections make sense start writing code - just make sure know help available!","code":""},{"path":"programming-basics.html","id":"r-and-rstudio","chapter":"1 Programming basics","heading":"1.1 R and RStudio","text":"short, R programming language write code RStudio Integrated Development Environment (IDE) makes working R easier. Think knowing English using plain text editor like NotePad write book versus using word processor like Microsoft Word. , look good much harder without things like spell-checking formatting. similar way, can use R without RStudio recommend . key thing remember although work using RStudio course, actually using two pieces software means need , need keep --date, cite work (see Appendix citing R RStudio needed).first need look starting R RStudio. two ways can use R Psychology student University Glasgow. First, can use online version R R web browser refer R server. Second, can download install R RStudio free laptop desktop computer.version R use?advantage using R server need install anything machine, simply access web browser. server already installed software need course. recommend using server computer install R (e.g., Chromebook), problems installing R computer.advantage installing R computer (call local installation) need connected internet use , easier save manage files, problems caused server stops working (rare short occurrence, happen time time).exception saving files, approaches largely identical work, however, experience tells us can install R computer, . computer install R, technical problems installation resolve, use server. short highly recommend installing R RStudio machine soon can. However, install reason, want use R server get running first couple chapters, whilst install R time, also OK!","code":""},{"path":"programming-basics.html","id":"installing-r-and-rstudio-on-your-computer","chapter":"1 Programming basics","heading":"1.1.1 Installing R and RStudio on your computer","text":"use R RStudio computer, please see Installing R links detailed instructions links files need download, well links series walkthroughs installing R different types computers. Note need install R, RStudio, likely RTools. free software take lot space computer. video Danielle Navarro (choose one machine type) also highly recommended","code":""},{"path":"programming-basics.html","id":"r-server","chapter":"1 Programming basics","heading":"1.1.2 R server","text":"prefer use R server now, find link R server Moodle. Please note R server use students staff School Psychology Neuroscience, University Glasgow, need GUID password login.","code":""},{"path":"programming-basics.html","id":"getting-to-know-r-studio","chapter":"1 Programming basics","heading":"1.2 Getting to know R Studio","text":"Whether server machine, first thing get orientated RStudio. know RStudio number windows. RStudio console window can try code (appearing bottom left window Figure 1.1), may script editor (top left - show first open RStudio), window showing functions objects created Environment tab (top right window figure), window shows plots, files packages, help documentation (bottom right).\nFigure 1.1: RStudio interface\nlearn use features included RStudio throughout course, however, highly recommend point watching RStudio Essentials 1 RStudio team. video lasts ~30 minutes gives tour main parts RStudio.","code":""},{"path":"programming-basics.html","id":"console-vs.-scripts","chapter":"1 Programming basics","heading":"1.2.1 Console vs. scripts","text":"first open RStudio see script like , just single pane left, console window. can write code console window test , key thing note save code anywhere - shut RStudio lose code. chapter , use console window show simple code examples moving forward tell work script files called R Markdown files. look later chapters can open new script file number ways, simplest top menu RStudio, selecting File >> New File >> R Script, see extra pane appear.","code":""},{"path":"programming-basics.html","id":"writing-code-with-functions-and-arguments","chapter":"1 Programming basics","heading":"1.3 Writing code with functions and arguments","text":"Code R made functions, arguments go functions, create outputs. Functions R execute specific tasks normally take number arguments. linguistics might want think verbs (function) require subject object (arguments). Another analogy toaster - function toaster type bread setting arguments, output toast. key thing spot functions end brackets parentheses, arguments go within parenthesis. tend look bit like :layout function two arguments argument takes value. make sense read .learn use lot functions throughout book can look arguments function takes help documentation using format ?function. see, functions, arguments required, optional. Optional arguments often use known default setting, value, option (normally specified help documentation) enter value. , like always leaving toaster setting - toaster task time without changing .example, let’s look help documentation function rnorm() - function randomly generates set numbers known Normal Distribution.","code":"\nfunction_name(argument1 = value, argument2 = value)"},{"path":"programming-basics.html","id":"activity-1","chapter":"1 Programming basics","heading":"1.3.1 Activity 1","text":"Open RStudio (either server machine) console window, type following code:help documentation rnorm() appear bottom right help panel. Usage section help, see rnorm() takes following form:Arguments section help, explanations arguments:n number observations/numbers/data points want create,mean mean observations/numbers/data points create.sd standard deviation observations/numbers/data points.Details section help notes values entered mean sd use default 0 mean 1 standard deviation. values function use arguments mean sd state . However, default value n, means must state value arguments n, otherwise code run.might sound little bit confusing try example. Still using rnorm() set required argument n ask R produce 5 random numbers.","code":"\n?rnorm\nrnorm(n, mean = 0, sd = 1)"},{"path":"programming-basics.html","id":"activity-2","chapter":"1 Programming basics","heading":"1.3.2 Activity 2","text":"Type following two lines code console window. Press enter/return end line \"run\" line. , type set.seed(12042016) press enter/return type rnorm(n = 5) press enter/return.now see numbers console window:numbers mean close 0 (M = -0.238) SD close 1 (SD = 0.48) - exact sampled small set sampling random. now can play function change additional arguments produce different set numbers. time say want 5 numbers (n = 5) want mean closer 10 (mean = 10) standard deviation closer 2 (sd = 2). follows see output numbers .time, produced 5 random numbers, now set numbers mean close 10 (M = 9.524) sd close 2 (0.961) specified. start get sense arguments within functions can change , can always remember use help documentation help us understand arguments function requires.\nset.seed() function\n\nlooking examples code online, may often see code starts function set.seed(). function controls random number generator - using functions generate numbers randomly (rnorm()), running set.seed() ensure get result (cases may want ). call set.seed() example means get random numbers book.\n","code":"\nset.seed(12042016)\nrnorm(n = 5)## [1] -0.2896163 -0.6428964  0.5829221 -0.3286728 -0.5110101\nset.seed(12042016)\nrnorm(n = 5, mean = 10, sd = 2)## [1]  9.420767  8.714207 11.165844  9.342654  8.977980"},{"path":"programming-basics.html","id":"stating-argument-names","chapter":"1 Programming basics","heading":"1.3.3 Stating argument names","text":"examples, written argument names code (example, wrote n = 5, mean = 10, sd = 2), however, strictly necessary. following two lines code produce similar outputs number values similar means standard deviations - although time run rnorm() produce slightly different set numbers, random.main thing lines code still work - code knows numbers. Importantly, however, key realise code following set order arguments - n mean sd. write argument names code use default order arguments: rnorm assume first number enter n, second number mean third number sd. can write arguments names , important know default order choose write argument names. however write argument names can write arguments whatever order like - still work produce six numbers mean close 3 standard deviation close 1.first learning R, may find useful write argument names can help remember understand part function . However, skills progress may find quicker omit argument names also see examples code online use argument names important able understand argument bit code referring (look help documentation check).course, always write argument names first time use function, however, subsequent uses may omitted.","code":"\nrnorm(n = 6, mean = 3, sd = 1)\nrnorm(6, 3, 1)\nrnorm(sd = 1, n = 6, mean = 3)"},{"path":"programming-basics.html","id":"tab-auto-complete","chapter":"1 Programming basics","heading":"1.3.4 Tab auto-complete","text":"writing code, one useful feature RStudio tab auto-complete functions (see Figure 1.2). write name function press tab key keyboard, RStudio show arguments function takes along brief description. press enter argument name fill name , just like auto-complete phone. can also use tab button writing function name auto-complete function name find functions start certain letters - maybe quite remember spell function example!\nFigure 1.2: Tab auto-complete\nauto-complete incredibly useful first learning R remember use feature frequently.","code":""},{"path":"programming-basics.html","id":"packages","chapter":"1 Programming basics","heading":"1.4 Base R and packages","text":"install R access range functions including options data wrangling statistical analysis. functions included default installation R typically referred Base R useful cheat sheet shows many Base R functions can found halfway page Contributed Cheatsheets along host cheatsheets.However, power R extendable open source - put simply, function exist work well, anyone can create new package contains data code allow perform new tasks. may find useful think Base R default apps come phone packages additional apps - ones really want use make phone - need download separately.","code":""},{"path":"programming-basics.html","id":"activity-3-install-the-tidyverse-optional","chapter":"1 Programming basics","heading":"1.4.1 Activity 3: Install the tidyverse (optional)","text":"order use package, must first install . following code installs package tidyverse, package use lot course introduce later chapters.PLEASE NOTE: need activity working R server using computers University lab Boyd Orr Building. activity performed device.working computer, use code install tidyverse. using R server using Boyd Orr lab machine skip activity.\nget error message says something like \"WARNING: Rtools required build R packages\" may need download install extra bit software called Rtools (one many reasons using server can easier).\ngo using packge? Note need install package , however, time start R need load packages want use. bit like need install app phone , need open every time want use .load packages use function library() - loads packages working library. Typically start analysis script loading packages need, come back next chapter.","code":"\ninstall.packages(\"tidyverse\")"},{"path":"programming-basics.html","id":"activity-4-load-the-tidyverse","chapter":"1 Programming basics","heading":"1.4.2 Activity 4: Load the tidyverse","text":"Run code load tidyverse working library.activity regardless whether using computer server.Often load packages get information console window. always get message often . also tend look like error message first - . just R telling done.Now loaded tidyverse package can use functions contains remember, need run library() function every time start R.","code":"\nlibrary(tidyverse)"},{"path":"programming-basics.html","id":"package-updates","chapter":"1 Programming basics","heading":"1.4.3 Package updates","text":"addition updates R R Studio, creators packages also sometimes update code. can add functions package, can fix errors. One thing avoid unintentionally updating installed package. run install.packages() always install latest version package overwrite older versions may installed. Sometimes problem, however, sometimes find update means code longer works package changed substantially. possible revert back older version package try avoid anyway.\navoid accidentally overwriting package later version, never include install.packages() analysis scripts case , someone else runs code mistake. Remember, server already packages need course need install packages using machine.\n","code":""},{"path":"programming-basics.html","id":"conflicts","chapter":"1 Programming basics","heading":"1.4.4 Package conflicts","text":"thousands different R packages package lots lots functions. Unfortunately, packages developed different people, sometimes different packages name different functions. example, packages dplyr MASS function named select(). run code see warning telling conflict.see warning following object masked 'package:dplyr': select. case, R telling function select() dplyr package hidden ('masked') another function name MASS package. try use select(), R use function package loaded recently - case use function MASS. issue think using one function really using another - often work differently get odd issues code expect. various solutions one simple one, already know clash, specify package want use particular function writing code format package::function, meaning \"use function package\", example:Clashes inevitable learning see one probably spot first learn resolve quickly.","code":"\nlibrary(dplyr)\nlibrary(MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\ndplyr::select()\nMASS::select()"},{"path":"programming-basics.html","id":"objects","chapter":"1 Programming basics","heading":"1.5 Objects","text":"learnt functions arguments earlier said functions give us outputs. Another name outputs, least specific types outputs, objects. Objects output functions basically - really can create objects without functions well. large part coding involve creating manipulating objects. Objects contain stuff. stuff can numbers, words, result functions, operations analyses. first key thing know object create give content. assign content object using <- - often called \"left arrow\" assignment operator stated \"assigned \". Note use =. large discussion objects assigned content equal content another time. now, just remember assign (<-) content, words, numbers, function output, objects. try now.","code":""},{"path":"programming-basics.html","id":"activity-5-create-some-objects","chapter":"1 Programming basics","heading":"1.5.1 Activity 5: Create some objects","text":"Type following code console window run line. see name, age, today, new_year, data appear environment pane.\nFigure 1.3: Objects environment\nNote examples, name,age, new_year always contain values emily, 35, date New Year's Day 2021, however, today draw date operating system day using computer, data randomly generated set data - saw earlier - values objects static.try changing name name age age, seeing update environment window.Importantly, learn future chapters, objects can involved calculations can interact . example:Finally, can store result operations objects new object :\nRemember may find helpful read <- contains assigned , e.g., name contains text emily emily assigned object name.\nconstantly creating objects throughout course learn behave go along, however, now enough understand way saving values, values can numbers, text, result operations, can used operations create new variables.\nmay also see objects referred 'variables'. difference two programming terms, however, used synchronously frequently.\n","code":"\nname <- \"emily\"\nage <- 16 + 19 \ntoday <- Sys.Date()\nnew_year <- as.Date(\"2021-01-01\")\ndata <- rnorm(n = 10, mean = 15, sd = 3)\nage + 10## [1] 45\nnew_year - today## Time difference of -304 days\nmean(data)## [1] 16.71506\ndecade <- age + 10"},{"path":"programming-basics.html","id":"look-env","chapter":"1 Programming basics","heading":"1.5.2 Looking after the environment","text":"Now starting learn windows particular environment window, writing lot code may find environment window (workspace) become cluttered many objects. can make difficult figure object need therefore run risk using wrong data frame. working new dataset, tried lots different code getting final version, good practice remember clear environment avoid using wrong object. can several way.remove individual objects, can type rm(object_name) console. Try now remove one objects created previous section. example, remove object age writing rm(age)clear objects environment run rm(list = ls()) console.clear objects environment can also click broom icon environment pane.\n\n\nFigure 1.4: Clearing workspace\n\n\nFigure 1.4: Clearing workspace\ngood idea keep environment clean tidy. leads us setting global options get confused.","code":""},{"path":"programming-basics.html","id":"global-options","chapter":"1 Programming basics","heading":"1.6 Global options","text":"open RStudio show last working , including code objects created, assuming first time used RStudio. might sound helpful, actually tends cause problems worth means risk accidentally using old version object - say Date kept environment last time work realise start working wrong Date wanted new one etc etc etc. reality, recommend changing settings time start RStudio, opens fresh new environment. can clicking top menu Tools >> Global Options deselecting boxes box looks like saving/applying changes.\nFigure 1.5: Global options\nsave lot hassle going forward. still encounter issues course going end chapter quick look getting help.","code":""},{"path":"programming-basics.html","id":"getting-help","chapter":"1 Programming basics","heading":"1.7 Getting Help","text":"","code":""},{"path":"programming-basics.html","id":"help-and-additional-resources","chapter":"1 Programming basics","heading":"1.7.1 Help and additional resources","text":"Getting good programming really means getting good trying stuff , searching help online, finding examples code use basis . difficulty exercises contained book can course ask help team, however, learning problem solve effectively key skill develop throughout course. wealth additional resources Appendix book might worth checking , four approaches take resolving issue hit problem.Use help documentation. struggling understand function works, remember ?function command.Think last run function code successfully? Look back see difference. skills build always look back go forward!get error message, copy paste Google - likely someone else problem.Trying googling question style package name function name want . example, arrange data tidyverse maybe sort data rAnd approaches work, , addition course materials PsyTeachR books, number excellent online resources learning data skills can serve quick guides:individual package quickguides found via top menu: Help >> Cheat SheetsR CookbookStackOverflowR Data ScienceSearch use #rstats hashtag TwitterYou find #rstats community referred helpful create lot excellent materials.","code":""},{"path":"programming-basics.html","id":"debugging-tips","chapter":"1 Programming basics","heading":"1.7.2 Debugging tips","text":"Another top skill resolving issues known debugging - fixing coding mistakes. fact large part coding trying figure code work true whether novice expert. progress course keep record mistakes make fixed . early chapters provide number common mistakes look undoubtedly make (fix!) new mistakes . Thing keep mind coding :loaded correct packages functions trying use? One common mistake write code load package, e.g., library(tidyverse) forget run .made typo? Coding specific spelling data DATA, t.test t_test.package conflict? tried specifying package function package::function?definitely error? red text R means error - sometimes just giving message information.","code":""},{"path":"programming-basics.html","id":"reset-your-r-sessions","chapter":"1 Programming basics","heading":"1.7.3 Reset your R sessions","text":"Finally, find code working figure , might worth starting new session. clear environment detach loaded packages - think like restarting phone.open R start writing code, loading packages, creating objects, typically new session. addition clearing workspace, can sometimes useful start new session. happen automatically time start R computer, although sessions can persist server.","code":""},{"path":"programming-basics.html","id":"activity-6","chapter":"1 Programming basics","heading":"1.7.4 Activity 6","text":"last activity shows quick way restart R inside RStudio. Top Menu, click Session >> Restart R.\nFigure 1.6: truth programming\nsaid, worry making mistakes. Accept make learn . remember help.","code":""},{"path":"programming-basics.html","id":"test-yourself","chapter":"1 Programming basics","heading":"1.8 Test yourself","text":"Finally, throughout book find additional questions like help check understanding. blanks fill , Multiple Choice, answers revealed chapter. unsure answer find explanation, just ask!Remember, run install.packages() always install latest version package overwrite older versions package may installed.Question 2.following code produce?default form rnorm() rnorm(n, mean, sd). need help remembering argument function , look help documentation running ?rnormQuestion 3. two packages functions name want specify exactly package use, code use?use form package::function, example dplyr::select. Remember first load packages R warn functions name - remember look !read_csv() looks like function brackets end <- assignment symbol likely 35 might input argument just value.Remember functions tend brackets parentheses end name arguments values go inside parentheses.assignment operator use , <-, assign content output functions object.answer c - One arguments spelt incorrectly. look closely see argument mean spelt two e's instead one e, meen. create error code crash. typical debugging issue coding just shows need careful spelling.","code":"\nrnorm(6, 50, 10)"},{"path":"programming-basics.html","id":"words-from-this-chapter","chapter":"1 Programming basics","heading":"1.9 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.End ChapterThat end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"intro-to-r.html","id":"intro-to-r","chapter":"2 Intro to R","heading":"2 Intro to R","text":"chapter going start introducing using code create reproducible research. cover things setting working directory, working directory, using R Markdown, writing code R Markdown. eight activities total chapter, worry, broken small steps help follow along.","code":""},{"path":"intro-to-r.html","id":"using-your-working-directory","chapter":"2 Intro to R","heading":"2.1 Using your Working Directory","text":"starting work data R, can save output created (almost always want ), first need tell R working directory . means tell R files going use (raw data) located, want save files created. Think just like different course modules, separate folders topic e.g. biology, history . working data, useful data sets files need nicely organised folders.highly recommend making new folder course, maybe called \"PsychResearchMethods\", \"ResearchMethodsLabs\", \"FundamentalsOfQuantAnalysis\" add sub-folders, inside folder carry chapter, saving data, scripts, portfolio files chapter folders. main thing call folder something sensible know . However, see red warning box !\nWhatever , call folder keep data skills work \"R\". , sometimes R identity crisis save load files properly. can also really damage setup R lead reinstall everthing. reason , remembering packages first chapter, R tends save packages folder called R. another folder called R gets confused stops working properly. can course folder called \"my_Rwork\" just \"R\" .\nTop tip 1: using R laptop rather server, can also good idea save work onto cloud storage server like OneDrive never lose work. Particularly work relates assignments. Just like written work, want lose coding work!Top tip 2: Coding every slightly easier file names folder names spaces . suggest folder names like \"PsychResearchMethods\" instead \"Psych Research Methods\".","code":""},{"path":"intro-to-r.html","id":"intro-a1","chapter":"2 Intro to R","heading":"2.1.1 Activity 1: Create your a folder for all your work","text":"Choose location work assignments module create folder necessary sub-folders chapter. blank now add semester.","code":""},{"path":"intro-to-r.html","id":"intro-a1b","chapter":"2 Intro to R","heading":"2.1.2 Activity 1b: Upload data files to the server","text":"Note: activity (activity 1b) using University Glasgow R Server - using local installation device need activity skip Activity 2.main disadvantage using R server need create folders server upload download files working server. Please aware link computer R server. change files server, appear computer download server, need careful submit assessment files submitting right file. main reason recommend installing R computer can.Going forward throughout book, using server, extra step also upload sever. going use data files session try example make clear get data files server.Log R server using link main course Moodle page.file pane click New folder create structure created computer.Download ahi-cesd.csv participant-info.csv chapter folder computer. download file book, right click link select \"save link \". Make sure files saved \".csv\". open machine often software like Excel can change setting ruin files.Now files stored computer, go RStudio server click Upload Browseand choose folder chapter working .Click Choose file go find data want upload.","code":""},{"path":"intro-to-r.html","id":"intro-a2","chapter":"2 Intro to R","heading":"2.1.3 Activity 2: Set the working directory","text":"Ok great, now folder structure keep everything nice ordered need set working directory clicking top menu, Session >> Set Working Directory >> Choose Directory select relevant folder chapter working directory.Note: Setting working directory means telling R data want work . also going save files output. often , learning, common error people set working directory R know data !Top tip: One first things always open R RStudio set working directory data .\nFigure 2.1: Setting working directory\n","code":""},{"path":"intro-to-r.html","id":"r-markdown-for-data-skills-and-portfolio-assignments","chapter":"2 Intro to R","heading":"2.2 R Markdown for data skills and portfolio assignments","text":"Ok great, now folder structure going use told R now write code!duration data skills book related assignments use worksheet format called R Markdown (abbreviated Rmd) great way create dynamic documents embedded chunks code. Remember saw scripts Chapter 1? Well R Markdown like script excellent features make much better.R Markdown documents self-contained fully reproducible (necessary data, able run someone else's analyses click button) makes easy share. important part open science training one reasons teach data skills way enables us share open reproducible information. Using worksheets enables keep record code write course, comes time portfolio assignments, can give task can fill required code.information R Markdown feel free look main webpage sometime http://rmarkdown.rstudio.com now, key advantage know allows write code document, along regular text, knit using package knitr create document either webpage (HTML), PDF, Word document (.docx). become clear example!","code":""},{"path":"intro-to-r.html","id":"intro-a3","chapter":"2 Intro to R","heading":"2.2.1 Activity 3: Open and save a new R Markdown document","text":"Open new R Markdown document clicking 'new item' icon click 'R Markdown' shown :\nFigure 2.2: Opening new R Markdown document\nnow prompted give title call \"Intro R\".Also, change author name GUID good practice portfolio assignments. * Leave output format selected HTML now!click OK open new R Markdown document.Save R Markdown document, clicking File >> Save top menu, name file \"Intro R\" well.set working directory correctly, now see file appear file viewer pane bottom right hand corner like example (file names folders different depending called folders file).\nFigure 2.3: New file working directory\n","code":""},{"path":"intro-to-r.html","id":"intro-a4","chapter":"2 Intro to R","heading":"2.2.2 Activity 4: Create a new code chunk","text":"Great. now R Markdown document start using see can combine code text create informative document.first open new R Markdown document see bunch default text looks like :\nFigure 2.4: New R Markdown text\ndefault text just give examples can R Markdown going show well following:Delete everything line 7On line 8 type \"\"Click Top Menu: Code >> Insert Chunk top menu.Markdown document now look something like :\nFigure 2.5: New R chunk\ncreated called code chunk. R Markdown, anything written outside code chunk assumed just normal text, just like text editor, anything written inside code chunk assumed code. makes easy combine text code one document.\ncreate new code chunk notice grey box starts ends three back ticks, followed {r}, ends three back ticks . structure creates code chunk. actually just type structure instead using Insert approach learning help bit!\n\nOne common mistake accidentally delete back ticks. useful thing notice code chunks tend different color background - default viewing settings code chunk grey normal text white. can use look mistakes. colour certain parts Markdown look right, check deleted backticks.\n\naddition, remember backticks (.e. `) single quotes (.e. ')!\n","code":""},{"path":"intro-to-r.html","id":"intro-a5","chapter":"2 Intro to R","heading":"2.2.3 Activity 5: Write some code","text":"Awesome! great learning think !Now going use code examples read Programming Basics add code R Markdown document.code chunk write code replace values name/age/birthday details). Remember four lines code inside code chunk!Note: Text dates need contained quotation marks, e.g. \"name\". Numerical values written without quotation marks, e.g. 45.Top tip: Missing /unnecessary quotation marks common cause code working - remember !","code":"\nname <- \"Emily\" \nage <- 35\ntoday <- Sys.Date()\nnext_birthday <- as.Date(\"2021-07-11\")"},{"path":"intro-to-r.html","id":"running-code-in-r-markdown","chapter":"2 Intro to R","heading":"2.3 Running code in R Markdown","text":"Brilliant! now code code chunk now going run code! Running code just trying make work, seeing works! working R Markdown document, several ways run lines code.First, one slow option can highlight code want run click Run >> Run Selected Line(s).\nFigure 2.6: Slow method running code\nAlternatively, can press green \"play\" button top-right code chunk run lines code chunk.\nFigure 2.7: Slightly faster method running code runs lines within code chunk!\nEven better though learn keyboard shortcuts becomes natural fluid typing makes learning easier!run single line code, make sure cursor line code want run press ctrl + enter.want run code code chunk, press ctrl + shift + enter.\nNote: using last method running lines code - positioning cursor line using ctrl + enter keyboard, note cursor specific point line, .e. start, middle end, can literally anywhere.","code":""},{"path":"intro-to-r.html","id":"intro-a6","chapter":"2 Intro to R","heading":"2.3.1 Activity 6: Run your code","text":"Now run code using one methods . see variables name, age, today, next_birthday appear environment pane top right corner.Clear environment using broom handle approach saw Chapter 1 try different method see works best !","code":""},{"path":"intro-to-r.html","id":"intro-a7","chapter":"2 Intro to R","heading":"2.3.2 Activity 7: Inline code","text":"Superb! code works know run . one incredible benefits said R Markdown can mix text code. Even better can combined code sentence put specific outputs code, like value, using called inline code. Think time copy paste value text one file another know easy can make mistakes. Inline code avoids . easier show inline code rather explain go.First, copy paste text exactly (change anything) underneath outside code chunk - white section grey code chunk using default views.Ok nothing happened done last magic step - next activity!","code":"My name is `r name` and I am `r age` years old. \n\nIt is `r next_birthday - today` days until my birthday."},{"path":"intro-to-r.html","id":"intro-a8","chapter":"2 Intro to R","heading":"2.3.3 Activity 8: Knitting your file","text":"final step going knit file. means going compile (.e. turn) code document presentable.\n* top menu, click Knit >> Knit HMTL. R Markdown now create new HTML document automatically save file working directory.Now look outputted HTML document sentence copied Activity 7. magic, slightly odd bit text copied pasted now appears normal sentence values pulled objects created.name Emily 35 years old. -113 days birthday.Pretty amazing ! going use inline coding often rest course hopefully can see just useful writing report lots numbers! R Markdown incredibly powerful flexible format - book written using ! key thing using inline coding structure, .e. backtick, followed lower case r, space, code, another backtick. get hang semester goes little practice.final things note knitting useful going forward data skills learning assignments:R Markdown knit code works - good way checking assignments whether written functioning code!can choose knit Word document rather HTML. can useful e.g., sharing others, however, may lose functionality probably look good recommend always knitting HTML.can choose knit PDF, however, unless using server requires LaTex installation quite complicated. already know LaTex use , knit PDF. know use LaTex, need us give instructions!R automatically open knitted HTML file viewer, however, can also navigate folder stored open HTML file web browser (e.g., Chrome Firefox).","code":""},{"path":"intro-to-r.html","id":"finished","chapter":"2 Intro to R","heading":"2.4 Finished","text":"done! first time using R written functioning code written reproducible output! send someone else R Markdown document able produce exactly HTML document , just pressing knit.key thing want take away chapter data skills going learn can broken manageable chunks going teach help learn . skills might new lot , going take step--step. amazed quickly can start producing professional-looking data visualisations analysis.questions anything contained chapter Programming Basics, please remember ask us!","code":""},{"path":"intro-to-r.html","id":"test-yourself-1","chapter":"2 Intro to R","heading":"2.5 Test Yourself","text":"One key first steps open RStudio :\n\nset working directoryput top tunes whilebuild foldersmake coffee\nOne common issues see code work first time people forgotten set working directory. working directory file want save files , output, contains data. Code needs know data set working directory first step open RStudioWhen using default environment color settings RStudio color background code chunk R Markdown? whiteredgreengreyWhen using default environment color settings RStudio color background code chunk R Markdown? whiteredgreengreyWhen using default environment color settings RStudio color background normal text R Markdown? whiteredgreengreyWhen using default environment color settings RStudio color background normal text R Markdown? whiteredgreengreyAssuming changed settings RStudio, code chunks tend grey background normal text tend white background. good way check closed opened code chunks correctly.Code chunks started :\n\nthree single quotesthree backticksthree double quotesthree single clefs\nCode chunks always take general format three backticks followed curly parentheses lower case r inside parentheses. Often people mistake backticks single quotes work. set code chunk correctly, using backticks, background color change!Inline coding :\n\nnicely organise code linewhere make sure code nicely indented sidean exuberant way exclaiming written good code!approach intergrating code text sentence outside code chunk\nInline coding incredibly useful approach merging text code sentence outside code chunk. can really useful want add values code directly text. Copying pasting can create errors easily better code can!","code":""},{"path":"intro-to-r.html","id":"words-from-this-chapter-1","chapter":"2 Intro to R","heading":"2.6 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.End ChapterThat end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"starting-with-data.html","id":"starting-with-data","chapter":"3 Starting with data","heading":"3 Starting with data","text":"Part becoming psychologist formulating asking research questions gathering data enable answer questions effectively. order , important understand aspects research process experimental design, ethics, data management visualisation.chapter continue develop reproducible scripts. means develop scripts completely transparently perform analysis start finish way yields result different people using software different computers. transparency key value science embodied “trust verify” motto. things reproducibly, others can understand check work. idea open science big debate scientific community moment. classic psychological experiments found replicable part explanation historical lack transparency data analysis methods. topic going cover lectures reading.Reproducible research benefits science, selfish reason, : important person benefit reproducible script future self. return analysis two weeks vacation, thank earlier self things transparent, reproducible way, can easily pick right left .part skill development important work data can become confident competent management analysis data. course, work real data shared researchers show real data looks like get used issues arise!","code":""},{"path":"starting-with-data.html","id":"getting-ready-to-work-with-data","chapter":"3 Starting with data","heading":"3.1 Getting ready to work with data","text":"chapter learn load packages required work data. load data RStudio getting organised format (structure) helps us answer research question. top tip remember always think back done - instance, remember packages , go back revise Programming Basics.begin working data need set-get data working directory.","code":""},{"path":"starting-with-data.html","id":"ld-a1","chapter":"3 Starting with data","heading":"3.1.1 Activity 1: Set-up the data, working directory and Rmd file","text":"Download ahi-cesd.csv participant-info.csv folder computer want use chapter!\ndownload file book, right click link select \"save link \". Make sure files saved \".csv\". open machine often software like Excel can change setting ruin files cause problems. look data load R RStudio.\nworking server, need upload files server well.\ndownload file book, right click link select \"save link \". Make sure files saved \".csv\". open machine often software like Excel can change setting ruin files cause problems. look data load R RStudio.working server, need upload files server well.Next, open RStudio ensure environment clear.\nserver, avoid number issues restarting session - click Session - Restart R\nserver, avoid number issues restarting session - click Session - Restart RSet working directory chapter folder. might want refer Activity 2 Chapter 2 unsure step.Now open new R Markdown document (.Rmd file) save working directory. Call file \"LoadingData\". can refer Activity 3 Chapter 2\nNote: R Markdown file (LoadingData.Rmd) must folder datafiles code going write work.\nNote: R Markdown file (LoadingData.Rmd) must folder datafiles code going write work.Finally, delete default R Markdown text insert new code chunk. Remember delete text code comes /line 7.now ready begin working data. top tip use white space take notes might help activity make reminders things !","code":""},{"path":"starting-with-data.html","id":"ld-a2","chapter":"3 Starting with data","heading":"3.1.2 Activity 2: Loading a package to our library","text":"Today need use tidyverse package. use package almost every single chapter course functions contains use data wrangling, descriptive statistics, visualisation. load package library using library() function.load tidyverse type following code code chunk run .Remember sometimes console window see information package loaded, sometimes . however see line code just run repeated console window. see red text, sure read might warning, error message.","code":"\nlibrary(tidyverse)"},{"path":"starting-with-data.html","id":"the-data","chapter":"3 Starting with data","heading":"3.2 The data","text":"chapter going using real data following paper:Woodworth, R.J., O'Brien-Malone, ., Diamond, M.R. Schüz, B., 2018. Data , ‘Web-based Positive Psychology Interventions: Reexamination Effectiveness’. Journal Open Psychology Data, 6(1).good read paper even briefly, just abstract, give sense paper data might look like, summary files contain data two scales well demographic information participants. two scales :Authentic Happiness Inventory (AHI),Center Epidemiological Studies Depression (CES-D) scale.","code":""},{"path":"starting-with-data.html","id":"ld-a3","chapter":"3 Starting with data","heading":"3.2.1 Activity 3: Read in data","text":"Now data folder need read data - \"read\" sense just means bring data RStudio store object can work . use function read_csv() allows us read .csv files. also functions allow read Excel files (e.g. .xlsx) formats, however course use .csv files software specific therefore better looking practice open science! .csv file can read basic text editor nearly machines.code chunk reads datafiles. Type code chunk run . look .First, create object called dat contains data ahi-cesd.csv file.Next create object called info contains data participant-info.csv.Note lines format object <- function(\"datafile_name.csv\")\nimperative double quotation marks around datafile name datafile name spelt correctly includes .csv part.\nremember <- called assignment operator can read \"assigned \". example, first line can read data ahi-cesd.csv assigned object called dat.\nimperative double quotation marks around datafile name datafile name spelt correctly includes .csv part.remember <- called assignment operator can read \"assigned \". example, first line can read data ahi-cesd.csv assigned object called dat.done activity correctly, preceding activities, now see objects dat pinfo appeared environment pane. check spelling filenames structure code lines well maybe working directory.\nWATCH ! also function called read.csv(). careful use function instead read_csv() different ways naming columns. activities assignments always ask expect use read_csv(). really reminder watch spelling functions careful use right functions.\n","code":"\ndat <- read_csv(\"ahi-cesd.csv\")\npinfo <- read_csv(\"participant-info.csv\")"},{"path":"starting-with-data.html","id":"looking-at-data","chapter":"3 Starting with data","heading":"3.3 Looking at Data","text":"Great! Now data read first step always initial check see data looks like. Normally idea already experiment ran using someones data might , best check . several ways can look data listed Activity 4 . Try see results differ.","code":""},{"path":"starting-with-data.html","id":"ld-a4","chapter":"3 Starting with data","heading":"3.3.1 Activity 4: Look at your data","text":"Option 1: environment pane, click name object want look . example, click names dat pinfo. open data give spreadsheet-like view (although edit like Excel)Option 2: environment pane, click small blue play button left dat pinfo. show structure object information including names variables object type (also see str(pinfo))Option 3: console window, type run str(pinfo) str(dat)Option 4: Repeat option 3 time use summary() function - e.g. summary(dat)Option 5: Repeat option 3 time use head() functionOption 6: Type name object want view console window run , e.g., type dat console window run .can see various different ways get idea data looks like. tells similar also different info. explore get book now just aware can use approaches see data. often Option 1 Option 2 give info need, quickest.","code":""},{"path":"starting-with-data.html","id":"joining-data","chapter":"3 Starting with data","heading":"3.4 Joining Data","text":"far awesome! data know looks like, start trying things data! first thing combine datafiles! two files, dat info really want single file data demographic information participants makes easier work data combined together. going use function inner_join() comes dplyr package - one packages loaded part tidyverse. worry much deliberately trying remember different packages functions come naturally practice give .Top tip: Remember use help function ?inner_join want information use function use tab auto-complete help write code.","code":""},{"path":"starting-with-data.html","id":"ld-a5","chapter":"3 Starting with data","heading":"3.4.1 Activity 5: Join the files together","text":"code create new object, called all_dat, combines data dat pinfo using information columns id intervention match participants' data across two sets data. going inner join approach - data kept participant exist datafiles. lots different joins see go book.Type run code new code chunk inner join two sets data.see can make sense happening\nall_dat new object data combined\nx first argument first data/object want combine\ny second argument second data/object want combine\nthird argument lists names columns want combine data . uses additional function c() say one column combine .\nall_dat new object data combinedx first argument first data/object want combiney second argument second data/object want combineby third argument lists names columns want combine data . uses additional function c() say one column combine .run code now see all_dat environment pane. View new dataset using one methods Activity 4. fact, try remember always view new object data create. Code often can run necessarily mean correct. programme ever knows code says thought said. Get habit always checking output!","code":"\nall_dat <- inner_join(x = dat, \n                      y = pinfo, \n                      by = c(\"id\", \"intervention\"))"},{"path":"starting-with-data.html","id":"selecting-data","chapter":"3 Starting with data","heading":"3.5 Selecting Data","text":"Excellent! now combined data one big object! However, frequently, datasets variables, information, data actually want use can make life easier create new object just data need. , final step today select just variables interest!\ncase, all_dat contains responses individual question AHI scale CESD scale, well total score (.e., sum individual responses). say analysis care total scores demographic information participants. going use new function called select() function, dplyr package, select columns interested store (.e. assign ) new object called summarydata","code":""},{"path":"starting-with-data.html","id":"ld-a6","chapter":"3 Starting with data","heading":"3.5.1 Activity 6: Pull out variables of interest","text":"Type run code new code chunk. also quick look code.\nsummarydata new object creating using select() function\n.data first argument wants know object going select columns . instance all_dat.\nnext list columns want keep. Every column must spelt correctly must exist object selecting . Makes sense really; otherwise function know wanted!\nsummarydata new object creating using select() function.data first argument wants know object going select columns . instance all_dat.next list columns want keep. Every column must spelt correctly must exist object selecting . Makes sense really; otherwise function know wanted!worked correctly see summarydata environment pane can run head(summarydata) now console window get view output. see red text console window worth checking spelling objects columns wanted select. everything gone plan output look something like :","code":"\nsummarydata <- select(.data = all_dat, \n                      ahiTotal, \n                      cesdTotal, \n                      sex, \n                      age, \n                      educ, \n                      income, \n                      occasion,\n                      elapsed.days)"},{"path":"starting-with-data.html","id":"knitting-our-reproducible-code","chapter":"3 Starting with data","heading":"3.6 Knitting our Reproducible code","text":"saw Activity 8 Chapter 2 final step making reproducible document knit HTML! Try knitting file HTML now! code working correctly get html document showing code! get output issues either relating code installation. can use debugging tips ask questions code. code looks correct sure speak one TEAM see might wrong.","code":""},{"path":"starting-with-data.html","id":"ld-debug","chapter":"3 Starting with data","heading":"3.6.1 Debugging tips","text":"downloaded files save file names exactly originally? download file find computer may automatically add number end file name. data.csv data(1).csv. Pay close attention names!used exact object names activity? Remember, name different Name. order make sure can follow along book, pay special attention ensuring use object names .used quotation marks needed?accidentally deleted back ticks (```) beginning end code chunks?","code":""},{"path":"starting-with-data.html","id":"code-layout","chapter":"3 Starting with data","heading":"3.7 Code Layout","text":"one quick point end day. may noticed wrote code :also written :exactly ! code chunk can take new line comma (,) code nicely idents . can make easier read debug code nicely presented essential!","code":"\nall_dat <- inner_join(x = dat, \n                      y = pinfo, \n                      by = c(\"id\", \"intervention\"))\nall_dat <- inner_join(x = dat, y = pinfo, by = c(\"id\", \"intervention\"))"},{"path":"starting-with-data.html","id":"ld-fin","chapter":"3 Starting with data","heading":"3.8 Finished!","text":", well done! Remember save work chapter folder make note mistakes made fixed . started journey become confident competent member open scientific community!Now good time get comfortable covered already revise activities support materials presented far needed. feeling comfortable can work way book pace push using additional resources highlighted Programming Basics. forget try tasks check understanding knowledge skills learning!Finally, using R server, strongly recommend download copy files working save machine local back-.","code":""},{"path":"starting-with-data.html","id":"ld-test","chapter":"3 Starting with data","heading":"3.9 Test yourself","text":"","code":""},{"path":"starting-with-data.html","id":"knowledge-questions","chapter":"3 Starting with data","heading":"3.9.1 Knowledge Questions","text":"loading .csv file, function use?\n\nread_csv()read.csv()select()library()\nRemember, course use read_csv() important use function otherwise may find data work expected.function inner_join() takes arguments x, y, . ?\n\nSpecifies first object joinSpecifies second object joinSpecifies column join objects common\nRemember, functions arguments arguments something slightly different. inner_join() argument says columns join . want join one column need put columns inside c() function.function select() ?\n\nKeeps observations specifyKeeps columns specifyKeeps objects specify\nselect() function comes one tidyverse packages - dplyr precise. main function use keep remove columns want want. start remember functions need work . Remember best approach think back !","code":""},{"path":"starting-with-data.html","id":"ld-debugex","chapter":"3 Starting with data","heading":"3.9.2 Debugging exercises","text":"One key skill learning fix errors code. exercises specifically design create errors. Ruun exercise try solve errors moving next one. Make note error message solved - might find helpful create new file just error solving notes. find often make errors running analyses; experts also make tonnes errors. difference novice expert first learning, error might slow , greatly speed practice. put errors!Restart R session (Session >> Restart R). Make sure working directory set right folder run code:produce error:figure fix error, make note .restarted session unloaded packages previously loaded - .e. tidyverse. function read_csv() part tidyverse package means order code run need run library(tidyverse) reload package can use function. Remember always need load packages library install . , think apps phone!Restart R session (Session >> Restart R). Make sure working directory set right folder run code:produce error:figure fix error, make note .loading data need provide full file name including file extension. case error caused writing ahi-cesd instead ahi-cesd.csv. far coding goes, two completely different files one exists working directory.Restart R session (Session >> Restart R). Make sure working directory set right folder run code:Look summary all_dat. see R duplicated intervention variable, now intervention.x intervention.y contain data. figure fix error, make note .want join two objects mulitple columns common need use c() command list columns. code done , just listed id intervention without enclosing c() defaults using just first one ignores column. objects intervention column keeps . rule , joining objects, join common columns!.","code":"\ndat <- read_csv(\"ahi-cesd.csv\")`could not find function \"read_csv\"`\nlibrary(tidyverse)\ndat <- read_csv(\"ahi-cesd\")`Error: 'ahi-cesd' does not exist in current working directory`.\nlibrary(tidyverse)\ndat <- read_csv (\"ahi-cesd.csv\")\npinfo <- read_csv(\"participant-info.csv\")\nall_dat <- inner_join(x = dat, \n                      y = pinfo, \n                      by = \"id\", \"intervention\") \nsummary(all_dat)"},{"path":"starting-with-data.html","id":"words-from-this-chapter-2","chapter":"3 Starting with data","heading":"3.10 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.End ChapterThat end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"data-wrangling-1.html","id":"data-wrangling-1","chapter":"4 Data wrangling 1","heading":"4 Data wrangling 1","text":"far starting get head around data storing objects learn, data comes lots different formats. One common formats two-dimensional table (two dimensions rows columns). Usually, row stands separate object (e.g. subject), column stands different variable (e.g. response, category, group). key benefit tabular data allows store different types data, e.g. numerical measurements, alphanumeric labels, categorical descriptors, etc, one place.One core skills learn working data data wrangling. means completing tasks determining outliers, clearing erroneous values, changing structure tables, merging information stored separate tables, reducing data subset observations, producing data summaries. may surprise learn researchers actually spend far time cleaning preparing data spend actually analysing . estimated 80% time spent data analysis involves data preparation tasks (Dasu & Johnson, 2003)!ask people data wrangling, many people seem operate assumption approach painstaking time-consuming cutting pasting data within proprietary spreadsheet programs like Excel. Indeed, many may waste days, weeks, even months manually transforming data laborious cutting, copying, pasting data. Wrangling data manually, hand, however terrible use time, error-prone reproducible. Additionally, day age can easily collect massive datasets online, just conceivable organise, clean, prepare data hand, alternative reproducible approach begun learn highly beneficial.short, able perform key data wrangling skills allow opportunity thrive, psychologist numerous roles require handling data. see truth although every dataset presents unique challenges, systematic principles follow make analyses easier, less error-prone, efficient, reproducible.Putting another way, data changes skills stay !now get learning new skills. lesson see data wrangling skills allow efficiently get answers nearly question might want ask data. learning properly make computer hard boring work , can focus bigger issues.","code":""},{"path":"data-wrangling-1.html","id":"tidyverse","chapter":"4 Data wrangling 1","heading":"4.1 Tidyverse","text":"first chapter introduced package called tidyverse going core lot data skills develop. tidyverse https://www.tidyverse.org/ (Wickham, 2017) actually collection packages developed team led world-famous data scientist Hadley Wickham. core packages contained within tidyverse dplyr, tidyr, readr, purrr, ggplot2, tibble, within six core packages access functions pretty much cover everything need order able wrangle visualise data.. , worry trying remember different packages , come practice. main thing note however previously typed library(tidyverse) code, seen loads packages one go. learn use whole host functions tidyverse progress, chapter going focus functions dplyr package, mainly data wrangling, ggplot2 visualisation (.e. creating images figures).Looking dplyr package specifically, contains six important functions based common English verbs help readability code. six verbs often referred Wickham Six \"one-table\" dplyr verbs perform actions single table data. six functions :Just looking names gives idea functions . example, select(), saw previously, selects columns! fooled, although operations functions may seem simplistic, ’s amazing can accomplish string together. Hadley Wickham, fact, claimed 90% data analysis can reduced operations described six functions. going use functions introduce today just show can .","code":""},{"path":"data-wrangling-1.html","id":"intro-to-the-babynames-database","chapter":"4 Data wrangling 1","heading":"4.2 Intro to the babynames database","text":"demonstrate power six dplyr verbs use work data babynames package. babynames dataset historical information births babies U.S. 1880 present day nice understandable dataset allow us us just focus getting know functions across series activities, beginning now!","code":""},{"path":"data-wrangling-1.html","id":"dw1-a1","chapter":"4 Data wrangling 1","heading":"4.2.1 Activity 1: Set-up","text":"Complete following series steps. unsure, try consulting previous two chapters, keeping mind something done !Open RStudio set working directory chapter folder. Ensure environment clear.\nworking Rserver, avoid number issues restarting session - top menu: Session >> Restart R\nworking Rserver, avoid number issues restarting session - top menu: Session >> Restart ROpen new R Markdown document save working directory. Call file \"DataWrangling1.Rmd\".working computer, install package babynames using install.packages() function - need quotation marks around package name. Remember, never install packages working university computer Rserver.Delete default R Markdown default text (.e. everything line 12 ) insert new code code chunk loads packages babynames tidyverse using library() function run code chunk.\nNote order packages deliberate!\nNote order packages deliberate!\norder libraries\n\nfirst chapter recall mentioned issue conflicts - situation two packages loaded library functions name different approaches jobs. default approach R takes situation \"mask\" function library loaded first. means use function library loaded recently. tidyverse contains functions use time, safest approach always load tidyverse last, regardless packages loading . truth, everything life, additional complication involved sometimes arises now taking approach running library(tidyverse) last going save lot issues.\n","code":"\nlibrary(babynames)\nlibrary(tidyverse)"},{"path":"data-wrangling-1.html","id":"dw1-a2","chapter":"4 Data wrangling 1","heading":"4.2.2 Activity 2: Look at the data","text":"Great! Now packages loaded look data. package babynames bit unique contains object name, babynames, contains data , well, baby names, can look object get sense data.look data typing word babynames console window running .see something like following output:first line tells us \"# tibble: 1924665 x 5\". means object looking actually tibble, type two dimensional table unique properties, data across 1924665 variables (columns) 1924665 million observations (rows). Yes, dataset contains 1.9 million observations. Interested analyzing data hand? thanks!Looking column names start get sense data . variables (columns) follows:, row represents data births given name sex given year. example, first row data tells us year 1880, 7065 baby girls (F) born U.S.given name Mary, accounted 7.238359% baby girls year.","code":"\nbabynames"},{"path":"data-wrangling-1.html","id":"dw1-a3","chapter":"4 Data wrangling 1","heading":"4.2.3 Activity 3: Your first plot","text":"Brilliant! Now know data looks like terms table - really tibble - show quick code help visualise data.new code chunk R Markdwon file, type code run .see output:\nFigure 4.1: Proportion four baby names 1880 2014\nNow know code right now make much sense worry expect fully understand code just yet. point really show much can accomplish little code. code creates figure (Figure 4.1) showing popularity four girl baby names - Alexandra, Beverly, Emily, Kathleen - 1880 2014. Popularity expressed proportion (y-axis) different years (x-axis). come learn, ggplot() main visualisation function, geom_line() creates linegraph.Now change names code chunk female names like run code see Figure changes.Now change names code chunk male names change sex “F” “M”. Run code see happens. Post photos new plots Teams channel.Now change code want display post image favorite figure created TEAMS channel get praise TEAM!complicated might first imagine read feeling confident. remove filter sex creating dat run plot code , make messy looking plot (try ). names two data points although numbers might small gendered names, usually always least one baby non-dominant name gender assigned name.can get around adding additional line code produces separate plots sex. See :\nFigure 4.2: Plots sex different scales\nfacet_wrap() function one can split figures based given variable - case sex, meaning give plot different sex categories . scales argument tells code can use different scales y-axis plot - large difference two scales helpful allow see data sets (run code remove scales argument run see difference) although run risk people misinterpreting data difference scales made clear.hand, scales two groups fairly similar, better keep scales aid comparison. time filter dataset gender neutral names might make sense scale - try without scales argument see happens\nFigure 4.3: Plots sex scale\nside point, countries assigned sex birth binary, data intersex, trans non-binary names. lieu , ’s Wikipedia page gender-neutral names naming laws around world hopefully make question ascribe someone’s entire gender identity bunch sounds letters use label .","code":"\ndat <- babynames %>% \n  filter(name %in% c(\"Emily\",\n                     \"Kathleen\",\n                     \"Alexandra\",\n                     \"Beverly\"), sex==\"F\")\n\nggplot(data = dat,\n       aes(x = year,\n           y = prop, \n           colour = name))+\n  geom_line()  \ndat2 <- babynames %>% \n  filter(name %in% c(\"Emily\",\"Kathleen\",\"Alexandra\",\"Beverly\"))\n\nggplot(data = dat2,aes(x = year,y = prop, colour=name))+\n  geom_line() +\n  facet_wrap(~sex, scales = \"free_y\", nrow = 2)\ndat3 <- babynames %>% \n  filter(name %in% c(\"Sam\",\"Alex\",\"Jordan\",\"Drew\"))\n\nggplot(data = dat3,aes(x = year,y = prop, colour=name))+\n  geom_line() +\n  facet_wrap(~sex, nrow = 2)"},{"path":"data-wrangling-1.html","id":"six-functions-for-wrangling-the-babynames","chapter":"4 Data wrangling 1","heading":"4.3 Six functions for wrangling the babynames","text":"Ok now fairly code understanding babynames, use learn bit six functions dplyr make lot data wrangling!","code":""},{"path":"data-wrangling-1.html","id":"dw1-a4","chapter":"4 Data wrangling 1","heading":"4.3.1 Activity 4: Selecting variables of interest with select()","text":"Often data lot variables need can easier focus just data need. babynames two numeric measurements name popularity - prop, proportion babies name, probably useful n, total number babies name, prop takes account different numbers babies born different years.Now saw previously, wanted create dataset includes certain variables, can use select() function dplyr package.new code chunk, type run code select columns year, sex, name prop store tibble object named babynames_reduced.\nfirst argument .data object want select variables , case babynames\nadditional arguments names variables want keep!\nfirst argument .data object want select variables , case babynamesthe additional arguments names variables want keep!Alternatively, can also state variables want keep! really handy want keep lot columns get rid maybe one two.Type run code new code chunk.\n, rather stating want keep year, sex, name prop, can say drop (.e. get rid ) column n using minus sign - variable name.\n, rather stating want keep year, sex, name prop, can say drop (.e. get rid ) column n using minus sign - variable name.Note select() change original tibble, babynames, makes new object stores new tibble specified columns, .e. babynames_reduced1 babynames_reduced2. see new objects environment pane. save new tibbles object, saved. example, code previous code saved output function assigned (using assignment operator, <-) new object.","code":"\nbabynames_reduced1 <- select(.data = babynames,\n                            year, \n                            sex, \n                            name, \n                            prop)\nbabynames_reduced2 <- select(.data = babynames, \n                             -n)\nselect(.data = babynames, \n       -n)"},{"path":"data-wrangling-1.html","id":"dw1-a5","chapter":"4 Data wrangling 1","heading":"4.3.2 Activity 5: Arranging the data with arrange()","text":"Superb! now know select variables. Another handy skill able change order data tibble. function arrange() sort rows table according columns supply.Type run code new code chunk.\nfirst argument .data says object work \nsecond argument says column sort data - instance sorting name\ndefault sorting order ascending!\nfirst argument .data says object work onthe second argument says column sort data - instance sorting namethe default sorting order ascending!look sort1 data now sorted ascending alphabetical order name. want data descending order? can wrap name variable desc() function.Type run code new code chunk. run code look sort_desc note data sorted descending year!Finally, can also sort one column combination ascending descending columns. look th code answer question :columns stated wrapped desc() function inside arrange() function columns sorted descending fashion, sorting year first, sex, prop. Note however output stored new object able work data later without saving object first!","code":"\nsort_asc <- arrange(.data = babynames,\n                    name)\nsort_desc <- arrange(babynames, \n                     desc(year)) \narrange(babynames, \n        desc(year), \n        desc(sex), \n        desc(prop)) "},{"path":"data-wrangling-1.html","id":"dw1-a6","chapter":"4 Data wrangling 1","heading":"4.3.3 Activity 6: Using filter() to choose observations","text":"well ! OK, previously used select() keep certain variables (.e. columns). However, frequently also want keep certain observations (.e. rows) - example, babies born 1999, Female babies (), babies named \"Mary\". using verb filter(). filter() function bit involved verbs, requires detailed explanation, also extremely powerful.example filter() function action. Take minutes think might . Perhaps even run new code chunk look ouput!think ? going tell now maybe write answer reading !OK, see correct. first part code tells function use object babynames. second argument, year > 2000, known Boolean expression: expression whose evaluation (action) results value either TRUE FALSE. filter() really keep observations (rows) expression (year > 2000 - reads \"year greater 2000\") evaluates TRUE, excludes (removes) row expression evaluates FALSE. effect, behind scenes, filter() goes entire set 1.9+ million observations, row row, checking value year row, keeping value greater 2000, rejecting less 2000.see boolean expression works, consider code . First create object values (pretend years) 1996:2005.Now run boolean expression fomr across set values:can see expression years > 2000 returns logical vector (vector TRUE FALSE values), entry represents whether expression (year > 2000) TRUE FALSE entry years. first five years (1996 2000) answer FALSE less 2000, last five years (2001 2005) TRUE. Note 2000 returns FALSE 2000 greater 2000; instead equivalent 2000.commonly used Boolean expressions.run just introduce get idea. suggest typing one new code chunk running looking output get idea .First example want observations specific name (e.g., Mary), use equivalence operator ==. Note use double equal signs, single equal sign.Alternatively, wanted names except Mary, use 'equals' operator single equals sign preceeded exclamation mark:Next, wanted names defined set - e.g., names British queens? Just like previously, can use %% approach shown .gives data names vector right hand side %%. Note names held together c() seen , name surrounded quotation marks seperated comma.Next, bit like got rid Marys, can always invert expression get opposite - exclamation mark tends . , instance, instead wanted get rid Marys, Elizabeths, Victorias use following:Ok brief intro filters worth keeping mind can include many expressions like additional arguments filter() pull rows expressions evaluate TRUE. instance, filter(babynames, year > 2000, prop > .01) keep rows (observations) year greater 2000 represent greater 1% names given sex; observation either expression false excluded. ability string together criteria makes filter() powerful member Wickham Six. Referring back section help lot answers problems face wrangling data!","code":"\nfilt1 <- filter(.data = babynames, \n                year > 2000)\nyears <- 1996:2005\nyears##  [1] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005\nyears > 2000##  [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\nonly_Marys <- filter(babynames, \n                     name == \"Mary\")\nno_Marys <- filter(babynames, \n                   name != \"Mary\") \nqueens <- filter(babynames, \n                 name %in% c(\"Mary\",\n                             \"Elizabeth\",\n                             \"Victoria\"))\nno_queens <- filter(babynames, \n                    !(name %in% c(\"Mary\",\n                                  \"Elizabeth\",\n                                  \"Victoria\")))"},{"path":"data-wrangling-1.html","id":"dw1-a7","chapter":"4 Data wrangling 1","heading":"4.3.4 Activity 7: Creating new variables with mutate()","text":"really well! little go promise! OK really learnt lot changing data sometimes need create new variable doesn’t exist dataset. instance, might want figure decade particular year belongs babynames data add data! create new variables, use function mutate().Type run code new code chunk.\n, mutating new column onto data storing object baby_decades\nfirst argument original data, babynames\nsecond argument name new column decade followed want column - decade.\ncreating decades using code `floor(year/10)*10. seems complicated says take year divide 10, get rid decimal places, multiply 10. example, 1945/10 = 194.5, get rid decimal places becomes 194, multiply 10 gives 1940s!.\n, mutating new column onto data storing object baby_decadesthe first argument original data, babynamesthe second argument name new column decade followed want column - decade.creating decades using code `floor(year/10)*10. seems complicated says take year divide 10, get rid decimal places, multiply 10. example, 1945/10 = 194.5, get rid decimal places becomes 194, multiply 10 gives 1940s!.start data look something like new column called decade mutated :mutates can much simpler like example . look code answer question :code create new object storing tibble original data new column called country states USA row. mutate() adds already unless add select() filter() remove columns rows. Note one answers wrong states usa lowercase code states uppercase, .e. USA. Remember specific.","code":"\nbaby_decades <- mutate(.data = babynames,\n                  decade = floor(year/10) *10)\nbaby_decades\nbaby_where <- mutate(.data = babynames,\n                  country = \"USA\")"},{"path":"data-wrangling-1.html","id":"dw1-a8","chapter":"4 Data wrangling 1","heading":"4.3.5 Activity 8: Grouping and summarising","text":"Brilliant! learning much changing data just one important step go. goal wrangling quantitative analysis summarise data somehow! Perhaps want calculate mean median, sum total data. can perform operations using function summarise().First, use object dat just data four girls names, Alexandra, Beverly, Emily, Kathleen. Type run code new code chunk dat already exist environment pane:Now start , going calculate total number babies across years given one four names.Type run code new code chunk\nsummarise() function use create summaries data\nper usual first argument object want use, case dat\nsecond argument name summary column, case total, summary value want create, case sum, using sum(), values n.\nsummarise() function use create summaries dataas per usual first argument object want use, case datthe second argument name summary column, case total, summary value want create, case sum, using sum(), values n.top tip coding get habit translating code full sentences make easier figure happening. can read code \"run function summarise() using data object dat create new variable named total result adding numbers column n store dat_sum\". look dat_sum see output:see total 2161374 babies four names dataset! learn summary functions go along, summarise() becomes even powerful combined final dplyr function, group_by(). Quite often, want produce summary statistics broken groups, examples, scores participants different conditions, reading time native non-native speakers group_by() helps us .two ways can use group_by(). First, can create new, grouped object . saying use group_by() function group data dat based categories variable name.Type run code new code chunk.look object viewer, look different original dat, however, underlying structure changed - can see typing group_dat console window running . says number groups secpnd line output - \"# Groups: name [4]\"However, run summarise code , now using new group_dat object look output.Type run code new code chunksummarise() performed exactly operation - adding total number column n - time done separately group, case variable name gives output looks like :\nget looks like error says summarise() ungrouping output (override .groups argument)worry, error just R telling done groups created become obvious examples later chapters.\ntwo final examples show can request multiple summary calculations performed function. example, following code calculates mean median number babies given name every year.can also add multiple grouping variables. example, following code groups baby_decades sex decade calculates summary statistics give us mean median number male female babies decade.look sum_decades, first lines look something like :Excellent! now run Wickham six functions allow us arrange, select, filter, mutate, group summarise data!","code":"\ndat <- babynames %>% \n  filter(name %in% c(\"Emily\",\n                     \"Kathleen\",\n                     \"Alexandra\",\n                     \"Beverly\"), sex == \"F\")\ndat_sum <- summarise(.data = dat,\n                     total = sum(n))\ngroup_dat <- group_by(.data = dat,\n                      name) \ngroup_sum <- summarise(.data = group_dat, \n                       total = sum(n)) \nsum_multi <- summarise(group_dat,\n                       mean_year = mean(n),\n                       median_year = median(n))\ngroup_decades <- group_by(baby_decades, \n                          sex, \n                          decade)\n\nsum_decades <- summarise(group_decades,\n                         mean_year = mean(n),\n                         median_year = median(n))"},{"path":"data-wrangling-1.html","id":"dw1-a9","chapter":"4 Data wrangling 1","heading":"4.4 Introducing Pipes","text":"final activity chapter essentially repeats already covered slightly different way. previous activities, created new objects new variables groupings called summarise() new objects separate lines code. result, multiple objects environment pane need make sure keep track different names.Instead, can use pipes. Pipes written %>% can read \"\". Pipes allow string together 'sentences' code 'paragraphs' need create intermediary objects. Really, something easier show tell.code similar code wrote creates one object., just note, may see warning run code regarding groups - similar previous time saw message just letting know output grouped . Nothing worry basically. , good practice, look output code, first lines gives us:Now just explain little , reason function, %>%, called pipe 'pipes' data next function. wrote code previously, first argument function dataset wanted work . use pipes automatically take data previous line code need specify .\nRead pipe like paragraph\n\nlearning code can useful practice read code 'loud' full sentences help understand . can read code \"starting babynames, create new variable called decade keep names Emily, Kathleen, Alexandra Beverly belong female babies, group dataset name decade calculate mean number babies name per decade.\" Try time write new bit code find code becomes much easier follow\npeople find pipes bit tricky understand conceptual point view, however, well worth learning use code starts getting longer much efficient mean write less code always good thing!","code":"\npipe_summary <- babynames %>%\n  mutate(decade = floor(year/10) *10) %>%\n  filter(name %in% c(\"Emily\",\n                     \"Kathleen\",\n                     \"Alexandra\",\n                     \"Beverly\"), sex==\"F\") %>%\n  group_by(name, \n           decade) %>%\n  summarise(mean_decade = mean(n))## `summarise()` has grouped output by 'name'. You can override using the `.groups` argument."},{"path":"data-wrangling-1.html","id":"dw1-fin","chapter":"4 Data wrangling 1","heading":"4.5 Finished!","text":"Brilliant! lot information hopefully started give sense approaches data wrangling main functions use get deeper book!","code":""},{"path":"data-wrangling-1.html","id":"ld-test","chapter":"4 Data wrangling 1","heading":"4.6 Test yourself","text":"","code":""},{"path":"data-wrangling-1.html","id":"knowledge-questions-1","chapter":"4 Data wrangling 1","heading":"4.6.1 Knowledge Questions","text":"following one Wickham Six functions?\n\nmelt()arrange()mutate()filter()\nfollowing functions use wanted keep certain columns?\n\nselect()arrange()mutate()filter()\nfollowing functions use wanted keep certain rows?\n\nselect()arrange()mutate()filter()\nfollowing functions use wanted add new column information?\n\nselect()arrange()mutate()filter()\nboolean expression add filter() function keep Male babies original babynames data?\n\nsex == Fsex == Msex < FSex == M\nmelt() function Wickham six functions. actually function one tend use.select() function keeping removing columns.filter() function keeping removing rows.mutate() function adding new columns.Assuming original data changed, \"sex == M\" work \"Sex == M\" column called Sex uppercase S. Remember exact.","code":""},{"path":"data-wrangling-1.html","id":"dw1-debugex","chapter":"4 Data wrangling 1","heading":"4.6.2 Debugging exercises","text":"Restart R session (Session >> Restart R). Make sure working directory set right folder run code console window:produce error:figure fix error, make note .indication loaded babynames package library using library() functionRestart R session (Session >> Restart R). Make sure working directory set right folder run code console window:produce error:figure fix error, make note .error saying function called summarise(). know function exists though. done call load function libary using library(tidyverse)Restart R session (Session >> Restart R). Make sure working directory set right folder run code console window:produce error:figure fix error, make note .actually one painful errors can see really help solve issue quite clear means. unexpected symbol means effectively code wrong seeing something think see. clear right? see ask forgotten comma somewhere maybe bracket quotation mark! Look issues see helps. error show error comes babynames mean_n just clear means. issue around last word mentions basically. answer comma missing data arguement; babynames mean_n. line read:incredibly frustrating time consuming error. Watch . partitioning code new lines comma can really help see errors :dastardly error! Well done spotted !!!","code":"\nbabynamesError: object 'babynames' not found\nlibrary(babynames)\n\ndat <- summarise(.data = babynames, mean_n = mean(n))Error in summarise(.data = babynames, mean_n = mean(n)) : \n  could not find function \"summarise\"library(babynames)\nlibrary(tidyverse)\n\ndat <- summarise(.data = babynames mean_n = mean(n))Error: unexpected symbol in \"dat <- summarise(.data = babynames mean_n\"\n\nlibrary(babynames)\nlibrary(tidyverse)\n\ndat <- summarise(.data = babynames, mean_n = mean(n))\nlibrary(babynames)\nlibrary(tidyverse)\n\ndat <- summarise(.data = babynames, \n                 mean_n = mean(n))"},{"path":"data-wrangling-1.html","id":"words-from-this-chapter-3","chapter":"4 Data wrangling 1","heading":"4.7 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.End ChapterThat end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"data-wrangling-2.html","id":"data-wrangling-2","chapter":"5 Data wrangling 2","heading":"5 Data wrangling 2","text":"said previously, one key aspects researcher's toolbox knowledge skill work data - regardless comes . fact, run experiment might get lots different types data various different files. instance, uncommon experimental software create new file every participants run participant's file contain numerous columns rows different data types, important. able wrangle data, manipulate different layouts, extract parts need, summarise , one important skills help learn book.last chapter introduced Wickham six one-table functions use data wrangling. course book reiterate functions skills, packages like tidyverse packages contains, across number different datasets give wide range exposure Psychology , reiterate skills apply across different datasets. Always remember, whilst data changes, skills stay !also started introduce different types data data structures. example, data often stored two-dimensional tables, either called data frames, tables, tibbles. ways storing data discover time mainly, book, using tibbles, mentioned previous chapter (like info, type vignette(\"tibble\") console). tibble really just table data columns rows information - additional features learn - within cells tibble, cell row column meets, get different types data, e.g. double, integer, character factorNote 1: Double Integer can referred numeric data, see word time time. clarity, use Double term number can take decimal (e.g. 3.14) Integer term whole number (decimal, e.g. 3).Note 2: wee bit confusingly Double data decimal places . instance, value 1 Double well Integer. However, value 1.1 Double never Integer. Integers decimal places. , work data make sense, highlights importance looking data checking type type determines can data.Today, going continue developing understanding data, build knowledge skills seen point, help develop ability work data. difference chapter going ask wrangling , based shown previously. build skills analyse novel dataset activities based around manipulating data. Remember pro-active learning refer back done , using example code guide. solutions tasks bottom page well check , sure try tasks first. also online cheatsheets need help - key cheatsheet chapter Data Transformation dplyr - forget ask questions forums.","code":""},{"path":"data-wrangling-2.html","id":"learning-to-wrangle-is-there-a-chastity-belt-on-perception","chapter":"5 Data wrangling 2","heading":"5.1 Learning to wrangle: Is there a chastity belt on perception","text":"Today going using data paper Chastity Belt Perception(Witt et al., 2018). can read full paper like summarise paper . paper asks research question ability perform action influence perception? instance, ability hit tennis ball influence fast perceive ball moving? phrase another way, expert tennis players perceive ball moving slower novice tennis players?experiment use tennis players however. Instead used Pong task: \"computerised game participants aim block moving balls various sizes paddles\". bit like classic retro arcade game. Participants tend estimate balls moving faster block smaller paddle opposed bigger paddle. can read paper get details wish hopefully gives enough idea help understand wrangling data. cleaned data little start . begin!","code":""},{"path":"data-wrangling-2.html","id":"dw2-a1","chapter":"5 Data wrangling 2","heading":"5.1.1 Activity 1: Set-up Data Wrangling 2","text":"First, download PongBlueRedBack 1-16 Codebook.csv chapter folder.\ntrouble downloading .csv files directly may prefer download data zip file unzip link: PongBlueRedBack 1-16 Codebook.zip. just unzip chapter folder.\ntrouble downloading .csv files directly may prefer download data zip file unzip link: PongBlueRedBack 1-16 Codebook.zip. just unzip chapter folder.Next, set working directory chapter folder. Ensure environment clear.\nusing Rserver, avoid number issues restarting session - click Session - Restart R\nusing Rserver, avoid number issues restarting session - click Session - Restart RNow, open new R Markdown document save working directory call file something informative like \"DataWrangling2\".Next, delete default R Markdown welcome text insert new code chunk.Finally, copy paste code code chunk run code.Remember:  use read_csv() function load data, data filename must ) quotation marks b) spelt exactly filename states, including spaces file extension (case .csv)done task correctly see following output:summary(), base package - default packages automatically installed - provides quick overview variables dataset can useful quick check indeed imported correct data. also provide basic descriptive statistics information whether data character (text) data can also useful check.alternative approach looking data types use glimpse() function, dplyr package, loaded tidyverse. Try following console window:see:look table can see eight column names followed <dbl>, short double, <chr> short character. , might mean much progress become adept recognising type data working , type data changes can data.","code":"\nlibrary(\"tidyverse\")\npong_data <- read_csv(\"PongBlueRedBack 1-16 Codebook.csv\")\nsummary(pong_data)##   Participant     JudgedSpeed      PaddleLength   BallSpeed    TrialNumber    \n##  Min.   : 1.00   Min.   :0.0000   Min.   : 50   Min.   :2.0   Min.   :  1.00  \n##  1st Qu.: 4.75   1st Qu.:0.0000   1st Qu.: 50   1st Qu.:3.0   1st Qu.: 72.75  \n##  Median : 8.50   Median :1.0000   Median :150   Median :4.5   Median :144.50  \n##  Mean   : 8.50   Mean   :0.5471   Mean   :150   Mean   :4.5   Mean   :144.50  \n##  3rd Qu.:12.25   3rd Qu.:1.0000   3rd Qu.:250   3rd Qu.:6.0   3rd Qu.:216.25  \n##  Max.   :16.00   Max.   :1.0000   Max.   :250   Max.   :7.0   Max.   :288.00  \n##  BackgroundColor      HitOrMiss       BlockNumber   \n##  Length:4608        Min.   :0.0000   Min.   : 1.00  \n##  Class :character   1st Qu.:0.0000   1st Qu.: 3.75  \n##  Mode  :character   Median :1.0000   Median : 6.50  \n##                     Mean   :0.6866   Mean   : 6.50  \n##                     3rd Qu.:1.0000   3rd Qu.: 9.25  \n##                     Max.   :1.0000   Max.   :12.00\nglimpse(pong_data)## Rows: 4,608\n## Columns: 8\n## $ Participant     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n## $ JudgedSpeed     <dbl> 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, ~\n## $ PaddleLength    <dbl> 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 50, 250, ~\n## $ BallSpeed       <dbl> 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7, 2, 5, ~\n## $ TrialNumber     <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~\n## $ BackgroundColor <chr> \"red\", \"blue\", \"red\", \"red\", \"blue\", \"blue\", \"red\", \"r~\n## $ HitOrMiss       <dbl> 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, ~\n## $ BlockNumber     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~"},{"path":"data-wrangling-2.html","id":"dw2-a2","chapter":"5 Data wrangling 2","heading":"5.1.2 Activity 2: Look at your data","text":"Great! Now data loaded , look pong_data see organized. can various ways, today, click pong_data environment pane.dataset see row (observation) represents one trial per participant 288 trials 16 participants. columns (variables) dataset follows:Just saw glimpse(), data double - .e. numbers - data character - .e. text. use data master skills Wickham Six verbs, taking verb turn. refer explanations example code previous chapter help complete . Remember six main functions :Now, wrangling!!! forget, can help take new code chunk activity write notes code chunks!","code":""},{"path":"data-wrangling-2.html","id":"dw2-a3","chapter":"5 Data wrangling 2","heading":"5.1.3 Activity 3: select()","text":"going start selections!Either inclusion (stating variables want keep) exclusion (stating variables want drop), select Participant, PaddleLength, TrialNumber, BackgroundColor HitOrMiss columns pong_data store new object named select_dat.","code":""},{"path":"data-wrangling-2.html","id":"dw2-a4","chapter":"5 Data wrangling 2","heading":"5.1.4 Activity 4: Reorder the variables","text":"Good work! Now, previously mentioned select() can also used reorder columns tibble, new tibble display variables order entered .Use select() keep columns Participant, JudgedSpeed, BallSpeed, TrialNumber, HitOrMiss pong_data display alphabetical order, left right. Save tibble new object named reorder_dat.","code":""},{"path":"data-wrangling-2.html","id":"dw2-a5","chapter":"5 Data wrangling 2","heading":"5.1.5 Activity 5: arrange()","text":"now master selections!!! move different skill test ability change order data rows.Arrange data following two variables: HitOrMiss (putting hits - 1 - first), JudgedSpeed (putting fast judgement - 1 - first) store object named arrange_dat.","code":""},{"path":"data-wrangling-2.html","id":"dw2-a6","chapter":"5 Data wrangling 2","heading":"5.1.6 Activity 6: filter()","text":"Great! keeping removing rows filter()! may need refer back different boolean operations complete activity!Use filter() extract Participants original pong_data :fast speed judgement;speeds 2, 4, 5, 7;missed ball.Store remaining data new object called pong_fast_missThere three parts filter best think individually combine .Filter fast speed judgements (JudgedSpeed)Filter fast speed judgements (JudgedSpeed)Filter speeds 2, 4, 5 7 (BallSpeed)Filter speeds 2, 4, 5 7 (BallSpeed)Filter Misses (HitOrMiss)Filter Misses (HitOrMiss)three filters one uses output preceding one, remember filter functions can take one argument. may also need use == instead just =.\nfilter function useful used wrongly can give misleading findings. important always check data perform action. say working comparative psychology run study looking cats, dogs horses perceive emotion. say data stored tibble animal_data column called animals tells type animal participant . Imagine wanted data just cats:\n\nfilter(animal_data, animals == \"cat\")\n\nExactly! wanted cats dogs?\n\nfilter(animal_data, animals == \"cat\", animals == \"dog\")\n\nRight? Wrong! actually says \"give everything cat dog\". nothing cat dog, weird - like dat cog. fact want everything either cat dog, :\n\nfilter(animal_data, animals == \"cat\"| animals == \"dog\")\n\n:\n\nfilter(animal_data, animals %% c(\"cat\", \"dog\"))\n\nused last approach, using %%, producing graph babynames. helpful function forget exists!\n","code":""},{"path":"data-wrangling-2.html","id":"dw2-a7","chapter":"5 Data wrangling 2","heading":"5.1.7 Activity 7: mutate()","text":"Brilliant work getting far! really starting get hang wrangling! Now, test skills mutate() first want introduce new function help understand can mutate().Previously learned mutate() function lets us create new variable dataset. However, also another useful function can combined recode() create new columns recoded values - change different categories variable represented. example, code adds new column pong_data judged speed converted text label 1 become Fast, 0 become \"Slow\". Run code look output!code bit complicated explain:JudgedSpeedLabel name new column,JudgedSpeed name old column one take information \nNote gave recoded variable name original overwrite .\nNote gave recoded variable name original overwrite .Fast Slow new codings 1 0 respectively new columnThe mutate() function also handy making calculations across columns data. example, say realise made mistake experiment participant numbers 1 higher every participant, .e. Participant 1 actually numbered Participant 2, etc. something like:Note , second example, giving new column name old column Participant. , happens overwriting old data new data! watch , mutate can create new column overwrite existing column, depending tell !Ok great! Now, imagine realise mistake dataset trial numbers wrong. first trial (trial number 1) practice excluded experiment actually started trial 2. turn! can either following activity two separate steps create new object time, can uses pipes %>% one line code. final tibble stored object called pong_data_renumbered.Filter trials number 1 (TrialNumber column) pong_data.use mutate() function renumber remaining trial numbers, starting one instead two, overwriting values TrialNumber column.Step 1. filter(TrialNumber equal 1). Remember store output variable using pipes.Step 1. filter(TrialNumber equal 1). Remember store output variable using pipes.Step 2. mutate(TrialNumber = TrialNumber minus 1)Step 2. mutate(TrialNumber = TrialNumber minus 1)","code":"\npong_data_mut1 <- mutate(pong_data, \n                    JudgedSpeedLabel = recode(JudgedSpeed, \n                                                    \"1\" = \"Fast\", \n                                                    \"0\" = \"Slow\"))\npong_data_mut2 <- mutate(pong_data, \n                         Participant = Participant + 1)"},{"path":"data-wrangling-2.html","id":"dw2-a8","chapter":"5 Data wrangling 2","heading":"5.1.8 Activity 8: Summarising data","text":"Excellent! now done wrangling want calculate descriptive statistics data using summarise(). summarise() range internal functions make life really easy, e.g. mean, sum, max, min, etc. See dplyr cheatsheet examples. additional one use time--time na.rm = TRUE can add calculating descriptive statistics say missing values. Missing values often appear NA job na.rm say whether remove (rm) NAs (na.rm = TRUE) (na.rm = FALSE). try calculate values data NAs, mean, return NA result know average nothing. dataset missing values show use try remember argument exists, use often save lot time!Back activity. , using data pong_data_renumbered calculate:total (sum) number hits combination background colour paddle length.mean number hits combination background colour paddle lengthRemember though, want produce descriptive statistics groups (background colour paddle length), two steps:First group data BackgroundColor PaddleLength using group_by()., use summarise() calculate total mean number hits (HitOrMiss) using grouped dataWe activity using pipes reduce amount code write. Remember try read code loud pronounce %>% ''. Copy paste code new code chunk run code.\nRemember, get looks like error says \"summarise() grouped output 'BackgroundColor'. can override using .groups argument.\", worry, error just code telling final object grouped.\nView pong_data_hits answer following questions see completed task correctly.View pong_data_hits answer following questions see completed task correctly.total number hits made small paddle (50) blue colour background?\n\n52751610591057\ntotal number hits made small paddle (50) blue colour background?three decimal places, mean number hits made small paddle (50) blue colour background?\n\n0.9220.920.4510.459\nthree decimal places, mean number hits made small paddle (50) blue colour background?Note:name column within pong_data_hits total_hits; called code. called anything wanted always try use something sensible.Make sure call variables something (anyone looking code) understand recognize later (.e. variable1, variable2, variable3. etc.), avoid spaces (use_underscores_never_spaces).","code":"\npong_data_hits <- pong_data_renumbered %>% \n  group_by(BackgroundColor, \n           PaddleLength) %>% \n  summarise(total_hits = sum(HitOrMiss, \n                             na.rm = TRUE),\n            meanhits = mean(HitOrMiss, \n                            na.rm = TRUE))## `summarise()` has grouped output by 'BackgroundColor'. You can override using the `.groups` argument."},{"path":"data-wrangling-2.html","id":"ungrouping-counting-pipes---quick-notes","chapter":"5 Data wrangling 2","heading":"5.2 Ungrouping, counting, pipes - quick notes!","text":"done superb work today! Congratulations. really proud . ending just want show couple new functions approaches help future activities.","code":""},{"path":"data-wrangling-2.html","id":"dw2-a9","chapter":"5 Data wrangling 2","heading":"5.2.1 Counting data","text":"First want look different ways counting observations. Often helpful know many observations , either total, broken groups. can help spot something gone wrong calculation, example, done something code mean median calculated using subset values intended.two ways counting number observations. first uses summarise() function n() within summarise(). example, code previous activity, extra line add column called n table contains number observations group. useful want add column counts table descriptive statistics.Note: function n() takes arguments just left blank shown, n(). However, functions, n prior = called anything want column called, e.g. n = n() number = n() just give different names column.However, just interested counts rather also calculating descriptives, method bit clunky. Instead, can use function count() specifically designed count observations require use summarise() group_by().count total number observations dataset example :give answer :Alternatively, count number observations level variable :give answer :method use depend whether want add counts table descriptives, functions useful know.","code":"\npong_count <- pong_data_renumbered %>% \n  group_by(BackgroundColor, \n           PaddleLength) %>% \n  summarise(total_hits = sum(HitOrMiss, \n                             na.rm = TRUE),\n            meanhits = mean(HitOrMiss, \n                            na.rm = TRUE),\n            n = n())\npong_data %>% # take pong_data\n  count() # and then count the observations in it\npong_data %>%\n  count(BackgroundColor)"},{"path":"data-wrangling-2.html","id":"dw2-ungroup","chapter":"5 Data wrangling 2","heading":"5.2.2 Ungrouping data","text":"grouping data together using group_by() function performing task , e.g. filter(), can good practice ungroup data performing another function - piping ungroup() function - related warnings summarise() mean changing groupings removing groupings can good . Forgetting ungroup dataset always affect processing, can really mess things. just good reminder always check data getting function ) makes sense b) expect.example might use function:","code":"\npong_data_ungroup <- pong_data %>%\n  group_by(BackgroundColor, \n           PaddleLength) %>%\n  summarise(total_hits = sum(HitOrMiss)) %>%\n  ungroup"},{"path":"data-wrangling-2.html","id":"dw2-pipes","chapter":"5 Data wrangling 2","heading":"5.2.3 Recapping Pipes (%>%)","text":"finally today just want throw quick recap pipes. used pipes little might still got hang reading help little. Remember pipes become useful string series functions together, rather using separate steps save data time new variable name getting confused. non-piped code create new object time, example, data, data_filtered, data_arranged, data_grouped, data_summarised just get final one actually want. creates lot variables tibbles environment can make everything unclear, difficult follow, eventually slow computer. Piped code however uses one variable name, saving space environment, clear easy read. pipes skip unnecessary steps avoid cluttering environment.example code use pipes find many hits large paddle length red background:First group data accordingly, storing pong_data_groupAnd summarise , storing answer total_hitsAnd finally filter just red, small paddle hitsAlternatively can make code even efficient, using less code, stringing sequence functions together using pipes. look like:code becomes easier read remember data goes one function \"\" another, \"\" another, .also worth remembering whilst pipes code can written single line, much easier see pipe function takes line. just remember every time add function pipeline, add %>% first note using separate lines function, %>% must appear end line start next line. Compare two examples . first work second second puts pipes end line need !","code":"\npong_data_group <- group_by(pong_data, \n                            BackgroundColor, \n                            PaddleLength)\n\npong_data_hits <- summarise(pong_data_group, \n                            total_hits = sum(HitOrMiss))\n\npong_data_hits_red_small <- filter(pong_data_hits, \n                                   BackgroundColor == \"red\", \n                                   PaddleLength == 250)\npong_data_hits_red_small <- pong_data %>% \n  group_by(BackgroundColor, \n           PaddleLength) %>% \n  summarise(total_hits = sum(HitOrMiss)) %>% \n  filter(BackgroundColor == \"red\", \n         PaddleLength == 250) # Piped version that won't work \ndata_arrange <- pong_data \n                %>% filter(PaddleLength == \"50\")\n                %>% arrange(BallSpeed) \n\n# Piped version that will work \ndata_arrange <- pong_data %>%\n                filter(PaddleLength == \"50\") %>%\n                arrange(BallSpeed) "},{"path":"data-wrangling-2.html","id":"dw2-fin","chapter":"5 Data wrangling 2","heading":"5.3 Finished!","text":"Brilliant work ! now learned number functions verbs need progress book. use next chapter sure go try make comfortable . questions please post Teams. Happy Wrangling!","code":""},{"path":"data-wrangling-2.html","id":"dw2-test","chapter":"5 Data wrangling 2","heading":"5.4 Test yourself","text":"","code":""},{"path":"data-wrangling-2.html","id":"knowledge-questions-2","chapter":"5 Data wrangling 2","heading":"5.4.1 Knowledge Questions","text":"type data likely :Male = CharacterNumericIntegerMale = CharacterNumericInteger7.15 = CharacterDoubleInteger7.15 = CharacterDoubleInteger137 = CharacterDoubleInteger137 = CharacterDoubleIntegerThere lot different types data well different types levels measurements can get confusing. important try remember can certain types analyses certain types data certain types measurements. instance, take average Characters just like take average Categorical data. Likewise, can maths Double data, just like can Interval Ratio data. Integer data funny sometimes Ordinal sometimes Interval, sometimes take median, sometimes take mean. main point always know type data using think can .Note last answer, 137, also double clear take decimal .Wickham Six use sort columns smallest largest:\n\nselectfiltermutatearrangegroup_bysummarise\nWickham Six use calculate mean column:\n\nselectfiltermutatearrangegroup_bysummarise\nWickham Six use remove certain observations - e.g. remove males:\n\nselectfiltermutatearrangegroup_bysummarise\nline code say? data %>% filter() %>% group_by() %>% summarise():\n\ntake data group filter summarise ittake data filter group summarise ittake data summarise filter group ittake data group summarise filter \n","code":""},{"path":"data-wrangling-2.html","id":"dw2-debug","chapter":"5 Data wrangling 2","heading":"5.4.2 Debugging tips","text":"debugging challenges today done lot work tips keep mind.Remember run library just write codeMake sure spelt data file name exactly shown. Spaces everything. change name csv file, fix code instead. different name file someone else code reproducible.Remember uploading data use read_csv() underscore, whereas data file dot name, filename.csv.Check datafile actually folder set working directory.Remember start new session time start new analysis - functions packages use great chance conflicting functions.Watch spelling functions remember put data first. Often people forget include data focussing want .Always look output functions build code. Often code runs think wrote code wrong. Code tell !using pipes, remember need data , start.separating pipes across different lines, remember line ends pipe, start pipe.lastly, remember data changes, skills stay . Always look back code working go .","code":""},{"path":"data-wrangling-2.html","id":"dw2-sols","chapter":"5 Data wrangling 2","heading":"5.5 Solutions to Activities","text":"find solutions questions. look giving questions good try speaking tutor issues.","code":""},{"path":"data-wrangling-2.html","id":"dw2-a3sol","chapter":"5 Data wrangling 2","heading":"5.5.1 Activity 3","text":"include variables name variables want:contrast, exclude variables name ones want put - prior name:","code":"\nselect_dat <- select(pong_data, \n                     Participant, \n                     PaddleLength, \n                     TrialNumber, \n                     BackgroundColor, \n                     HitOrMiss)\nselect_dat <-select(pong_data, \n                    -JudgedSpeed, \n                    -BallSpeed, \n                    -BlockNumber)"},{"path":"data-wrangling-2.html","id":"dw2-a4sol","chapter":"5 Data wrangling 2","heading":"5.5.2 Activity 4","text":"reorder variables using select() state order variables want . Note works column order.","code":"\nreorder_dat <- select(pong_data, \n                      BallSpeed, \n                      HitOrMiss, \n                      JudgedSpeed, \n                      Participant, \n                      TrialNumber)"},{"path":"data-wrangling-2.html","id":"dw2-a5sol","chapter":"5 Data wrangling 2","heading":"5.5.3 Activity 5","text":"arrange rows certain columns use arrange() need remember include desc() change running smallest--largest largest--smallest.","code":"\narrange_dat <- arrange(pong_data, \n                       desc(HitOrMiss), \n                       desc(JudgedSpeed))"},{"path":"data-wrangling-2.html","id":"dw2-a6sol","chapter":"5 Data wrangling 2","heading":"5.5.4 Activity 6","text":"Remembering filters can take one argument code filter data :Remembering filters can take one argument code filter data :JudgedSpeed 1 ANDJudgedSpeed 1 ANDBallSpeed either 2, 4, 5, 7, ANDBallSpeed either 2, 4, 5, 7, ANDHitOrMiss 0.HitOrMiss 0.","code":"\npong_fast_miss <- filter(pong_data, \n                         JudgedSpeed == 1, \n                         BallSpeed %in% c(\"2\", \"4\", \"5\", \"7\"), \n                         HitOrMiss == 0)"},{"path":"data-wrangling-2.html","id":"dw2-a7sol","chapter":"5 Data wrangling 2","heading":"5.5.5 Activity 7","text":"use pipes following code appropriate:First filter. remember can call first object pong_data_filt anything makes sense others.separate step mutate.used pipes solution:Remember include data , pipes end line, start line.also written - find better remember data goes first :","code":"\npong_data_filt <- filter(pong_data, \n                         TrialNumber >= 2) \n\npong_data_renumbered <- mutate(pong_data_filt, \n                               TrialNumber = TrialNumber - 1)\npong_data_renumbered <- filter(pong_data, \n                               TrialNumber >= 2) %>%\n  mutate(TrialNumber = TrialNumber - 1)\npong_data_renumbered <- pong_data %>%\n  filter(TrialNumber >= 2) %>%\n  mutate(TrialNumber = TrialNumber - 1)"},{"path":"data-wrangling-2.html","id":"words-from-this-chapter-4","chapter":"5 Data wrangling 2","heading":"5.6 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"data-wrangling-3.html","id":"data-wrangling-3","chapter":"6 Data Wrangling 3","heading":"6 Data Wrangling 3","text":"last chapter, looked using one-table Wickham verbs filter, arrange, group_by, select, mutate summarise pong data. also learnt little pipes saw quick counting ungrouping. sure try activities moving start add functions allow us skills data wrangling.Today, progression working one table/tibble, focus working data across two tibbles going add two functions, skills already know. :pivot_longer() allows us transform tibble wide format long format.inner_join() allows us combine two tibbles together based common columns. actually saw function Chapter 3 might recap.just recap, still struggling idea function, remember function takes input, performs action, gives output. Think toaster like mentioned : takes bread input; performs action heating ; gives output, toast. good thing lot functions use nicely named verbs describe - mutate() mutates (adds column); arrange() arranges columns, summarise() summarises, etc. keep mind know memorise different functions; practice repetition quickly learn remember ones package come . Sort like find spoons kitchen - look fridge, washing machine, drawer. Nope, learnt, repetition, look drawer first time. functions. Keep mind research methods like language use work makes sense. , practicing!","code":""},{"path":"data-wrangling-3.html","id":"tidy-data","chapter":"6 Data Wrangling 3","heading":"6.1 Tidy data","text":"first little data structure organisation.book, use type data organisation known tidy data. data format easily processed tidyverse package. However, data work always start formatted efficient way possible. happens first step put Tidy Data format. two fundamental principles defining Tidy Data:variable must column.observation must row.Tidy Data (Wickham, 2014) adds following principle:type observation unit forms table.Grolemund Wickham (2017) restate third principle :value must cell (.e. grouping two variables together, e.g. time/date one cell).cell specific row column meet; single data point tibble cell example. Grolemund Wickham (2017) book useful read free, browsing chapter Tidy Data help visualise want arrange data. Try keep principles mind whilst .\nworked kind data , particularly used Excel, likely used wide format long format data. wide format, participant's data one row multiple columns different data points. means data set tends wide many rows participants.\n\nLong format row single observation, typically single trial experiment response single item questionnaire. multiple trials per participant, multiple rows participant. identify participants, need variable kind participant id, can simple distinct integer value participant. addition participant identifier, measurements taken observation (e.g., response time) experimental condition observation taken .\n\nwide format data, row corresponds single participant, multiple observations participant spread across columns. instance, survey data, separate column survey question.\n\nTidy mix approachs functions tidyverse assume tidy format, typically first thing need get data, particularly wide-format data, reshape wrangling. teach really important skills.\n\ninformation tidy data available .\n","code":""},{"path":"data-wrangling-3.html","id":"analysing-the-autism-spectrum-quotient-aq","chapter":"6 Data Wrangling 3","heading":"6.2 Analysing the Autism Spectrum Quotient (AQ)","text":"continue building data wrangling skills chapter tidy data Autism Spectrum Quotient (AQ) questionnaire. AQ10 non-diagnostic short form AQ 10 questions per participant. discrete scale higher participant scores AQ10 autistic-like traits said display. person scoring 6 recommended diagnosis. can see example AQ10 link: AQ10 Example.Today's GoalToday working data 66 participants goal chapter find AQ score data-wrangling skills.four data files work download chapter folder. can either right click save file separately , click download files zip file unzip:responses.csv containing AQ survey responses 10 questions 66 participantsqformats.csv containing information question coded - .e. forward reverse codingscoring.csv containing information many points specific response get; depending whether forward reverse codedpinfo.csv containing participant information Age, Sex importantly ID number.\nuse .csv\n\ncsv stands 'comma separated variable', basic way storing transferring data. really just stores numbers text nothing else. great thing basic can read many different machines need expensive licenses open .\nnow series eight activities build data wrangling skills. Remember try things ,ask questions, solutions bottom chapter stuck.","code":""},{"path":"data-wrangling-3.html","id":"getting-set-up","chapter":"6 Data Wrangling 3","heading":"6.3 Getting Set-Up","text":"first activity getting ready analyse data try steps need help, consult earlier chapters.","code":""},{"path":"data-wrangling-3.html","id":"dw3-a1","chapter":"6 Data Wrangling 3","heading":"6.3.1 Activity 1: Set-up Data Wrangling 3","text":"Open RStudio set working directory chapter folder. Ensure environment clear.\nusing Rserver, avoid number issues restarting session - click Session - Restart R\nusing Rserver, avoid number issues restarting session - click Session - Restart ROpen new R Markdown document save working directory. Call file \"DataWrangling3\".Make sure downloaded four .csv files saved chapter folder. Remember change file names data.csv data (1).csv.Delete default R Markdown welcome text insert new code chunk loads package tidyverse using library() function. Remember solutions needed.","code":""},{"path":"data-wrangling-3.html","id":"loading-in-data","chapter":"6 Data Wrangling 3","heading":"6.4 Loading in Data","text":"Now need load .csv data files using read_csv() function save tibbles environment. example, load responses.csv file save object responses, type:","code":"\nresponses <- read_csv(\"responses.csv\") "},{"path":"data-wrangling-3.html","id":"dw3-a2","chapter":"6 Data Wrangling 3","heading":"6.4.1 Activity 2: Load in the data","text":"Add following lines code RMarkdown new code chunk complete load four .csv data files. Use code example name object original file name (minus .csv part), , e.g. responses.csv gets saved responses. Remember run lines data loaded stored environment.","code":"responses <-  read_csv()    # load in survey responses\nqformats <-                 # load in question formats\nscoring <-                  # load in scoring info\npinfo <-                    # load in participant information"},{"path":"data-wrangling-3.html","id":"looking-at-data-1","chapter":"6 Data Wrangling 3","heading":"6.5 Looking at Data","text":"Now data loaded always best look get idea layout. showed number ways glimpse() View() functions. Remember put Console window put name data brackets see arranged. add Markdown file, just trying things Console window","code":""},{"path":"data-wrangling-3.html","id":"dw3-a3","chapter":"6 Data Wrangling 3","heading":"6.5.1 Activity 3: Look at your data","text":"look data responses see think Tidy answer following question:data responses format?\n\nTidyLongWide\nreponses tibble far tidy; row represents multiple observations participant, .e. row shows responses multiple questions number rows participants (66)- wide format. Remember want data tidy format described .Eh, remind tibble?Remember, tibble simply dataframe - table data columns rows - really handy working using tidyverse package. say tibble, can think dataframe rows columns information numbers stored - like responses, tibble. info, see : Tibbles.","code":""},{"path":"data-wrangling-3.html","id":"gathering-your-data","chapter":"6 Data Wrangling 3","heading":"6.6 Gathering Your Data","text":"now data need loaded , order make easier us get AQ score participant, need change layout responses tibble Tidy Data.","code":""},{"path":"data-wrangling-3.html","id":"dw3-a4","chapter":"6 Data Wrangling 3","heading":"6.6.1 Activity 4: Gathering with pivot_longer()","text":"first step use function pivot_longer() transform data. pivot functions can easier show tell - may find useful exercise run code compare tibble newly created object rlong tibble original object, respones, reading .break code little help understand :tidyverse functions, first argument specifies dataset use base, case responses.\nremember experienced confident get write argument names, e.g. data =.\nremember experienced confident get write argument names, e.g. data =.second argument, cols specifies columns want transform. easiest way visualise think columns new long-form dataset change. case, single column Id remain constant transform columns contain participant's responses question.\ncolon notation first_column:last_column used select variables first column specified second column specified. code, cols specifies columns want transform Q1 Q10.\nNote \"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.\ncolon notation first_column:last_column used select variables first column specified second column specified. code, cols specifies columns want transform Q1 Q10.Note \"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.third fourth arguments names new columns creating.\nnames_to specifies names new columns created.\nFinally, values_to names new column contain measurements, case call Response.\nnew column names put quotes already exist tibble. always case case function.\nNote also names anything using names code makes sense.\nLastly, need write names_to = ... values_to = ... otherwise columns created correctly.\nnames_to specifies names new columns created.Finally, values_to names new column contain measurements, case call Response.new column names put quotes already exist tibble. always case case function.Note also names anything using names code makes sense.Lastly, need write names_to = ... values_to = ... otherwise columns created correctly.now run code explained bit, may find helpful go back compare rlong responses see argument matches output table.","code":"\nrlong <- pivot_longer(data = responses,\n                      cols = Q1:Q10,\n                      names_to = \"Question\", \n                      values_to = \"Response\")"},{"path":"data-wrangling-3.html","id":"joining-your-data","chapter":"6 Data Wrangling 3","heading":"6.7 Joining Your Data","text":"Now responses data tidy format, closer able calculate AQ score person. However, still need extra information:questions reverse forward scored (.e., strongly agree positive negative response)? information found qformatsHow many points given specific response? information found scoring.typical analysis situation different information different tables need join altogether. pieces information contained qformats scoring respectively, want join responses create one informative tidy table information need. can type join function inner_join(); function combine information two tibbles using column common tibbles.","code":""},{"path":"data-wrangling-3.html","id":"dw3-a5","chapter":"6 Data Wrangling 3","heading":"6.7.1 Activity 5: Combining data","text":"wee bit different approach activity try see get . Remember solutions end chapter.Copy code new code chunk.Next, replace NULL values code necessary variable names join rlong qformats Question. need extra help, revisit Starting Data chapter used function ! Make sure try first.\nHint: join two tibbles (x y) \"common column\".\nHint: join two tibbles (x y) \"common column\".Now look tibble rlong2. can see matched question scoring format, forward reverse.\nforward reverse scoring\n\nlot questionnaires questions Forward scored questions Reverse scored. mean? Imagine situation options replying question : 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. forward-scoring question get 1 point extremely agree, 2 agree, 3 neutral, etc. reverse scoring question get 5 extremely agree, 4 agree, 3 neutral, etc.\n\nreasoning behind shift sometimes agreeing disagreeing might favorable depending question worded. Secondly, sometimes questions used just catch people - imagine two similar questions one reverse meaning . scenario, people respond opposites. respond might paying attention.\nfar good information want bring tibble. now need combine information new tibble, rlong2, scoring tibble know many points attribute question based answer participant gave, whether question forward reverse coded. , can use inner_join() function, time common columns found rlong2 scoring QFormat Response - .e. two common columns.","code":"\nrlong2 <- inner_join(x = NULL, \n                     y = NULL, \n                     by = \"NULL\")"},{"path":"data-wrangling-3.html","id":"dw3-a6","chapter":"6 Data Wrangling 3","heading":"6.7.2 Activity 6: Combining more data","text":", code , going show combine data two common columns. Really combining tibbles corresponding information two tibbles - participants appear tibbles different info tibble want one tibble. called inner-join first information task.can ever join two tables inner_join() . multiple tibbles join like , need multiple calls inner_join().one common column two tibbles joining, best combine columns avoid repeated columns names new tibble. example, run join produces columns named variable.x variable.y means another column datasets add =.careful use c() multiple variables . common error results variable.x/variable.y issue . forget use c() just state different columns, code look first column ignore rest.Now type line new code chunk RMarkdown file, run , view new object. code combines rlong2 scoring based matching data QFormat ResponseYou now created rscores information participant responded question question coded scored, within one tibble. need now sum scores participant get AQ score.","code":"\nrscores <- inner_join(rlong2, \n                      scoring, \n                      c(\"QFormat\", \"Response\"))"},{"path":"data-wrangling-3.html","id":"dw3-a7","chapter":"6 Data Wrangling 3","heading":"6.7.3 Activity 7: Calculating the AQ scores","text":"First, based knowledge last chapter, type line code replace NULLs obtain individual aq_scores participant.\nhint: group_by() - group individual participants?\nhint: summarise() - column sum obtain AQ scores?\nhint: group_by() - group individual participants?hint: summarise() - column sum obtain AQ scores?, save Markdown knit make sure code works.participant grouped Id.summed value Score might get full AQ Score particpipant.Excellent. look aq_scores see individual AQ scores participant!","code":"\naq_scores <- rscores %>% \n             group_by(NULL) %>%\n             summarise(AQ = sum(NULL))"},{"path":"data-wrangling-3.html","id":"pipes-again","chapter":"6 Data Wrangling 3","heading":"6.8 Pipes Again!","text":"now complete code load data, convert Tidy, combine tables calculate AQ score participant. , look , code efficient using pipes. see well understand pipes now!","code":""},{"path":"data-wrangling-3.html","id":"dw3-a8","chapter":"6 Data Wrangling 3","heading":"6.8.1 Activity 8: One last thing on pipes","text":"Go back code try rewrite using pipes %>% efficient possible.\nhint: point first argument function name object created line, good chance used pipe!\nhint: point first argument function name object created line, good chance used pipe!bits code piped together one chain:","code":"\n`rlong <- pivot_longer(responses, names_to = \"Question\", values_to = \"Response\", Q1:Q10)`\n\n`rlong2 <- inner_join(rlong, qformats, \\\"Question\\\")`\n\n`rscores <- inner_join(rlong2, scoring, c(\\\"QFormat\\\", \\\"Response\\\"))`\n\n`aq_scores <- rscores %>% group_by(Id) %>% summarise(AQ = sum(Score))`"},{"path":"data-wrangling-3.html","id":"finished-1","chapter":"6 Data Wrangling 3","heading":"6.9 Finished","text":"Brilliant work today! now recapped one-table verbs started expand knowledge two-table verbs. great know example, seen , actually took handful reproducible steps get messy data tidy data; imagine hand Excel cutting pasting? mention mistakes make! Excellent work! DataWrangling expert! finishing, remember go anything unsure , questions, please post Teams. additional questions help check understanding.","code":""},{"path":"data-wrangling-3.html","id":"dw3-test","chapter":"6 Data Wrangling 3","heading":"6.10 Test yourself","text":"Complete sentence, higher AQ score...\n\nless autistic-like traits displayedhas relation autistic-like traitsthe autistic-like traits displayed\nComplete sentence, higher AQ score...Assuming code worked, AQ score (just number) Participant ID . 87:\n\n2356\nAssuming code worked, AQ score (just number) Participant ID . 87:Type box many participants AQ score 3 (just number): Type box many participants AQ score 3 (just number): cut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Based data, many participants might referred assessment?\n\n2468\ncut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Based data, many participants might referred assessment?mentioned, higher score AQ10 autistic-like traits participant said show.mentioned, higher score AQ10 autistic-like traits participant said show.code filter(aq_scores, Id == 87), give tibble 1x2 showing ID number score. just wanted score use pull() shown yet: filter(aq_scores, Id == 87) %>% pull(AQ). answer AQ score 2.code filter(aq_scores, Id == 87), give tibble 1x2 showing ID number score. just wanted score use pull() shown yet: filter(aq_scores, Id == 87) %>% pull(AQ). answer AQ score 2.changing argument filter. filter(aq_scores, AQ == 3) %>% count(). answer 13. Remember can counting code makes reproducible accurate every time. might make mistakes.changing argument filter. filter(aq_scores, AQ == 3) %>% count(). answer 13. Remember can counting code makes reproducible accurate every time. might make mistakes.filter(aq_scores, AQ > 6) %>% count() filter(aq_scores, AQ >= 7) %>% count(). answer 6.filter(aq_scores, AQ > 6) %>% count() filter(aq_scores, AQ >= 7) %>% count(). answer 6.Recap Wickham Verbs!function(s) use approach following problems?dataset 400 adults, want remove anyone age 50 years . , use mutate()filter()arrange()group_by()summarise()select() function.dataset 400 adults, want remove anyone age 50 years . , use mutate()filter()arrange()group_by()summarise()select() function.interested overall summary statistics data, overall average total number observations. , use summarise()group_by()filter()select()mutate()arrange() function.interested overall summary statistics data, overall average total number observations. , use summarise()group_by()filter()select()mutate()arrange() function.dataset column number cats person , column number dogs. want calculate new column contains total number pets participant . , use summarise()select()group_by()mutate()arrange()filter() function.dataset column number cats person , column number dogs. want calculate new column contains total number pets participant . , use summarise()select()group_by()mutate()arrange()filter() function.want calculate average participant dataset. use group_by() arrange()arrange() mutate()filter() select()group_by() summarise() functions.want calculate average participant dataset. use group_by() arrange()arrange() mutate()filter() select()group_by() summarise() functions.want order dataframe participants number cats , want new dataframe contain columns. use select() summarise()group_by() mutate()arrange() select()filter() select() functions.want order dataframe participants number cats , want new dataframe contain columns. use select() summarise()group_by() mutate()arrange() select()filter() select() functions.filter() helps us keep remove rows!summarise() main function creating means, medians, modes, etc.mutate() can used add columns help add information.want summary statistics individual groups participants first group_by() summarise().need filter() first reduce people based number cats just select() columns want keep.","code":""},{"path":"data-wrangling-3.html","id":"dw3-sols","chapter":"6 Data Wrangling 3","heading":"6.11 Activity solutions","text":"find solutions questions. look giving questions good try trying find help Google Teams issues.","code":""},{"path":"data-wrangling-3.html","id":"dw3-a1sol","chapter":"6 Data Wrangling 3","heading":"6.11.1 Activity 1","text":"load tidyverse library following:","code":"\nlibrary(tidyverse)"},{"path":"data-wrangling-3.html","id":"dw3-a2sol","chapter":"6 Data Wrangling 3","heading":"6.11.2 Activity 2","text":"Remember always use read_csv() working book.read data objects follows.","code":"\nresponses <- read_csv(\"responses.csv\")                  \nqformats <- read_csv(\"qformats.csv\")                 \nscoring <- read_csv(\"scoring.csv\")                  \npinfo <- read_csv(\"pinfo.csv\")"},{"path":"data-wrangling-3.html","id":"dw3-a5sol","chapter":"6 Data Wrangling 3","heading":"6.11.3 Activity 5","text":"inner_join() first tibble rlong, second tibble qformats common column Question.","code":"\nrlong2 <- inner_join(x = rlong, \n                     y = qformats, \n                     by = \"Question\")"},{"path":"data-wrangling-3.html","id":"dw3-a7sol","chapter":"6 Data Wrangling 3","heading":"6.11.4 Activity 7","text":"create AQ score first group_by Id column individual participants data, sum Score column obtain AQ score.","code":"\naq_scores <- rscores %>% \n             group_by(Id) %>% # group by the ID number in column Id\n             summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores."},{"path":"data-wrangling-3.html","id":"dw3-a8sol","chapter":"6 Data Wrangling 3","heading":"6.11.5 Activity 8","text":"tricky one might read code. Remember %>% can read ...First, take data responses thentake columns Q1 Q10, put column names Question scores Response thenjoin qformats match data column Question thenjoin scoring match data columns Qformat Response thengroup participant ID thencalculate total AQ score","code":"\naq_scores2 <- responses %>% \n  pivot_longer(cols = Q1:Q10,\n               names_to = \"Question\", \n               values_to = \"Response\") %>%  \n  inner_join(qformats, \"Question\") %>% \n  inner_join(scoring, c(\"QFormat\", \"Response\")) %>% \n  group_by(Id) %>% \n  summarise(AQ = sum(Score)) "},{"path":"data-wrangling-3.html","id":"words-from-this-chapter-5","chapter":"6 Data Wrangling 3","heading":"6.12 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"intro-to-data-visualisation.html","id":"intro-to-data-visualisation","chapter":"7 Intro to Data Visualisation","heading":"7 Intro to Data Visualisation","text":"","code":""},{"path":"intro-to-data-visualisation.html","id":"data-visualisation","chapter":"7 Intro to Data Visualisation","heading":"7.1 Data visualisation","text":"Data Visualisation. able visualise data, relationships variables, incredibly useful important skill. statistical analyses present summary statistics, visualise data :quick easy way check data make sense, identify unusual trends.way honestly present features data anyone reads research.means checking data fits assumptions descriptive inferential tests statistical analyses intend use.Grolemund Wickham tell us R Data Science:Visualisation fundamentally human activity. good visualisation show things expect, raise new questions data. good visualisation might also hint ’re asking wrong question, need collect different data. Visualisations can surprise , don’t scale particularly well require human interpret .main package use visualisation within tidyverse umbrella called ggplot2 main starting function visualisations ggplot(). reason say \"main starting function\" ggplot() builds plots combining layers (see, example, Figure 7.1 (Nordmann et al., 2021)) - .e. one function creates first layer, basic plot area, add functions arguments add additional layers data, labels, colors, etc. used making plots software might seem bit odd first, however, means can customise layer separately order make complex beautiful figures relative ease. can get sense possible (website start slow build go!\nFigure 7.1: Building figure using ggplot2 layers system shown Nordmann et al. (2021)\n","code":""},{"path":"intro-to-data-visualisation.html","id":"setting-up-to-visaulise","chapter":"7 Intro to Data Visualisation","heading":"7.2 Setting up to Visaulise","text":"use data files Chapter 2, Starting Data, already know data contains can focus just visualising .","code":""},{"path":"intro-to-data-visualisation.html","id":"introviz-a1","chapter":"7 Intro to Data Visualisation","heading":"7.2.0.1 Activity 1: Set-up","text":"data contains happiness depression scores:Download ahi-cesd.csv participant-info.csv folder computer chapter!\nMake sure downloaded .csv files saved chapter folder. Remember change file names data.csv data (1).csv.\nMake sure downloaded .csv files saved chapter folder. Remember change file names data.csv data (1).csv.Open RStudio set working directory chapter folder. Ensure environment clear.\nserver, avoid number issues restarting session - click Session - Restart R\nserver, avoid number issues restarting session - click Session - Restart ROpen new R Markdown document save working directory. Call file \"DataVisualisation1\".Delete default R Markdown welcome text insert new code chunk.Type run code load tidyverse package load data files.good idea code brief summary:loads tidyverseIt reads datafiles tibbles separate objects, dat pinfo.Joins data together one larger tibble stores object called all_datSelect number columns keep data discards others.","code":"\nlibrary(tidyverse) \n\ndat <- read_csv(\"ahi-cesd.csv\")\npinfo <- read_csv(\"participant-info.csv\")\n\nall_dat <- inner_join(dat, \n                      pinfo, \n                      by= c(\"id\", \"intervention\"))\nsummarydata <- select(.data = all_dat, \n                      ahiTotal, \n                      cesdTotal, \n                      sex, \n                      age, \n                      educ, \n                      income, \n                      occasion, \n                      elapsed.days) "},{"path":"intro-to-data-visualisation.html","id":"dealing-with-factors-and-categories","chapter":"7 Intro to Data Visualisation","heading":"7.3 Dealing with Factors and Categories","text":"go need perform additional step data processing glossed point. First, run code look structure dataset:can see variables automatically considered numeric (case double represented <dbl>). going problem whilst different categories within sex, educ, income represented numbers, want treat categories, call factors. get around , need convert variables factor data type. Fortunately already know good function ! can use mutate() overriding original variable data classified factor.","code":"\nglimpse(summarydata)## Rows: 992\n## Columns: 8\n## $ ahiTotal     <dbl> 32, 34, 34, 35, 36, 37, 38, 38, 38, 38, 39, 40, 41, 41, 4~\n## $ cesdTotal    <dbl> 50, 49, 47, 41, 36, 35, 50, 55, 47, 39, 45, 47, 33, 27, 3~\n## $ sex          <dbl> 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, ~\n## $ age          <dbl> 46, 37, 37, 19, 40, 49, 42, 57, 41, 41, 52, 41, 52, 58, 5~\n## $ educ         <dbl> 4, 3, 3, 2, 5, 4, 4, 4, 4, 4, 5, 4, 5, 5, 5, 4, 3, 4, 3, ~\n## $ income       <dbl> 3, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 3, 2, 2, 2, ~\n## $ occasion     <dbl> 5, 2, 3, 0, 5, 0, 2, 2, 2, 4, 4, 0, 4, 0, 1, 4, 0, 5, 4, ~\n## $ elapsed.days <dbl> 182.025139, 14.191806, 33.033831, 0.000000, 202.096887, 0~"},{"path":"intro-to-data-visualisation.html","id":"introviz-a2","chapter":"7 Intro to Data Visualisation","heading":"7.3.1 Activity 2: Factors","text":"Type run code change categories factors.\ncan read line mutate \"overwrite data column values now considered factors doubles\"\nexample, 1s sex change categorical factors instead numerical 1s.\nRemember mutate new column name old one, overwrite column.\ncan read line mutate \"overwrite data column values now considered factors doubles\"example, 1s sex change categorical factors instead numerical 1s.Remember mutate new column name old one, overwrite column.imporant step remember , look data, categories represented numbers factors. might end really confused looking figures!","code":"\nsummarydata <- summarydata %>%\n  mutate(sex = as.factor(sex),\n         educ = as.factor(educ),\n         income = as.factor(income))"},{"path":"intro-to-data-visualisation.html","id":"barplots","chapter":"7 Intro to Data Visualisation","heading":"7.4 Barplots","text":"Ok great, now ready visualising plotting. first example create barplot data showing number male female participants within data. barplot plot shows counts categorical data, factors, height bar represents count particular variable.","code":""},{"path":"intro-to-data-visualisation.html","id":"introviz-a3","chapter":"7 Intro to Data Visualisation","heading":"7.4.0.1 Activity 3: Bar plot","text":"Read following section try different code chunks. Following , changing parts code see happens, help see layers build .first layerThe first line (layer) sets base graph: data use aesthetics (go x y axis, plot grouped).aes() can take x y argument, however, bar plot just asking R count number data points group need specify .\nFigure 7.2: First ggplot() layer sets axes\nnext layer adds geom shape, case use geom_bar() want draw bar plot.\nNote adding layers, using + layers. important difference pipes visualisation. mention later add layers (+), pipe !\nNote adding layers, using + layers. important difference pipes visualisation. mention later add layers (+), pipe !\nFigure 7.3: Basic barplot geom_bar() added\nAdding fill first layer separate data level grouping variable give different colour. case, different coloured bar level sex.\nFigure 7.4: Barplot colour\ncan see, adding fill() also produced plot legend right graph. multiple grouping variables want legends know groups color part plot referring , case redundant tell us anything axes labels already. can get rid adding show.legend = FALSE geom_bar() code.\nFigure 7.5: Barplot without legend\nExcellent. far good! might want tidy plot make look bit nicer. First can edit axis labels informative. common functions use :scale_x_continuous() adjusting x-axis continuous variablescale_y_continuous() adjusting y-axis continuous variablescale_x_discrete() adjusting x-axis discrete/categorical variablescale_y_discrete() adjusting y-axis discrete/categorical variableAnd functions two common arguments use :name controls name axis - .e. overall variable called example (e.g. Groups)labels controls names break points axis - .e. conditions within variable called example (e.g. dogs cats)lots ways can customise axes stick now.Type run code change axes labels change numeric sex codes (1s 2s) words (Female Male).\nNote: using scale_x_discrete() x-axis discrete variable data (Female Male), using scale_y_continuous() y-axis continuous data (count many people )\nNote: labels arguments must written correct order data. make 1's Female 2's Male, flipped order Male Female, make 1's Male 2's Female. Remember code tell always check output!\nNote: using scale_x_discrete() x-axis discrete variable data (Female Male), using scale_y_continuous() y-axis continuous data (count many people )Note: labels arguments must written correct order data. make 1's Female 2's Male, flipped order Male Female, make 1's Male 2's Female. Remember code tell always check output!\nFigure 7.6: Barplot axis labels\nNow default colors ok might want adjust colours visual style plot. ggplot2 comes built number different built-themes called.Type code new code chunk run .\nuse theme_minimal() try changing others see happens. start typing theme_ code chunk, instead theme_minimal(), trying options come auto-complete. Examples include, theme_bw(), theme_classic(), theme_light(), etc.\nuse theme_minimal() try changing others see happens. start typing theme_ code chunk, instead theme_minimal(), trying options come auto-complete. Examples include, theme_bw(), theme_classic(), theme_light(), etc.\nFigure 7.7: Barplot minimal theme\nOk color individual bars plot? Well, various options adjust colours good way inclusive use colour-blind friendly palette can also read printed black--white. , can add function scale_fill_viridis_d(). function 5 colour options, , B, C, D, E. like option = \"E\" can play around choose one prefer.Type run code new code chunk. Try changing option either , B, C D see one like!\nFigure 7.8: Barplot colour-blind friendly colour scheme\nFinally, can also adjust transparency bars adding alpha geom_bar(). Play around value see value prefer.\nFigure 7.9: Barplot adjusted alpha\ncan see, just lines code can create effective figure. top tip remember figure series layers, write code like . Avoid trying write whole figure blank code chunk. Instead, create first code chunk, run , add next layer, run , add next layer, run , etc., etc. make much easier follow code debug issues.\nadd layers, pipe \n\njust wanted remind key point - add layers + pipe layers %>%. try pip layer probably see error looks something like :\n\nError: mapping must created aes(). use %>% instead +?\n\nwatch common errore see people first starting learn visualise ggplot2\n","code":"\nggplot(summarydata, aes(x = sex))\nggplot(summarydata, aes(x = sex)) +\n  geom_bar()\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar()\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE)\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \n                              \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\")\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \n                              \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal()\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \n                              \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE, \n           alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \n                              \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")"},{"path":"intro-to-data-visualisation.html","id":"the-violin-boxplot","chapter":"7 Intro to Data Visualisation","heading":"7.5 The Violin-boxplot","text":"numerous different styles visualisations figures can create. start format ggplot(data, aes(x, y)) + geom_... learn get deeper book can look cheatsheets help menus: top menu - Help >> Cheat Sheets >> Data Visualisation ggplot2. instance, geom_point() scatterplots, geom_histogram() histograms, geom_line() lineplots. want show type figure becoming lot common field due quality information tells data - violin-boxplot.","code":""},{"path":"intro-to-data-visualisation.html","id":"introviz-a4","chapter":"7 Intro to Data Visualisation","heading":"7.5.0.1 Activity 4: Violin-boxplot","text":"violin boxplot actually merge violin plot boxplot. violin-boxplot just boxplot laid top violin plot - give additional information. part final activities today create violin-boxplot, hopefully now able see similar structure bar chart code. fact, three differences:added y argument first layer wanted represent two variables, just count.geom_violin() additional argument trim.geom_boxplot() additional argument width. Try adjusting value see happens.Type run code new code chunk see produces.\nTry setting trim argument geom_violin() TRUE seeing happens.\nTry adjusting value width argument within geom_boxplot() seeing happens.\nTry setting trim argument geom_violin() TRUE seeing happens.Try adjusting value width argument within geom_boxplot() seeing happens.\nFigure 7.10: Violin-boxplot\n","code":"\nggplot(summarydata, aes(x = income, \n                        y = ahiTotal, \n                        fill = income)) +\n  geom_violin(trim = FALSE, \n              show.legend = FALSE, \n              alpha = .4) +\n  geom_boxplot(width = .2, \n               show.legend = FALSE, \n               alpha = .7)+\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \n                              \"Average\", \n                              \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()"},{"path":"intro-to-data-visualisation.html","id":"layer-order","chapter":"7 Intro to Data Visualisation","heading":"7.6 Layer order","text":"said , one key thing note ggplot2 use layers. Whilst built layers step--step chapter, independent remove except first layer. Additionally, although independent, order put matter show now.","code":""},{"path":"intro-to-data-visualisation.html","id":"introviz-a5","chapter":"7 Intro to Data Visualisation","heading":"7.6.0.1 Activity 5: Layers part 2","text":"Type run code new code chunk look output.Now type run code new code chunk compare output output code . see difference?compare two figures, shown ease, first puts boxplots top violins whereas second puts violins top boxplots. plot different layer literally puts top already . great reason always look output just run code blindly always get think !\nFigure 7.11: Showing impact changing order layers. Figure shows boxplots top violin plots. Figure B shows violins top boxplots. codes order geoms different.\n","code":"\nggplot(summarydata, aes(x = income, y = ahiTotal)) +\n  geom_violin() +\n  geom_boxplot()\nggplot(summarydata, aes(x = income, y = ahiTotal)) +\n  geom_boxplot() +\n  geom_violin()"},{"path":"intro-to-data-visualisation.html","id":"saving-your-figures","chapter":"7 Intro to Data Visualisation","heading":"7.7 Saving your Figures","text":"Great work today! just want show one last helpful function save export figures. Much like favourite jumper, point nobody gets see ! useful able save copy plots image file can use presentation report. One approach can use function ggsave().","code":""},{"path":"intro-to-data-visualisation.html","id":"introviz-a6","chapter":"7 Intro to Data Visualisation","heading":"7.7.0.1 Activity 6: Saving plots","text":"two ways can use ggsave(). tell ggsave() plot want save, default save last plot created. demonstrate run code Activity 5 produce nice violin-boxplot:\nFigure 7.12: Violin-boxplot2\nNow got plot want save last produced plot, ggsave() requires tell file name save plot type image file want create (example uses .png also use e.g., .jpeg image types).Type run code new code chunk check chapter folder. performed correctly see saved image file.Note image tends save default size, size image displayed viewer, can change manually think dimensions plot correct need particular size file type.Type run code overwrite image file new dimensions.\ntry different dimensions units see difference. might want create violin-boxplot-v1, ...-v2, ...-v3, compare . Remeber can use ?ggsave() console window bring help function.\ntry different dimensions units see difference. might want create violin-boxplot-v1, ...-v2, ...-v3, compare . Remeber can use ?ggsave() console window bring help function.Alternatively, second way using ggsave() save plot object, just like done tibbles, tell ggsave() object want save.Type run code check folder image file. Resize plot think needs .\ncode saves plot Activity 5 object named viobox saves image file \"violin-boxplot-stored.png\".\nNote: add ggsave(). Instead separate line code tell object save. , + ggsave()\ncode saves plot Activity 5 object named viobox saves image file \"violin-boxplot-stored.png\".Note: add ggsave(). Instead separate line code tell object save. , + ggsave()Finall, note save plot object, see plot displayed anywhere. get figure display need type object name console (.e., viobox). benefit saving figures way making lots plots, accidentally save wrong one explicitly specifying plot save rather just saving last one.","code":"\nggplot(summarydata, aes(x = income, \n                        y = ahiTotal, \n                        fill = income)) +\n  geom_violin(trim = FALSE, \n              show.legend = FALSE, \n              alpha = .4) +\n  geom_boxplot(width = .2, \n               show.legend = FALSE, \n               alpha = .7)+\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \n                              \"Average\", \n                              \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()\nggsave(\"violin-boxplot.png\")\nggsave(\"violin-boxplot.png\", \n       width = 10, \n       height = 8, \n       units = \"in\")\nviobox <- summarydata %>%\n  ggplot(aes(x = income,\n             y = ahiTotal,\n             fill = income)) +\n  geom_violin(trim = FALSE, \n              show.legend = FALSE, \n              alpha = .4) +\n  geom_boxplot(width = .2, \n               show.legend = FALSE, \n               alpha = .7)+\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \n                              \"Average\", \n                              \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n\nggsave(\"violin-boxplot-stored.png\", plot = viobox)"},{"path":"intro-to-data-visualisation.html","id":"introviz-fin","chapter":"7 Intro to Data Visualisation","heading":"7.8 Finished!","text":"Well done! ggplot can bit difficult get head around first, particularly used making graphs different way. clicks, able make informative professional visualisations ease, , amongst things, make report write look professional!","code":""},{"path":"intro-to-data-visualisation.html","id":"test-yourself-2","chapter":"7 Intro to Data Visualisation","heading":"7.9 Test Yourself","text":"appropriate order functions create boxplot?\n\ngeom_plot() + geom_boxplot()geom_boxplot() + ggplot()ggplot() %>% geom_boxplot()ggplot() + geom_boxplot()\nline code run, assuming data libraries loaded data column names spelt correctly?line code create barplot, assuming data libraries loaded data column names spelt correctly?wanted boxplot top violin plot, order functions write?\n\nggplot() + geom_violin() + geom_boxplot()ggplot() %>% geom_boxplot() %>% geom_violin()ggplot() %>% geom_violin() %>% geom_boxplot()ggplot() + geom_boxplot() + geom_violin()\nr unhide()ggplot() + geom_boxplot() correct answer rest either use pipes, wrong order, wrong functions.line code run uses pipes instead adding layerThe line code run geom_barplot() function geom_bar()correct order ggplot() + geom_violin() + geom_boxplot() others either use pipes, wrong order, wrong functions.","code":"\nggplot(summarydata, aes(x = sex, fill = sex)) %>%\n  geom_bar()\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_barplot()"},{"path":"intro-to-data-visualisation.html","id":"words-from-this-chapter-6","chapter":"7 Intro to Data Visualisation","heading":"7.10 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.End ChapterThat end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"probability.html","id":"probability","chapter":"8 Probability","heading":"8 Probability","text":"chapter going bring ideas around probability. reading chapter - ask read try - activities try along way think make sure getting idea. Probability really important helping us make inference sample population , go look different inferential tests, now good handle data wrangling visualisation, now seem ideal time recap introduce key aspects probability.worry chapter make complete sense. may take reads may take reading get gist coming back ideas try datasets . main key concept want take away chapter, get nothing else chapter, can establish probability obtaining value distribution, probability can make inference likely unlikely obtain value. Try keep mind go next sections ok.first half chapter contain coding, instead, going recap core statistical concepts. need additional resources beyond discussed lectures, may find useful. essential can skip may just add little bit need help understand ideas behind probability.Read Statistical thinking (Noba Project)Watch Normal Distribution - Explained Simply (10 mins)Watch Probability explained (8 mins)Watch Binomial distribution (12 minutes)","code":""},{"path":"probability.html","id":"introduction-to-probability","chapter":"8 Probability","heading":"8.1 Introduction to probability?","text":"Probability (p) extent event likely occur represented number 0 1. example, probability flipping coin landing 'heads' estimated p = .5, .e., 50% chance getting head flip coin.fact, calculating probability individual event occurring can formulated :\\[p = \\frac{number \\   \\ ways \\ \\ event \\ \\  arise}{number \\ \\ possible \\ outcomes}\\]\nexample, probability randomly drawing name hat 12 names one name definitely ? Well, 12 possible outcomes, 1 way name arise, formula becomes:\\[p = \\frac{1}{12} = 0.0833333\\]Meaning probability 0.0833333. , wanted write percentage 8.3333333%, meaning every 100 draws hat expect name come 8.3 times. four names hat one :\\[p = \\frac{1}{4} = 0.25\\]24 names hat one :\\[p = \\frac{1}{24} = 0.0416667\\]can see probability event occurring changes number possible outcomes. Makes sense really! possible outcomes, less likely specific one outcome going happen. far good!","code":""},{"path":"probability.html","id":"prob-a1","chapter":"8 Probability","heading":"8.1.0.1 Activity 1: Probability","text":"Try answer questions check understanding.probability selecting name hat ten names hat name one ? 0.10.250.04166666666666670.0833333333333333What probability selecting name hat 100 names hat name one ? careful one! 00.10.010.1","code":""},{"path":"probability.html","id":"types-of-data","chapter":"8 Probability","heading":"8.2 Types of data","text":"tackle probability also depends type data/variables working (.e. discrete continuous). also referred Level Measurements recap different types data.Discrete data can take integer values (whole numbers). example, number participants experiment discrete - half participant! Discrete variables can also broken nominal ordinal variables.Ordinal data set ordered categories; know top/best worst/lowest, difference categories. example, ask participants rate attractiveness different faces based 5-item Likert scale (unattractive, unattractive, neutral, attractive, attractive). know attractive better attractive say certain difference neutral attractive size distance unattractive unattractive.Nominal data also based set categories ordering matter (e.g. left right handed). Nominal sometimes simply referred categorical data.Continuous data hand can take value. example, can measure age continuous scale (e.g. can age 26.55 years), examples include reaction time distance travel university every day. Continuous data can broken Interval Ratio data.Interval data data comes form numerical value difference points standardised meaningful. example temperature, difference temperature 10-20 degrees difference temperature 20-30 degrees.Ratio data like interval true zero point. interval temperature example , experiencing negative temperatures (-1,-2 degrees) Glasgow ratio data see negative values .e. -10 cm tall.","code":""},{"path":"probability.html","id":"prob-a2","chapter":"8 Probability","heading":"8.2.0.1 Activity 2: Types of data","text":"Try answer questions check understanding. types data measurements?Time taken run marathon (seconds): categoricalintervalordinalratioFinishing position marathon (e.g. 1st, 2nd, 3rd): categoricalordinalintervalratioWhich Sesame Street character runner dressed : ordinalratiocategoricalintervalTemperature runner dressed cookie monster outfit (degrees Celsius): intervalcategoricalordinalratio","code":""},{"path":"probability.html","id":"probability-distributions","chapter":"8 Probability","heading":"8.3 Probability distributions","text":"OK great. know bit probability bit data types. Next thing need think probability distributions! probability distribution theoretical counterpart observed frequency distribution. frequency distribution simply shows many times certain event actually occurred. probability distribution says many times occurred. Say example run test many times different flips coin produce either heads tails. count frequency distribution. expected, based simulations mathematicians, probability distribution. Mathematicians actually simulated number different probability distributions, know different types data tend naturally fall known distribution. , can use distributions help us calculate probability event without run . say another way, can determine probability event running test many many times , can use one simulated probability distributions save us lot time effort. going show .three distributions look , help us understand probability, :uniform distributionThe binomial distributionThe normal distribution","code":""},{"path":"probability.html","id":"the-uniform-distribution","chapter":"8 Probability","heading":"8.4 The uniform distribution","text":"uniform distribution possible outcome equal chance occurring. take example , pulling name hat 12 names. individual name equal chance drawn (p = .08). visualised distribution, look like distribution - outcome, case name, chance occurring:\nFigure 8.1: Uniform distribution, every outcome equal probability occurring.\nuniform distribution feature regularly Psychology, except perhaps experiments randomising block people get first performing chi-square test, helps us start understand outcome probability distribution.","code":""},{"path":"probability.html","id":"the-binomial-distribution","chapter":"8 Probability","heading":"8.5 The binomial distribution","text":"next distribution want look binomial distribution. binomial (bi = two, nominal = categories) distribution, used discrete data, probability distribution calculates probabilities success situations two possible outcomes e.g., flipping coin; outcomes either heads tails! binomial distribution models probability number successes observed (e.g. heads wanted heads), given probability success number observations (e.g. many times get heads (success) ten coin flips (observations)). probably worth pointing researcher determine success (heads tails) ease try stick heads.Let’s say flip coin 10 times. Assuming coin fair (probability heads = .5), many heads expect get? figure shows results simulation 10,000 coin flips (like simulation , can see code clicking \"Show code\"). However, distribution means can use know data binomial distribution work probability different outcomes. example, instead running whole bunch tests, use distribution answer question probability getting least 3 heads flip coin 10 times?.\nFigure 7.3: Probability . heads 10 coin tosses\nNote expected understand code right nowAgain, binomial distribution hugely common Psychology really starting see can ask questions outcomes based probability distributions opposed running tests . look distribution common psychology - normal distribution","code":"\nheads10000 <- replicate(n = 10000, \n                        expr = sample(0:1, \n                                      10, \n                                      TRUE) %>%\n                          sum())\n\ndata10000 <- tibble(heads = heads10000) %>%\n                group_by(heads) %>%     \n                summarise(n = n(),\n                          p=n/10000)\n\nggplot(data10000, aes(x = heads,y = p)) + \n  geom_bar(stat = \"identity\") + \n  labs(x = \"Number of Heads\", \n       y = \"Probability of Heads in 10 flips (p)\") +\n  theme_bw() +\n  scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10))"},{"path":"probability.html","id":"the-normal-distribution","chapter":"8 Probability","heading":"8.6 The normal distribution","text":"normal distribution, reflects probability value occurring continuous variable. Examples continuous variables include height age, single person can score anywhere along continuum. example, person 21.5 years old 176 cm tall.normal distribution looks like :\nFigure 7.5: Normal Distribution height. \\(\\mu\\) = mean (average), \\(\\sigma\\) = standard deviation\nSomething note normal distribution symmetrical, meaning equal probability observations mean occurring. means , mean plot heights 170 cm, expect number people height 160 cm number people height 180 cm. second thing note distribution symmetrical, mean, median, mode distribution equal middle distribution highest probability occurring. move away middle distribution, probability outcomes occurring starts reduce. plays important role analyses come see later chapters.Now, however, way coin flips, can use know data normal distribution estimate probability certain outcomes, probability someone taller 190cm?probabilities, real-world data come close normal distribution, (almost certainly) never match exactly. collect observations data might expect normally distributed, data get increasingly closer normal distribution. example, simulation experiment collect heights 5000 participants. can see, add observations, data starts look like normal distribution previous figure.\nFigure 7.6: simulation experiment collecting height data 2000 participants\n","code":""},{"path":"probability.html","id":"prob-a3","chapter":"8 Probability","heading":"8.6.0.1 Activity 3: Normal distribution","text":"Complete sentences make sure understanding .normal distribution, mean, median, mode always differentare equalsum zero.normal distribution, away mean observation higher probability occuringthe lower probability occuring.Whereas binomial distribution based situations two possible outcomes, normal distribution based situations data categorical variablehas three possible valuesis continuous variable.","code":""},{"path":"probability.html","id":"prob-a4","chapter":"8 Probability","heading":"8.6.0.2 Activity 4: Distribution test","text":"distribution likely associated following?Scores IQ test Uniform distributionBinomial distributionNormal distributionWhether country won lost Eurovision song contest Uniform distributionBinomial distributionNormal distributionPicking spade card normal pack playing cardsUniform distributionBinomial distributionNormal distribution","code":""},{"path":"probability.html","id":"using-the-binomial-distribution","chapter":"8 Probability","heading":"8.7 Using the binomial distribution","text":"Now, going calculate probabilities based binomial distribution. chapter, first time need load tidyverse. functions need contained Base R. want refresher difference Base R packages, see Programming Basics.","code":""},{"path":"probability.html","id":"prob-a5","chapter":"8 Probability","heading":"8.7.0.1 Activity 5: Getting Set-Up","text":"Open new R Markdown document, call \"Probability\" save relevant chapter folder, remembering delete default text need.going use three Base R functions work binomial distribution:dbinom() - density function: gives probability x successes given number trials probability success single trial (e.g., probability flipping 8/10 heads fair coin?).pbinom() - probability distribution function: gives cumulative probability getting number successes certain cut-point (e.g. probability getting 0 5 heads 10 flips), given size probability. known cumulative probability distribution function cumulative density function.qbinom() - quantile function: opposite pbinom() gives x axis value given probability p, plus given size prob, probability flipping head .5, many heads expect get 10 flips?try functions answer two questions:probability getting exactly 5 heads 10 flips?probability getting 2 heads 10 flips?","code":""},{"path":"probability.html","id":"prob-a6","chapter":"8 Probability","heading":"8.7.0.2 Activity 6: dbinom()","text":"start question 1, probability getting exactly 5 heads 10 flips?want predict probability getting 5 heads 10 trials (coin flips) probability success flip 0.5 ('ll either heads tails 50/50 chance write 0.5). use dbinom() work :dbinom() (density) function three arguments:x: number ‘heads’ want know probability . Either single value, 3, series values, 0:10. case want know 5 heads, write 5.size: number trials (flips) simulating; case, 10 flips.prob: probability ‘heads’ one trial. chance 50-50 probability state 0.5 .5Type run code new code chunk:Looking outcome, answer following questions:two decimal places, probability getting 5 heads 10 coin flips? probability expressed percent? 0.25%2.5%25%","code":"\ndbinom(x = 5, size = 10, prob = 0.5)"},{"path":"probability.html","id":"prob-a7","chapter":"8 Probability","heading":"8.7.0.3 Activity 7: pbinom()","text":"OK, question 2. probability getting 2 heads 10 flips?time use pbinom() want know cumulative probability getting maximum 2 heads 10 coin flips. set cut-point 2 still probability getting heads 0.5.Note: pbinom() takes arguments size prob argument just like dbinom(). However, first input argument q rather x. dbinom x fixed number, whereas q possibilities including given number (e.g. 0, 1, 2).Type run code new code chunk:Looking outcome, answer following question:probability getting maximum 2 heads 10 coin flips 2 decimal places? probability expressed percent? 0.05%0.5%5%","code":"\npbinom(q = 2, size = 10, prob = 0.5)"},{"path":"probability.html","id":"prob-a8","chapter":"8 Probability","heading":"8.7.0.4 Activity 8: pbinom() 2","text":"try one scenario cut-point make sure understood . probability getting 7 heads 10 flips?can use function previous example, however, extra argument want get correct answer. try running code used first change q = 2 q = 7 see get.tells us probability .95 95% - seem right ? seems high getting 7 heads 10 coin flips! ? Well, default behaviour pbinom() calculate cumulative probability lower tail curve, .e., specify q = 2 calculates probability outcomes including 2. specified q = 7 means calculated probability getting outcome 0, 1, 2, 3, 4, 5, 6, 7 - shown blue area figure - obviously high.\nFigure 7.7: Lower upper tails\nget right answer, add lower.tail = FALSE code interested upper tail distribution. want cumulative probability include 7, know q words including, order get 7 , set q = 6. now calculate cumulative probability getting 7, 8, 9, 10 heads 10 coin flips. Remember, set q = 7 including 7, looking upper tail distribution give us 8, 9 10. want 7, 8, 9 10, set including 6, leaves us 7 .Try run code new code chunk:Looking outcome, answer following question:probability getting 7 10 heads 10 coin flips 2 decimal places? probability expressed percent? 0.017%0.1717%","code":"\npbinom(q = 7, size = 10, prob = .5) ## [1] 0.9453125\npbinom(q = 6, size = 10, prob = .5, lower.tail = FALSE) "},{"path":"probability.html","id":"prob-a9","chapter":"8 Probability","heading":"8.7.0.5 Activity 9: qbinom()","text":"OK great! excellent tricky stuff. Remember though whole point show using probability distributions can ask sorts questions probability outcome series outcomes.Now consider scenario use quantile function qbinom. Imagine accosted street magician want bet can predict whether coin land heads tails. suspect done something coin fair probability coin landing head longer .5 50/50 - suspect coin now much likely land tails. null hypothesis coin trick coin probability heads tails even. going run single experiment test hypothesis, 10 trials. minimum number heads acceptable p really equal .5?used argument prob previous two functions, dbinom pbinom, represents probability success single trial (probability 'heads' one coin flip, .5). qbinom, prob still represents probability success one trial, whereas p represents overall probability success across trials. run pbinom, calculates number heads give probability.know looking binomial distribution sometimes even coin fair, get exactly 5/10 heads. Instead, want set cut-, probability well say unlikely get result coin fair example use default cut-statistical significance psychology, .05, 5%.words, ask minimum number successes (e.g. heads) maintain overall probability .05, 10 flips, probability success one flip .5. use code:code see answer 2. means magician flipped fewer two heads ten, conclude less 5% probability happened coin fair. reject null hypothesis coin unbiased heads politely ask kind magician money back!However, ten trials probably far want accuse magician bit dodge. Run code answer following questions:cut-ran 100 trials? cut-ran 1000 trials? cut-ran 10,000 trials? Notice trials run, precise estimates become, , closer probability success single flip (.5). simplification, think relates sample size research studies, participants , precise estimate .also mention qbinom also uses lower.tail argument works similar fashion pbinom. try good know case ever need .Visualise !go playing around different numbers coin flips probabilities dbinom() pbinom() app!\nFigure 7.8: Binomial distribution app\n","code":"\nqbinom(p = .05, size = 10, prob = .5)## [1] 2\nqbinom(p = .05, size = c(100, 1000, 10000), prob = .5)"},{"path":"probability.html","id":"using-the-normal-distribution","chapter":"8 Probability","heading":"8.8 Using the normal distribution","text":"similar set functions exist help us work distributions, including normal distribution going use three :dnorm()- density function, calculating probability specific valuepnorm()- probability distribution function, calculating probability getting least specific valueqnorm()- quantile function, calculating specific value associated given probabilityAs can probably see, functions similar functions used work binomial distribution. use data height Scottish people show functions work normal distribution","code":""},{"path":"probability.html","id":"probability-of-heights","chapter":"8 Probability","heading":"8.8.1 Probability of heights","text":"Data Scottish Health Survey (2008) shows :average height 16-24 year old Scottish man 176.2 centimetres, standard deviation 6.748.average height 16-24 year old Scottish woman 163.8 cm, standard deviation 6.931.time writing, currently data Scottish trans non-binary people.figure simulation information - , can see code used run simulation clicking \"Show code\" button note asked understand right now.\nFigure 8.2: Simulation Scottish height data\ntest normal distribution, round chapter, use information calculate probability observing least specific height pnorm(), heights associated specific probabilities qnorm().","code":"\nmen <- rnorm(n = 100000, mean = 176.2, sd = 6.748)\nwomen <- rnorm(n = 100000, mean = 163.8, sd = 6.931)\n\nheights <- tibble(men, women) %>%\n  pivot_longer(names_to = \"sex\", values_to = \"height\", men:women)\n\nggplot(heights, aes(x = height, fill = sex)) +\n  geom_density(alpha = .6) +\n  scale_fill_viridis_d(option = \"E\") +\n  theme_minimal()"},{"path":"probability.html","id":"prob-a10","chapter":"8 Probability","heading":"8.8.1.1 Activity 10:pnorm()","text":"pnorm() requires three arguments:q value want calculate probability . Note however set exactly number want 1 less number want. data continuous discrete binomial distribution.mean mean datasd standard deviation datalower.tail works depends whether interested upper lower tail,Type code code chunk replace NULLs calculate probability meeting 16-24 y.o. Scottish woman tall taller average 16-24 y.o. Scottish man.hint: asking female distribution use mean sdhint: average male 176.2hint: tall taller upper.hint: solution end chapter stuck.Looking outcome, answer following questions.probability meeting 16-24 y.o. Scottish woman taller average 16-24 y.o. Scottish man? probability expressed percent? 0.04%0.4%4%","code":"\npnorm(q = NULL, mean = NULL, sd = NULL, lower.tail = NULL)"},{"path":"probability.html","id":"prob-a11","chapter":"8 Probability","heading":"8.8.1.2 Activity 11: pnorm 2","text":"Fiona tall Scottish woman (181.12 cm) 16-24 y.o. range date men taller .Using pnorm() , proportion Scottish men Fiona willing date 2 decimal places? hint: want know male population\nhint: Fiona 181.12 cm tall want taller .\nhint: want know male populationhint: Fiona 181.12 cm tall want taller .probability expressed percent? 0.23%2.3%23%","code":""},{"path":"probability.html","id":"prob-a12","chapter":"8 Probability","heading":"8.8.1.3 Activity 12: pnorm 3","text":"hand, Fiona bisexual date women shorter .proportion Scottish women Fiona willing date 2 decimal places? hint: female distribution, lower Fiona.\nhint: female distribution, lower Fiona.probability expressed percent? 0.99%9.9%99%","code":""},{"path":"probability.html","id":"prob-a13","chapter":"8 Probability","heading":"8.8.1.4 Activity 13: qnorm()","text":"Finally, previous examples calculated probability particular outcome. Now want calculate outcome associated particular probability can use qnorm() .qnorm() similar pnorm() one exception, rather specifying q known observation quantile, instead specify p, known probability.Replace NULLs code calculate tall 16-24 y.o. Scottish man order top 5% (.e., p = .05) height distribution Scottish men age group. Remember solutions end chapter. can confirm right answering question:Visualise !go playing around different distributions dnorm() pnorm() app - access ","code":"\nqnorm(p = NULL, mean = NULL, sd = NULL, lower.tail = NULL)"},{"path":"probability.html","id":"finished-2","chapter":"8 Probability","heading":"8.9 Finished","text":"! key concepts take away chapter different types data tend follow known distributions, can use distributions calculate probability particular outcomes. foundation many statistical tests learn course. example, want compare whether scores two groups different, , whether come different distributions, can calculate probability scores group 2 distribution group 1. probability less 5% (p = .05), might conclude scores significantly different. oversimplification obviously, can develop good understanding probability distributions stand good stead rest statistics content.long read test today sure make notes check understanding different concepts. Please also remember ask questions unsure .","code":""},{"path":"probability.html","id":"prob-sols","chapter":"8 Probability","heading":"8.10 Activity solutions","text":"","code":""},{"path":"probability.html","id":"prob-a6sol","chapter":"8 Probability","heading":"8.10.0.1 Activity 6","text":"two decimal places, probability getting 5 heads 10 coin flips?","code":"\n.25"},{"path":"probability.html","id":"prob-a7sol","chapter":"8 Probability","heading":"8.10.0.2 Activity 7","text":"probability getting maximum 2 heads 10 coin flips 2 decimal places?","code":"\n.06"},{"path":"probability.html","id":"prob-a8sol","chapter":"8 Probability","heading":"8.10.0.3 Activity 8","text":"probability getting 7 10 heads 10 coin flips 2 decimal places?","code":"\n.17"},{"path":"probability.html","id":"prob-a10sol","chapter":"8 Probability","heading":"8.10.0.4 Activity 10","text":"probability meeting 16-24 y.o. Scottish woman taller average 16-24 y.o. Scottish man?","code":"\npnorm(q = 176.2, mean = 163.8, sd = 6.931, lower.tail = FALSE)"},{"path":"probability.html","id":"prob-a11sol","chapter":"8 Probability","heading":"8.10.0.5 Activity 11","text":"Using pnorm() , proportion Scottish men Fiona willing date 2 decimal places?","code":"\npnorm(q = 181.12, mean = 176.2, sd = 6.748, lower.tail = FALSE)"},{"path":"probability.html","id":"prob-a12sol","chapter":"8 Probability","heading":"8.10.0.6 Activity 12","text":"proportion Scottish women Fiona willing date 2 decimal places?","code":"\npnorm(q = 181.12, mean = 163.8, sd = 6.931, lower.tail = TRUE)"},{"path":"probability.html","id":"prob-a13sol","chapter":"8 Probability","heading":"8.10.0.7 Activity 13","text":"answer last question :","code":"\nqnorm(p = .05, mean = 176.2, sd = 6.748, lower.tail = FALSE)"},{"path":"probability.html","id":"words-from-this-chapter-7","chapter":"8 Probability","heading":"8.11 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"correlations.html","id":"correlations","chapter":"9 Correlations","heading":"9 Correlations","text":"far book building skills knowledge data wrangling now basis lot work research. may think tasks ask get harder course progresses true. hardest part working data beginning, trying learn new terminology, figuring load data wrangle format need. whilst may feel like yet covered lot, really worth reflecting just far come short time.instance, can now:Understand functions, arguments, objects, variables, tibbles .Read data working environment analysis.Tidy data appropriate format.Calculate range descriptive statistics.Create plots data.amazing! honest, see, actually covers large percentage . now start moving actually running inferential statistics, make inferences population interest sample collected. Specifically, chapter going move performing correlation create plot visualise data. Correlation test measures relationship two variable. words, measure two variables correlation analysis tells whether related manner, either positively negatively, strong relationship - discuss correlation always talk strength direction relationship. figure shows three examples correlations just give idea:\nFigure 9.1: Schematic examples extreme bivariate relationships\nNote: dealing correlations always refer relationships predictions. correlation, X predict Y, X cause effect Y. look different tests book can take cause--effect prediction, correlation can say whether X Y related.Now, said , actually carry correlation fairly straightforward show today little . hardest part correlations really wrangling data interpreting results mean. going run correlations chapter give good practice running interpreting relationships two variables.chapter use examples Miller Haden (2013), Chapter 11, looking relationship four variables: reading ability, intelligence (IQ), number minutes per week spent reading home (Home); number minutes per week spent watching TV home (TV). use data across series tasks help understand correlations . need read chapter hopefully just names variables can see situation unethical manipulate variables measuring exist environment appropriate; hence use correlations. clarity, example observational research; correlations observational, observational research uses correlations.Finally, two main types correlations people tend think (though ):Pearson's product-moment correlation (often shortened Pearson correlation) symbolised rSpearman's rank correlation coefficient (often shortened Spearman correlation) symbolised rho sometimes \\(r_s\\) sometimes \\(\\rho\\)talk little bit two correlations work today. shall begin now!","code":""},{"path":"correlations.html","id":"set-up-the-data","chapter":"9 Correlations","heading":"9.1 Set-up the Data","text":"always, first activity getting ready analyse data try steps need help, consult earlier chapters.","code":""},{"path":"correlations.html","id":"corr-a1","chapter":"9 Correlations","heading":"9.1.0.1 Activity 1: Set-up","text":"Open RStudio set working directory chapter folder. Ensure environment clear.\nusing Rserver, avoid number issues restarting session - click Session - Restart R\nusing Rserver, avoid number issues restarting session - click Session - Restart ROpen new R Markdown document save working directory. Call file \"Correlations\".Download MillerHadenData.csv save folder. Make sure change file name .\nprefer can download data zip folder clicking \nRemember change file names data.csv data (1).csv.\nprefer can download data zip folder clicking hereRemember change file names data.csv data (1).csv.Delete default R Markdown welcome text insert new code chunk loads following packages, specific order, using library() function. Remember solutions needed.\nLoad packages order, car,correlation, report, psych, tidyverse\nused four packages likely need install using install.packages(). Remember though machine console window. using RServer need install .\nLoad packages order, car,correlation, report, psych, tidyversewe used four packages likely need install using install.packages(). Remember though machine console window. using RServer need install .Finally, load data object named mh using read_csv().package requires using library() function time. example, library(car), libary(correlation), etc, etc.mh <- read_csv(\"What_is_your_datafile_called.csv\")","code":""},{"path":"correlations.html","id":"corr-a2","chapter":"9 Correlations","heading":"9.1.0.2 Activity 2: Look at your data","text":"Excellent! loaded data correctly able look one various methods looked already.Look data using head() function see following:can see, five columns :participant number (Participant),Reading Ability score (Abil),Intelligence score (IQ),number minutes spent reading Home per week (Home),number minutes spent watching TV per week (TV).focus Reading Ability IQ practice can look relationships free time.probable hypothesis today Reading Ability increases Intelligence (remember causality ). phrasing alternative hypothesis (\\(H_1\\)) formally, hypothesise reading ability school children, measured standardized test, intelligence, measured standardized test, show positive relationship. hypothesis test today remember always state null hypothesis (\\(H_0\\)) relationship reading ability IQ.","code":""},{"path":"correlations.html","id":"assumptions-of-the-test","chapter":"9 Correlations","heading":"9.2 Assumptions of the test","text":"Now running analysis check assumptions test, assumptions checks data must pass can use certain tests. assumptions change test use given test based well data meets assumptions test. short, Pearson correlation Spearman correlation different assumptions need check data see test use.","code":""},{"path":"correlations.html","id":"corr-a3","chapter":"9 Correlations","heading":"9.2.0.1 Activity 3: Assumptions","text":"correlations, main assumptions need check :data interval, ratio, ordinal?data point participant variables?data normally distributed variables?relationship variables appear linear?spread homoscedasticity?look assumptions turn see correlation use.Assumption 1: Level MeasurementIf want run Pearson correlation need interval ratio data; Spearman correlations can run ordinal, interval ratio data. type data ?type data analysis probably ratiointervalordinalnominal data continuousdiscrete unlikely true zeroAre variables continuous?difference 1 2 scale equal difference 2 3?Assumption 2: Pairs DataGreat! data looks least interval continuous. Next, correlations must data point participant two variables correlated. make sense - correlate empty cell! now go check data point columns participant.Note: can check missing data visual inspection - literally using eyes. missing data point show NA, short applicable, available, answer. alternative use .na() function. can really handy lots data visual inspection just take long. example ran following code:look output function, FALSE tells data-point cell. .na() asks cell NA; empty. cell empty come back TRUE. cells data , showing FALSE. wanted ask opposite question, data cell, write !.na() read \"NA\". Remember, exclamation mark ! turns question opposite.However looked data, looks like everyone data columns test skills little whilst . Answer following questions:missing data represented tibble? empty cellNAa large numberdon't knowWhich code leave just participants missing Reading Ability data mh:\nfilter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)code leave just participants missing Reading Ability data mh: filter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)filter(dat, .na(variable)) versus filter(dat, !.na(variable))Assumption 3-5: Normality, linearity, homoscedasticityBrilliant! know data type know missing data. remaining assumptions best checked visualisations. can use histograms QQ-plots check data (Abil IQ) normally distributed, can use scatterplot IQ function Abil check whether relationship linear, homoscedasticity, without outliers. alternative use z-scores check outliers cut-usually set around \\(\\pm2.5SD\\) \\(\\pm3SD\\). using mutate function (e.g. mutate(z = (X - mean(X))/SD(X))), today just use visual checks.now ask create figures look together, whole, answer questions last assumptions.Histograms NormalityType code new code chunk run create histogram Abil.code look similar code used create bar plot Chapter 7. specified want display Abil x-axis shape want produce histogram, hence geom_histogram(). Just like geom_bar(), need specify y-axis histogram, always count. figure look shown .\nFigure 9.2: Histogram Abil\nNow, new code chunk, write run code produce histogram variable IQ. Remember solutions end chapter.Q-Q Plots NormalityAs said look figures minute first need plots. One Q-Q plot allows us check normality. Q-Q plot require us use package car rather ggplot2. can make Q-Q plots ggplot2 useful, however, code still simple.new code chunk, type run code create Q-Q plot Abil.code looks little different code used point comes Base R. uses notation object$variable x variable read \"use variable Abil object mh. figure look like :\nFigure 9.3: Q-Q plot Abil\nQ-Q plot includes confidence envelope (blue dotted lines) around data understanding data points fall within dotted lines can assume normality. ggplot2 version Q-Q plots make difficult add confidence envelope, using different package. qqPlot() also print IDs extreme data points. case, 4th 15th data point Abil flagged, although fall within confidence envelope, appear problematic. also explains might see message stating ## [1] 15 4. 15th 4th value worth considering!Now, new code chunk, write run code create Q-Q plot IQ.Information: Normality residualsOne thing note move , terms normality, fact normality residuals matters, residuals difference individual data points line best fit. However, fully understand need cover information first introducing concept stage confusing. One approach field can use however data normally distributed highly likely residuals also normally distributed. look residuals next chapter, instead use normality raw data proxy normality residuals.Scatterplots linearity homoscedasticityFinally, order assess linearity homoscedasticity, can create scatterplot using ggplot2.new code chunk, copy run code create scatterplot relationship IQ Ability.ggplot2 code similar already encountered bar chart violin-boxplot.first line data sets base plot specify wish display Abil x-axis, IQ y-axis, use dataset mh.first geom, geom_point(), adds data points,second geom, geom_smooth, adds line best fit. shaded area around line confidence interval around data. can turned setting se = FALSE additional argument.figure look follows:\nFigure 7.6: Scatterplot scores\nNote: worry see message stating ## geom_smooth() using formula 'y ~ x'. just letting know plotting line best fit (blue line)Now, remembering ggplot2 works layers can customise layer, edit code add layer scale_x_continuous() changes label Abil Reading Ability.Checking assumptionsNow figures need able check assumptions. Try answer following questions, based visualisations:assumption normality met variables? YesNoIs assumption linearity met? YesNoIs assumption homoscedasticity met? YesNoBased , correlation method use? PearsonSpearmanWhen assessing assumptions use visualisations decision always judgement call. dataset, data 25 participants therefore unlikely ever observe perfect normality linearity dataset. likely researcher assume data approximately normal, evidence non-linear relationship, spread data points around line relatively even. Many students become fixated needing 'perfect' dataset follows exactly normal distribution. unlikely ever happen real data - learn trust instincts!Finally, data interval, continuous, normally distributed, relationship linear assumption homoscedasticity met, use Pearson correlation.","code":"\nis.na(mh)\nggplot(data = mh, aes(x = Abil)) +\n  geom_histogram()\nqqPlot(x = mh$Abil)## [1] 15  4\nggplot(data = mh, aes(x = Abil, y = IQ)) +\n  geom_point()+\n  geom_smooth(method = lm)## `geom_smooth()` using formula 'y ~ x'"},{"path":"correlations.html","id":"descriptives-of-the-correlation","chapter":"9 Correlations","heading":"9.3 Descriptives of the Correlation","text":"Now checked assumptions confirmed use Pearson correlation, next step descriptives. key thing keep mind scatterplot actually descriptive correlation. Meaning article, report, use scatterplot determine type correlation use also describe potential relationship regards hypothesis. always expect see scatterplot write-type analysis.","code":""},{"path":"correlations.html","id":"corr-a4","chapter":"9 Correlations","heading":"9.3.0.1 Activity 4: Descriptive statistics","text":"Looking scatterplot, spend couple minutes thinking describing relationship Ability IQ terms hypothesis. Remember descriptive analysis stage, nothing confirmed. relationship appear predicted hypothesis? discussion solutions end chapter.Hint 1: hypothesised reading ability intelligence positively correlated. see scatterplot?Hint 2: Keep mind subjective stage.Hint 3: Remember talk relationship prediction. correlational work, regression.Hint 4: Can say something strength (weak, medium, strong) direction (positive, negative)?addition scatterplot, can sometimes relevant include means standard deviations scales correlation. always relevant , example, measuring something like anxiety, stress, IQ, can informative include information help demonstrate sample compares population norms. calculate descriptives also good practice data-wrangling skills.new code chunk, write run code calculate mean score standard deviation Abil IQ using summarise() store output function object called descriptives\nName output calculations Abil_mean, Abil_SD, IQ_mean, IQ_SD. Make sure use exact spellings otherwise later activities work.\nhint: already seen calculate mean(), median(), number people (n()), sum() within summarise() function. descriptives sd(), min() max() can also calculated similar way using mean() median().\nName output calculations Abil_mean, Abil_SD, IQ_mean, IQ_SD. Make sure use exact spellings otherwise later activities work.hint: already seen calculate mean(), median(), number people (n()), sum() within summarise() function. descriptives sd(), min() max() can also calculated similar way using mean() median().performed correctly, view descriptives look similar :Answer following questions confirm understanding output:mean Reading Ability? 54.1256.1255.12What mean IQ? 99.04100.04101.04If population norm mean IQ 100, comparable sample population? samevery different","code":""},{"path":"correlations.html","id":"inferentials-of-the-correlation","chapter":"9 Correlations","heading":"9.4 Inferentials of the correlation","text":"Excellent! checked assumptions descriptives. data looks consistent population norms scatterplot suggest positive relationship two variables. Finally run correlation!often many different functions can used achieve thing actually going show two ways running correlation people prefer one approach data type results come easy work output.","code":""},{"path":"correlations.html","id":"corr-a5","chapter":"9 Correlations","heading":"9.4.0.1 Activity 5: Run the correlation","text":"First, use correlation() function correlation package. Remember help function can type e.g., ?correlation console window. correlation() function requires:name data set usingThe name first variable want select correlationThe name second variable want select correlationThe type correlation want run: e.g. pearson, spearmanThe type NHST tail want run: e.g. \"less\",\"greater\", \"two.sided\"example, data stored dat want two-sided pearson correlation variables (columns) X Y, :wanting run Pearson correlation two-sided alternative.new code chunk, using information correlation() , type run Pearson correlation IQ Ability store output object called results.View output typing View(results) console windowThe second method use cor.test() Base R function uses similar code correlation() except variables specified. cor.test() use object$variable syntax saw qqPlot():new code chunk, type run code view output typing results2 console.\nspecified x = mh$IQ meaning first variable, x, column IQ object mh.\nspecified x = mh$IQ meaning first variable, x, column IQ object mh.Look output differs results. come back shown two ways shortly.","code":"\ncorrelation(data = dat, \n            select = \"X\", \n            select2 = \"Y\",  \n            method = \"pearson\", \n            alternative = \"two.sided\")\nresults2 <-  cor.test(x = mh$IQ, \n                      y = mh$Abil, \n                      method = \"pearson\", \n                      alternative = \"two.sided\")"},{"path":"correlations.html","id":"interpreting-output-and-writing-up","chapter":"9 Correlations","heading":"9.5 Interpreting output and writing up","text":"Excellent work. can see, running correlation actually really quick, hard work checking assumptions data-wrangling. now tibble called results gives output correlation Reading Ability IQ school children measured Miller Haden (2013) Chapter 11. left now interpret output write .","code":""},{"path":"correlations.html","id":"corr-a6","chapter":"9 Correlations","heading":"9.5.0.1 Activity 6: Interpreting the correlation","text":"Look results answer following questions:value Pearson's r 2 decimal places? direction relationship Ability IQ : positivenegativeno relationshipThe strength relationship Ability IQ : strongmediumweakAssuming \\(\\alpha = .05\\) relationship Ability IQ : significantnot significantThe alternative hypothesis reading ability school children, measured standardized test, intelligence, standardized test, positively correlated. Based results can say alternative hypothesis: acceptedis rejectedis provenis provenThe test statistic, case r value, usually labelled estimate.Y increases X increases relationship positive. Y increases X decreases relationship negative. change Y X changes relationshipDepending field correlation values greater .5 strong; .3 .5 medium, .1 .3 small.field standard says less .05 significant p-value less .05.alternative hypothesis can accepted rejected, never proven. case, results matched alternative hypothesis therefore accepted. Remember null hypothesis hand can rejected retained.","code":""},{"path":"correlations.html","id":"corr-a7","chapter":"9 Correlations","heading":"9.5.0.2 Activity 7: Write-up","text":"Now interpreted output want write . Copy paste exactly white space R Markdown document knit file.knit code, assuming done tasks correctly, code pasted transform readable passage follows:shown Figure 7.5, appeared positive relationship Reading Ability (M = 55.12, SD = 6.08) IQ (M = 100.04, SD = 9.04), line alternative hypothesis. Pearson correlation found significant, medium positive correlation two variables (r (23) = 0.45, p = 0.024) alternative hypothesis therefore accepted.get fairly ok start write-. perfect good start. instance, r-value first 0 just r = .xx. Likewise, p-value first 0 either. always bit tidying .Note: Remember relationship said significant p-value relationship lower accepted level (normally called alpha set \\(\\alpha = .05\\)). Alternatively, relationship p-value higher accepted level said non-significantBut two approachesThe reason shown two methods performing correlations way outputs results. correlation() produces tibble means easy work pull values join another table needed already tidyverse format. cor.test() hand produces list type object, harder work . However, output cor.test() also happens work functions report package, report() report_table() give automatic report analyses. example, report() presents fixed write-correlation available information. correlations, perhaps less useful, however, complex statistics reporting function can really help learning data output, introducing now. report() currently work output correlation() showed ways. Run console window see mean - probably get error.Note: write-comes report considered something copy paste report. means just obtaining overview quickly help confirm thinking. issues presentation numbers writing, additional info needed. Basically, use functions approaches start writing, write-.","code":"As shown in Figure 7.5, there appeared to be a positive relationship between Reading Ability (M = `r round(pluck(descriptives$Abil_mean),2)`, SD = `r round(pluck(descriptives$Abil_SD),2)`) and IQ (M = `r round(pluck(descriptives$IQ_mean),2)`, SD = `r round(pluck(descriptives$IQ_SD),2)`), in line with the alternative hypothesis. A Pearson correlation found a significant, medium positive correlation between the two variables (r (`r results$df_error`) = `r round(results$r, 2)`, *p* = `r round(results$p, 3)`) and the alternative hypothesis is therefore accepted. \nreport(results2)"},{"path":"correlations.html","id":"multiple-correlations","chapter":"9 Correlations","heading":"9.6 Multiple Correlations","text":"Finally, round chapter, want briefly show running multiple correlations one. ran one correlation. However, lots variables dataset, get quick overview patterns, might want run correlations time create matrix scatterplots one time. can functions psych correlation packages (cor.test() works one correlation time). use Miller Haden data still tibble called mh.","code":""},{"path":"correlations.html","id":"corr-a8","chapter":"9 Correlations","heading":"9.6.0.1 Activity 8: Scatterplot matrix","text":"new code chunk, type run following code. pairs.panels()) function comes psych library creates matrix scatterplots, histograms, correlation coefficients can use give overview relationships one time. useful checking assumptions one place.\nFigure 9.4: Scatterplot matrix\nNotice something wrong? pairs.panels() create plots variables data (correlation() ). means correlated Participant ID number well, totally meaningless.Instead, can use pipes help us . code :Takes dataset mh ;Uses select() get rid Participant column ;Pipes remaining data pairs.panels() functionThe additional arguments:\nellipses = FALSE turns correlation ellipses,\nlm = TRUE use linear line best fit,\n`method = \"pearson\", specifies Pearson correlation.\nellipses = FALSE turns correlation ellipses,lm = TRUE use linear line best fit,`method = \"pearson\", specifies Pearson correlation.additional arguments adjust plot pairs.panel creates can look help documentation interested.produces:\nFigure 9.5: Adjusted scatterplot matrix\n","code":"\npairs.panels(mh)\nmh %>%\n  select(-Participant) %>%\n  pairs.panels(ellipses = FALSE, \n               lm = TRUE, \n               method = \"pearson\")"},{"path":"correlations.html","id":"corr-a9","chapter":"9 Correlations","heading":"9.6.0.2 Activity 9: Running multiple correlations","text":"perform multiple correlations one go, use correlation() function. package. Rather specifying two variables correlation, can also provide data frame multiple variables run possible correlations variables. Similar , want remove Participant column .method controls correlation computed, default pearson needed run non-parametric version change spearman.method controls correlation computed, default pearson needed run non-parametric version change spearman.p_adjust reason using correlation package. lectures discussed problem multiple comparisons - idea run lots lots tests false positive rate increase probability finding significant result increase.\nargument applies correction p-value adjusts number correlations performed. several different methods can look help documentation, setting bonferroni. default setting actually less conservative holm can read might chose instead help function typing ?correlation console window.\np_adjust reason using correlation package. lectures discussed problem multiple comparisons - idea run lots lots tests false positive rate increase probability finding significant result increase.argument applies correction p-value adjusts number correlations performed. several different methods can look help documentation, setting bonferroni. default setting actually less conservative holm can read might chose instead help function typing ?correlation console window.Note: running multiple correlations may positive may negative, option specify one two-tailed test.Note: running multiple correlations may positive may negative, option specify one two-tailed test.Run code calculate view correlation resultsRun code calculate view correlation resultsWhich produces following output:corr_results tibble lists results correlation corresponding statistics. Look table answer following questions:correlation Abil Home positive negative? PositiveNegativeThis means Abil scores increase, Home scores IncreaseDecreaseWhat strongest positive correlation? Abil * IQAbil * HomeAbil * TVWhat strongest negative correlation? Abil * TVIQ * TVHome * TVIs correlation Abil IQ significant? YesNoIs correlation Abil Home significant? YesNoHow describe strength correlation Home TV? WeakMediumStrongThink back lecture. calculating effect size?Negative correlations denoted negative r value.\nPositive correlations mean one score goes , negative correlations mean one score goes goes .\n3 & 4. Remember correlations take values -1 - 1 nearer one either direction stronger correlation (.e., r value 0 demonstrate lack relationship.\n5 & 6. traditional cut-significance .05. Anything .05 considered significant. careful pay attention decimal places.\nCohen's guidelines recommend weak = 1. - .3, medium = .3 - .5, strong > .5.\nr effect size.\nPositive correlations mean one score goes , negative correlations mean one score goes goes .\n3 & 4. Remember correlations take values -1 - 1 nearer one either direction stronger correlation (.e., r value 0 demonstrate lack relationship.\n5 & 6. traditional cut-significance .05. Anything .05 considered significant. careful pay attention decimal places.Cohen's guidelines recommend weak = 1. - .3, medium = .3 - .5, strong > .5.r effect size.Nice work! can really easy run lot correlations . However, need remember appropriate research. just wildly run every correlation can write favourite. PreRegistration ideas, Registered Reports, helps reduce Questionable Research Practices, just another example setting advance, plan , prevent bad practice!","code":"\ncorr_results <- mh %>%\n  select(-Participant) %>%\n  correlation(method = \"pearson\", \n              p_adjust = \"bonferroni\")\n\ncorr_results\nknitr::kable(corr_results)"},{"path":"correlations.html","id":"corr-fin","chapter":"9 Correlations","heading":"9.7 Finished!","text":"Excellent work today! can now add running, interpreting writing correlations list knowledge skills research methods toolbox. Remember actually lot work preparation data really running correlation just one function. might worthwhile repeating first activities two variables test understanding. questions, please post Teams.","code":""},{"path":"correlations.html","id":"test-yourself-3","chapter":"9 Correlations","heading":"9.8 Test Yourself","text":"Look code answer following questions:analysis show?\n\nrelationship IQ Reading Abilitythe relationship IQ time spent watching TV homethe relationship IQ time spent reading homethe effect IQ time spent reading Home\ntype correlation analysis ?\n\none-tailed pearson analysistwo-tailed pearson analysisone-tailed spearman analysistwo-tailed spearman analysis\nNow try running code answering following questions.three decimal places, r-value correlation IQ time spent reading Home?\n\n0.3340.5530.2020.988\nthree decimal places, p-value correlation IQ time spent reading Home?\n\n0.2020.3340.5530.988\ndegrees freedom correlation IQ time spent reading Home?\n\n25230.5530.988\nanalysis two-tailed pearson correlation looking relationship IQ amount time spent reading home. can tell two variables code IQ Home, code stating pearson, two-sided (another name two-tailed). run analysis find result r(23) = .202, p = .334.","code":"\nresults <- correlation(data = mh, \n                       select = \"IQ\", \n                       select2 = \"Home\",  \n                       method = \"pearson\", \n                       alternative = \"two.sided\")"},{"path":"correlations.html","id":"corr-sols","chapter":"9 Correlations","heading":"9.9 Activity solutions","text":"find solutions questions. look giving questions good try trying find help Google Teams issues.","code":""},{"path":"correlations.html","id":"corr-a1sol","chapter":"9 Correlations","heading":"9.9.0.1 Activity 1","text":"","code":"\nlibrary(car)\nlibrary(correlation)\nlibrary(report)\nlibrary(psych)\nlibrary(tidyverse)\nmh <- read_csv(\"MillerHadenData.csv\")"},{"path":"correlations.html","id":"corr-a3sol","chapter":"9 Correlations","heading":"9.9.0.2 Activity 3","text":"histogram IQThe qqPlot IQThe scatterplot","code":"\nggplot(data = mh, aes(x = IQ)) +\n  geom_histogram()\nqqPlot(x = mh$IQ)## [1]  3 14\nggplot(data = mh, aes(x = Abil, y = IQ)) +\n  geom_point()+\n  geom_smooth(method = lm)+\n  scale_x_continuous(name = \"Reading Ability\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"correlations.html","id":"corr-a4sol","chapter":"9 Correlations","heading":"9.9.0.3 Activity 4","text":"scatterplotBased scatterplot might suggest reading ability scores increase, IQ scores also increase appear data inline hypothesis two variables positively correlated. appears medium strength relationship.means standard deviations","code":"\ndescriptives <- summarise(mh, \n                          Abil_mean = mean(Abil),\n                          Abil_SD = sd(Abil),\n                          IQ_mean = mean(IQ),\n                          IQ_SD = sd(IQ))\n\ndescriptives"},{"path":"correlations.html","id":"corr-a5sol","chapter":"9 Correlations","heading":"9.9.0.4 Activity 5","text":"correlation using correlation()","code":"\nresults <- correlation(data = mh, \n                       select = \"IQ\", \n                       select2 = \"Abil\",  \n                       method = \"pearson\", \n                       alternative = \"two.sided\")\n\nresults"},{"path":"correlations.html","id":"words-from-this-chapter-8","chapter":"9 Correlations","heading":"9.10 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"t-tests.html","id":"t-tests","chapter":"10 t-tests","heading":"10 t-tests","text":"Experiments compare results two conditions two groups common within Psychology often want know effect given variable. One really confusing things however research design many names type design. clarify:One-sample used study one group people known norm criterion - example, comparing mean IQ sample known population norm IQ 100.Independent-samples -subjects designs mean thing - different participants different conditions.contrast, within-subjects, dependent-samples, paired-samples, repeated-measures tend mean participants conditionsMatched-pairs design means different people different conditions matched participants across conditions effectively person (e.g. age, IQ, Social Economic Status, etc)Mixed-design combination within-subjects -subjects designs one experiment. example, say looking attractiveness dominance male female faces. Everyone might see male female faces (within) half participants ratings attractiveness half participants ratings trustworthiness ().get better understanding tests run look running example -subjects t-test within-subjects t-test series activities. Remember solutions bottom page stuck, please ask questions forums.","code":""},{"path":"t-tests.html","id":"between-subjects-t-tests-two-sample","chapter":"10 t-tests","heading":"10.1 Between-Subjects t-tests (two-sample)","text":"begin looking -subjects t-test used comparing outcome two groups different people. using data Schroeder Epley (2015) perception people job applications. can take look Psychological Science article , Schroeder, J. Epley, N. (2015). sound intellect: Speech reveals thoughtful mind, increasing job candidate's appeal. Psychological Science, 26, 277--891., like essential completing activities. abstract article explains different experiments conducted, specifically looking data set Experiment 4, based information Open Stats Lab. abstract reads:person's mental capacities, intellect, observed directly instead inferred indirect cues. predicted person's intellect conveyed strongly cue closely tied actual thinking: voice. Hypothetical employers (Experiments 1-3b) professional recruiters (Experiment 4) watched, listened , read job candidates' pitches hired. evaluators (employers) rated candidate competent, thoughtful, intelligent heard pitch rather read , result, favourable impression candidate interested hiring candidate. Adding voice written pitches, trained actors (Experiment 3a) untrained adults (Experiment 3b) read , produced results. Adding visual cues audio pitches alter evaluations candidates. conveying one's intellect, important one's voice, quite literally, heard.summarise, 39 professional recruiters Fortune 500 companies evaluated job pitches M.B.. candidates University Chicago Booth School Business. methods results appear pages 887-889 article want look specifically details original data, wide format, can found Open Stats Lab website later self-directed learning. Today however, working modified version \"tidy\" format can downloaded plan reproduce results article Pg 887.","code":""},{"path":"t-tests.html","id":"data-and-descriptives","chapter":"10 t-tests","heading":"10.1.1 Data and Descriptives","text":"always, first activity getting ready analyse data try steps need help, consult earlier chapters.","code":""},{"path":"t-tests.html","id":"ttest-a1","chapter":"10 t-tests","heading":"10.1.1.1 Activity 1: Set-up","text":"Open RStudio set working directory chapter folder. Ensure environment clear.\nusing Rserver, avoid number issues restarting session - click Session - Restart R\nusing Rserver, avoid number issues restarting session - click Session - Restart ROpen new R Markdown document save working directory. Call file \"ttests\".Download evaluators.csv ratings.csv save t-test folder. Make sure change file names .\nprefer can download data zip folder clicking \nRemember change file names data.csv data (1).csv.\nprefer can download data zip folder clicking hereRemember change file names data.csv data (1).csv.Delete default R Markdown welcome text insert new code chunk loads following packages, specific order, using library() function. Remember solutions needed.\nLoad packages order, Hmisc, broom, car,effectsize, report, tidyverse\nused packages likely need install using install.packages(). Remember though machine console window. using RServer need install .\nLoad packages order, Hmisc, broom, car,effectsize, report, tidyverseagain used packages likely need install using install.packages(). Remember though machine console window. using RServer need install .Finally, load data held evaluators.csv tibble object named evaluators using read_csv().Remember look data help understand structure layout data. can whatever way prefer.Now data, explored , things can make working bit easier. look data, particular sex column, see actually coded numeric want treat categorical. Secondly, can tricky work 1s 2s mean people, can \"recode\" variables labels easier work . using combination mutate(), already know, recode() function dplyr package loaded part tidyverse, .factor() function base. Converting categorical data factors make easier work visualisations analysis.","code":""},{"path":"t-tests.html","id":"ttest-a2","chapter":"10 t-tests","heading":"10.1.1.2 Activity 2: Explore the dataset","text":"new code chunk, copy code see can follow .First use mutate() recode() recode sex new variable called sex_labels 1 = male 2 = female.\ncareful using recode() multiple functions different packages called name better use package::function() approach specify dplyr::recode() get right one.\ncareful using recode() multiple functions different packages called name better use package::function() approach specify dplyr::recode() get right one.use mutate() .factor() overwrite sex_labels condition factors.Now see can create count different sex labels answer following question. One approach group_by() %>% count() group ? Maybe store tibble object called eval_counts.many participants noted female: many participants noted male: many data points missing sex? ","code":"\nevaluators <- evaluators %>%\n  mutate(sex_labels = dplyr::recode(sex, \"1\" = \"male\", \"2\" = \"female\"),\n         sex_labels = as.factor(sex_labels),\n         condition = as.factor(condition))"},{"path":"t-tests.html","id":"ttest-a3","chapter":"10 t-tests","heading":"10.1.1.3 Activity 3: Ratings","text":"Excellent work. evaluator data ready work now going calculate called \"overall intellect rating\" given evaluator, calculated averaging ratings competent, thoughtful intelligent evaluator; held within ratings.csv. overall rating measure intellectual evaluators thought candidates , depending whether evaluators read listened candidates' resume pitches. Note, however, looking ratings individual candidates; looking overall ratings evaluator. bit confusing makes sense stop think little. interested medium received resume impacted rating candidate. done , combine overall intellect rating overall impression ratings overall hire ratings evaluator, end goal tibble called ratings2 - following structure:following steps describe create tibble good practice try . Look table think need? trick data analysis data wrangling first think want achieve - end goal - think functions need use get . solution hidden just stpes course want look . look steps. Steps 1, 2 3 calculate new overall intellect rating. Steps 4 5 combine rating information.Load data found ratings.csv tibble object called ratings. (e.g. read csv)Load data found ratings.csv tibble object called ratings. (e.g. read csv)filter() relevant variables (thoughtful, competent, intelligent) new tibble stored objected called something useful (call iratings), calculate mean Rating evaluator (e.g. group_by & summarise).filter() relevant variables (thoughtful, competent, intelligent) new tibble stored objected called something useful (call iratings), calculate mean Rating evaluator (e.g. group_by & summarise).Add new column called Category every entry word intellect. tells us every number tibble intellect rating. (e.g. mutate)Add new column called Category every entry word intellect. tells us every number tibble intellect rating. (e.g. mutate)Now create new tibble called ratings2 filter just \"impression\" \"hire\" ratings original ratings tibble.Now create new tibble called ratings2 filter just \"impression\" \"hire\" ratings original ratings tibble.Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2. (e.g. bind_rows(object1, object2))Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2. (e.g. bind_rows(object1, object2))Join ratings2 evaluator tibble created Task 1 (e.g. inner_join()). Keep necessary columns shown (e.g. select()) arrange Evaluator Category (e.g. arrange()).Join ratings2 evaluator tibble created Task 1 (e.g. inner_join()). Keep necessary columns shown (e.g. select()) arrange Evaluator Category (e.g. arrange()).Finally, calculate n, mean SD condition category help reporting descriptive statistics.","code":"\n# 1. load in the data\nratings <- read_csv(\"ratings.csv\")\n\n# 2. first step: pull out the ratings associated with intellect\niratings <- ratings %>%\n  filter(Category %in% c(\"competent\", \"thoughtful\", \"intelligent\"))\n\n# second step: calculate means for each evaluator\nimeans <- iratings %>%\n  group_by(eval_id) %>%\n  summarise(Rating = mean(Rating))\n\n# 3. add Category variable \n# this way we can combine with 'impression' and 'hire' into a single table, very useful!\nimeans2 <- imeans %>%\n  mutate(Category = \"intellect\")\n\n# 4., 5. & 6. combine into a single table\nratings2 <- ratings %>%\n  filter(Category %in% c(\"impression\", \"hire\")) %>%\n  bind_rows(imeans2) %>%\n  inner_join(evaluators, \"eval_id\") %>%\n  select(-age, -sex) %>%\n  arrange(eval_id, Category)\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(n = n(), m = mean(Rating), sd = sd(Rating))## `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument."},{"path":"t-tests.html","id":"visualising-two-groups","chapter":"10 t-tests","heading":"10.1.2 Visualising two groups","text":"Brilliant! Now data workable fashion, going start looking visualisations making figures. always visualise data run statistical analysis. Visualisations serve part descriptive measures help interpret results test also give understanding spread data part test assumptions. data categorical IV, going look using violin-boxplots saw introduction visualisation chapter. past people tended use barplots Newman Scholl (2012) point , barplots misleading viewers underlying data actually looks. can read paper like, info, hopefully end section see violin-boxplots informative.","code":""},{"path":"t-tests.html","id":"ttest-a4","chapter":"10 t-tests","heading":"10.1.2.1 Activity 4: Visualisation","text":"visualise intellect ratings listened read conditions. code use create figure follows explanation . Put code new code chunk run .first part code uses pipe filter data just intellect rating:ratings %>% filter(Category == \"intellect) filter(ratings, Category == \"intellect\")code also reflects nicely difference pipes (%>%) used wrangling + used visualisations ggplot. Notice switch pipes plus start adding layers visualisation.main parts code create violin-boxplot :ggplot() creates base layer sets data x y axes.geom_violin() creates density plot. reason called violin plot data normally distributed look something like violin.geom_boxplot() creates boxplot, showing median inter-quartile range (see like information). boxplot can also give good idea data skewed - median line middle box. median moved towards one th extremities box, data likely skewed.finally, use stat_summary() displaying mean confidence intervals. Within function, fun.data specifies summary function gives us summary data want plot, case, mean_cl_normal calculate mean plus upper lower confidence interval limits. also specify mean_se wanted standard error. Finally, geom specifies shape plot want use display summary, case want pointrange (literally point (mean) range (CI)).figure look like :\nFigure 10.1: Violin-boxplot evaluator data\nTry answer following question:condition evaluators give higher ratings overall? listenedreadWould descriptives (means, sds, figure) inline hypothesis evaluators favour resumes listened resumes read? yesnoNice informative figure huh? gives good representation data two conditions, clearly showing spread centre points. compare Figure 7 original paper see difference. actually get much information approach. even get sense maybe data questionable whether skewed , .code really useful well know want use . maybe play code try things see happens. instance:Try setting trim = TRUE, show.legend = FALSE, /altering value width see arguments .change Category == \"intellect\" Category == \"hire\" Category == \"impression\" create visualisations conditions.","code":"\nratings2 %>%\n  filter(Category == \"intellect\") %>%\nggplot(aes(x = condition, y = Rating)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(aes(fill = condition), width = .2, show.legend = FALSE) + \n  stat_summary(geom = \"pointrange\", fun.data = \"mean_cl_normal\")  +\n  labs(x = \"Condition\", y = \"Rating Score\")"},{"path":"t-tests.html","id":"assumptions","chapter":"10 t-tests","heading":"10.1.3 Assumptions","text":"Great. visualised data well able make descriptive analysis going . Now want get ready run actual analysis. one final thing going decide t-test? hang say, decide ? going run -subjects t-test! Right? Yes! , know say, one -subjects t-test can run. two common ones :Student's -subjects t-testWelch's -subjects t-testWe going recommend , least analysis code, use Welch's -subjects t-test reasons explained paper Delarce et al,m (2017) Now read paper effectively, Welch's -subjects t-test better maintaining false positive rate test (\\(\\alpha\\), usually set \\(\\alpha\\) = .05) requested level. show run Welch's t-test .assumptions Welch's -subjects t-test :data continuous, .e. interval/ratioThe data independentThe residuals normally distributed groupWe know 1 2 true design experiment, measures used, looking data. test assumption 3, can create Q-Q plots residuals. -subject t-test residuals difference mean group data point. E.g., mean group 10 participant group scores 12, residual participant 2.Thinking back lectures, ran Student's t-test instead Welch t-test, 4th assumption ? Homogeneity varianceHomoscedascityNominal data","code":""},{"path":"t-tests.html","id":"ttest-a5","chapter":"10 t-tests","heading":"10.1.3.1 Activity 5: Assumptions","text":"Run code calculate plot residuals \"listened\" condition \"intellect\" ratings.Run code calculate plot residuals \"read\" condition \"intellect\" ratings.look plots get something looks like listened condition:\nFigure 10.2: Residual plots listened condition. circle represents indivudal rater. data normally distributed fall close diagonal line.\nsomething like read condition.\nFigure 10.3: Residual plots read intellect condition. circle represents indivudal rater. data normally distributed fall close diagonal line.\nlooking data fall close diagonal line. Looking plots, maybe suggest \"listened\" condition great data points moving away line far ends. \"read\" condition seems bit better, least subjectively! always deviation diagonal line perhaps data plots relatively close respective diagonal lines.addition Q-Q plots can also run test residuals known Shapiro-Wilk test. Shapiro-Wilk's test alternative hypothesis data significantly different normal. , find significant result using test interpretation data normal. find non-significant finding interpretation data significantly different normal. One technical point test actually say data normal either just significantly different normal. , remember assumptions degree subjectivity . use shapiro.wilk() function base package run Shapiro-Wilk's test.new code chunk, run lines code look output.Try answer following questions:According Shapiro-Wilk's test, data normally distributed listened condition? YesNoAccording Shapiro-Wilk's test, data normally distributed read condition? YesNoSo can see, p-value listened condition p = .174, p-value read condition p = .445. interesting position often happens. figures \"listened\" bit unclear, figure \"read\" looks ok tests show non-significant difference normality. ? Well combine knowledge data make reasoned decision. situation majority information pointing data normal. However, known issues Shapiro-Wilks test small sample sizes must always take results like caution. never good idea run small sample reality might want design study larger sample groups. said, unreasonable take assumption normality held.\ninfo though, options convinced data normal.\n\nTransform data try normalise distribution. cover like know , page good start. usually recommended days still use .\n\nUse non-parametric test. non-parametric equivalent independent t-test Mann-Whitney equivalent paired-samples t-test Wilcoxon signed-ranks test. Though modern permutation tests better. cover useful know read paper.\n\nnothing. Delacre, Lakens & Leys, 2017 argue large enough sample (>30), Welch test robust deviations assumptions. large samples normality even less issue, design studies large samples.\n","code":"\nlistened_intellect_residuals <- ratings2 %>%\n  filter(condition == \"listened\", Category == \"intellect\") %>%\n  mutate(group_resid = Rating - mean(Rating)) %>%\n  select(group_resid)\n\nqqPlot(listened_intellect_residuals$group_resid)\nread_intellect_residuals <- ratings2 %>%\n  filter(condition == \"read\", Category == \"intellect\") %>%\n  mutate(group_resid = Rating - mean(Rating)) %>%\n  select(group_resid)\n\nqqPlot(read_intellect_residuals$group_resid)## [1] 6 8## [1] 11 18\nshapiro.test(x = listened_intellect_residuals$group_resid)\nshapiro.test(x = read_intellect_residuals$group_resid)"},{"path":"t-tests.html","id":"inferential-analysis","chapter":"10 t-tests","heading":"10.1.4 Inferential analysis","text":"Now checked assumptions data seems fit Welch's t-test can go ahead run test. going conduct t-tests Intellect, Hire Impression ratings separately; time comparing evaluators' overall ratings listened group versus overall ratings read group see significant difference two conditions: .e. evaluators listened pitches give significant higher lower rating evaluators read pitches.","code":""},{"path":"t-tests.html","id":"ttest-a6","chapter":"10 t-tests","heading":"10.1.4.1 Activity 6: Running the t-test","text":"First, create separate objects intellect, hire, impression data using filter(). completed intellect object replace NULLs code create one hire impression.finally ready run t-test. funny right, may realised now, work analysis involves set-getting data ready, running tests generally just one function. conduct t-test use t.test() function base takes following format called formula syntax:~ called tilde. can read '' \"analyse DV IV\".variable left tilde dependent outcome variable, DV_column_name.variable(s) right tilde independent predictor variable, IV_column_name.paired = FALSE indicates want run paired-samples test data -subjects design.run first test:new code chunk, type run code, thenview output typing intellect_t console.Similar used cor.test() correlations, output t.test() list type object can make harder work . time, going show use function tidy() broom package convert output tidyverse format.Run code. can read \"take object intellect_t try tidy tibble\".View object clicking results_intellect environment.see, results_intellect now nice tibble format makes easy extract individual values. worth looking values explanations:estimate difference two means (alphabetically entered mean 1 minus mean 2)estimate1 mean group 1estimate2 mean group 2statistic t-statisticp.value p-valueparameter degrees freedomcon.low conf.high confidence interval estimatemethod type test, Welch's, Student's, paired, one-samplealternative whether test one two-tailedAnd now know run test tidy , try :Complete code new code chunk replacing NULLs run t-tests hire impression ratings, tidy yet.now tidy data respective objects - hire_t results_hire, etc.sure look tests see outcome . make easier, going join results t-tests together using bind_rows() - can tibbles column names passed tidy().Copy run code. First, specifies individual tibbles want join gives label (hire, impression, intellect), specify ID column named (test).produces :looking along line p-values might significant differences. However, remember consider multiple comparisons.","code":"\nintellect <- filter(ratings2, Category == \"intellect\")\nhire <- NULL\nimpression <- NULL \nt.test(DV_column_name ~ IV_column_name, \n       paired = FALSE,\n       data = my_object)\nintellect_t <- t.test(Rating ~ condition, \n                      paired = FALSE, \n                      data = intellect,\n                      alternative = \"two.sided\")\nresults_intellect <- intellect_t %>%\n  tidy()\nhire_t <- NULL\nimpression_t <- NULL\nresults_hire <- NULL\nresults_impression <- NULL\nresults <- bind_rows(hire = results_hire, \n                     impression = results_impression, \n                     intellect = results_intellect, \n                     .id = \"test\")"},{"path":"t-tests.html","id":"ttest-a7","chapter":"10 t-tests","heading":"10.1.4.2 Activity 7: Correcting for multiple comparisons","text":"run three t-tests, actually increasing false positive rate due called familywise error - essentially, instead false positive rate .05, false positive rate 1-(1-.05)^3 = 0.142625, \"3\" formula number tests ran. correct can apply multiple comparison correction just like correlations ran lot correlations. , going add column results tibble shows adjusted p-values using p.adj() mutate().Type run code new code chunk look output.\ninside p.adjust(), p.value says column p-values , bonferroni says adjustment use.\ninside p.adjust(), p.value says column p-values , bonferroni says adjustment use.Looking adjusted p-values, try answer following questions:Listened significantly preferred hire condition adjusting multiple comparisons? TRUEFALSEListened significantly preferred impression condition adjusting multiple comparisons? TRUEFALSEListened significantly preferred intellect condition adjusting multiple comparisons? TRUEFALSE","code":"\nresults_adj <- results %>%\n  mutate(p.adjusted = p.adjust(p = p.value, \n                               method = \"bonferroni\"))"},{"path":"t-tests.html","id":"effect-size","chapter":"10 t-tests","heading":"10.1.5 Effect Size","text":"can see, even correcting multiple comparisons, effects still significant maintained false positive rate. one thing can add effect size. Remember effects significant large, significant medium, significant small. effect size tells us magnitude effect size way can compare across studies - said standardised - common effect size t-test called Cohen's D.","code":""},{"path":"t-tests.html","id":"ttest-a8","chapter":"10 t-tests","heading":"10.1.5.1 Activity 8: Effect size","text":"Whilst Cohen's D relatively straightforward hand, use function cohens_d() effectsize package. code similar syntax t.test().code run Cohen's D intellect completed .\nfirst argument specify formula, using syntax t.test(), dv ~ iv.\npooled_sd FALSE ran Welch test variances assumed equal TRUE ran regular Student's t-test.\nfirst argument specify formula, using syntax t.test(), dv ~ iv.pooled_sd FALSE ran Welch test variances assumed equal TRUE ran regular Student's t-test.Run complete code replacing NULLs calculate effect sizes hire impression","code":"\nintellect_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = intellect)\nhire_d <- NULL\nimpression_d <- NULL"},{"path":"t-tests.html","id":"interpretation","chapter":"10 t-tests","heading":"10.1.6 Interpretation","text":"Great Work! take second recap understanding data.","code":""},{"path":"t-tests.html","id":"ttest-a9","chapter":"10 t-tests","heading":"10.1.6.1 Activity 9: Interpreting the results","text":"results hire significant? Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places). Use adjusted p-values:\nMean estimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nresults hire significant? Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places). Use adjusted p-values:Mean estimate1 (listened condition) = Mean estimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = results impression significant? Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMeanestimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nresults impression significant? Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Meanestimate1 (listened condition) = Meanestimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = According Cohen's (1988) guidelines, effect sizes three tests SmallMediumLargeAccording Cohen's (1988) guidelines, effect sizes three tests SmallMediumLarge","code":""},{"path":"t-tests.html","id":"write-up","chapter":"10 t-tests","heading":"10.1.7 Write-Up","text":"finally -subjects t-test, look write .","code":""},{"path":"t-tests.html","id":"ttest-a10","chapter":"10 t-tests","heading":"10.1.7.1 Activity 10: Write-up","text":"refer back original paper pg 887, can see, example, authors wrote:particular, recruiters believed job candidates greater intellect—competent, thoughtful, intelligent—listened pitches (M = 5.63, SD = 1.61) read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p < .01, 95% CI difference = [0.85, 3.13], d = 1.16.compare findings, something like :bonferroni-corrected Welch t-test found recruiters rated job candidates intellectual listened resumes (M = 5.64, SD = 1.61) read resumes (M = 3.65, SD = 1.91), t(33.43) = 3.48, p = 0.004, 95% CI difference = [0.83, 3.15], d = 1.12.can create paragraph, using code, copying pasting exactly white space R Markdown document knitting file.Note replicated analysis exactly - authors paper conducted Student's t-test whilst conducted Welch tests also applied multiple comparison correction. can look two examples see difference. also worthwhile trying write-two remaining conditions moving within-subjects t-tests.","code":"A bonferroni-corrected Welch t-test found that recruiters rated job candidates as more intellectual when they listened to resumes (M = `r results_intellect$estimate1%>% round(2)`, SD = `r round(group_means$sd[3], 2)`) than when they read resumes (M = `r results_intellect$estimate2%>% round(2)`, SD = `r round(group_means$sd[6], 2)`), t(`r round(results_intellect$parameter, 2)`) = `r round(results_adj$statistic[3],2)`, p = `r results_adj$p.adjusted[3] %>% round(3)`, 95% CI of the difference = [`r round(results_intellect$conf.low, 2)`, `r round(results_intellect$conf.high, 2)`], d = `r round(intellect_d$Cohens_d,2)`. "},{"path":"t-tests.html","id":"within-subjects-paired-samples","chapter":"10 t-tests","heading":"10.2 Within-subjects (paired-samples)","text":"final activity run paired-samples t-test within-subject design go one quickly just point differences . example draw Open Stats Lab look data data Mehr, S. ., Song. L. ., & Spelke, E. S. (2016). 5-month-old infants, melodies social. Psychological Science, 27, 486-501.{target = \"_blank\"}.premis paper parents often sing children , even infants, children listen look parents sung . authors sought explore psychological function music parents infants, examining research question particular melodies may convey important social information infants. specifically, common knowledge songs melodies convey information social affiliation. authors argue melodies shared within social groups. Whereas children growing one culture may exposed certain songs infants (e.g., “Rock--bye Baby”), children growing cultures (even groups within culture) may exposed different songs. Thus, novel person (someone infant never seen ) sings familiar song, may signal infant new person member social group.test researchers recruited 32 infants parents take part following experiment. first visit lab, parents taught new lullaby (one neither infants heard ). experimenters asked parents sing new lullaby child every day next 1-2 weeks. Following 1-2 week exposure period, parents infant returned lab complete experimental portion study. Infants first shown screen side--side videos two unfamiliar people, silently smiling looking infant. researchers recorded looking behaviour (gaze) infants ‘baseline’ phase. Next, one one, two unfamiliar people screen sang either lullaby parents learned different lullaby (lyrics rhythm, different melody). Finally, infants saw silent video used baseline, researchers recorded looking behaviour infants ‘test’ phase. details experiment’s methods, please refer Mehr et al. (2016) Experiment 1.","code":""},{"path":"t-tests.html","id":"the-data-1","chapter":"10 t-tests","heading":"10.2.1 The Data","text":"","code":""},{"path":"t-tests.html","id":"ttest-a11","chapter":"10 t-tests","heading":"10.2.1.1 Activity 11: Getting the data ready","text":"First, download Mehr Song Spelke 2016 Experiment 1.csv clicking link putting working directory.\neasier can download data zip file clicking link.\neasier can download data zip file clicking link.Next, type run code new code chunk. code loads data wrangling get data working format:\nfilters just first experiment paper\nselects id preferential looking time babies baseline stage test stage.\nfinally renames two preferential looking time columns names easier work using rename() function.\nfilters just first experiment paperselects id preferential looking time babies baseline stage test stage.finally renames two preferential looking time columns names easier work using rename() function.","code":"\ngaze <- read_csv(\"Mehr Song and Spelke 2016 Experiment 1.csv\") %>%\n  filter(exp1 == 1) %>%\n  select(id,\n         Baseline_Proportion_Gaze_to_Singer,\n         Test_Proportion_Gaze_to_Singer) %>%\n  rename(baseline = Baseline_Proportion_Gaze_to_Singer,\n         test = Test_Proportion_Gaze_to_Singer)"},{"path":"t-tests.html","id":"assumptions-1","chapter":"10 t-tests","heading":"10.2.2 Assumptions","text":"now data ready work , sure look get understanding data, want consider assumptions within-subjects t-test.assumptions t-test little different (although similar) -subjects t-tests . areThe data continuous, .e. interval/ratioAll participants appear conditions/groups.residuals normally distributed.Aside data paired rather independent, .e. participants two conditions, rather two groups people different conditions, key difference within-subjects test, data actually determined difference scores two conditions participant. example, say participant one scores 10 condition 1 7 condition 2, data actually 3, participants. looking scored either condition , difference conditions. data must continuous residuals must normally distributed .","code":""},{"path":"t-tests.html","id":"ttest-a12","chapter":"10 t-tests","heading":"10.2.2.1 Activity 12: Assumptions","text":"Type run code first calculate difference scores (diff) residuals (group_resid).next plots Q-Q plot residuals carrying Shapiro-Wilk's test residualsAnd look plot see:Shapiro-Wilk's suggests:Now saw , Q-Q plot want data fall approximately diagonal line, Shapiro-Wilks test looking non-significant finding. Based two tests, can therefor say data meets assumption normality can proceed.","code":"\ngaze_residual <- gaze %>%\n  mutate(diff = baseline - test) %>%\n  mutate(group_resid = diff - mean(diff))\n\nqqPlot(gaze_residual$group_resid)\n\nshapiro.test(gaze_residual$group_resid)## [1] 22 29## \n##  Shapiro-Wilk normality test\n## \n## data:  gaze_residual$group_resid\n## W = 0.97818, p-value = 0.7451"},{"path":"t-tests.html","id":"descriptives","chapter":"10 t-tests","heading":"10.2.3 Descriptives","text":"Now going look descriptives. made sense keep data wide-form point make easy calculate column difference score, now transform tidy data can easily create descriptives plot data using tidyverse tools.","code":""},{"path":"t-tests.html","id":"ttest-a13","chapter":"10 t-tests","heading":"10.2.3.1 Activity 13: Descriptives and visualisations","text":"Type run code gather data using pivot_longer().Next create violin-boxplot data using knowledge (code) Activity 4 .Finally, create descriptives table contains n, mean, standard deviation condition.\nprefer, actually work difference scores instead two different conditions. Whilst analyse difference, people plot either difference two conditions descriptives.\nprefer, actually work difference scores instead two different conditions. Whilst analyse difference, people plot either difference two conditions descriptives.done step correctly, see plot looks like :\nFigure 7.7: Preferential Looking time infants baseline stage (left) test stage (right).\ndescriptives:look differences know plot confidence interval difference, essential . looking done worth spending minutes try predict outcome t-test null hypothesis difference preferential looking time babies baseline test conditions.","code":"\ngaze_tidy <- gaze %>%\n  pivot_longer(names_to = \"time\", \n               values_to = \"looking\", \n               cols = c(baseline, test))"},{"path":"t-tests.html","id":"inferential-analysis-1","chapter":"10 t-tests","heading":"10.2.4 Inferential Analysis","text":"brings us running t-test effect size. code almost identical independent code two differences:t.test() specify paired = TRUE rather FALSEIn cohens_d() specify method = paired rather pooled_sd","code":""},{"path":"t-tests.html","id":"ttest-a14","chapter":"10 t-tests","heading":"10.2.4.1 Activity 14: Paired-samples t-test","text":"Now go running within-subjects t-test based knowledge. data need gaze_tidy(). Store output t-test tibble object gaze_test\n.e. pipe output t-test `tidy() one line code.\n.e. pipe output t-test `tidy() one line code.calculate Cohen's D t-test store gaze_dAnd done correctly, see gaze_test something like :","code":"\ngaze_test <- NULL\ngaze_d <- NULL"},{"path":"t-tests.html","id":"write-up-and-interpretation","chapter":"10 t-tests","heading":"10.2.5 Write-Up and Interpretation","text":"Looking output test, actually similar -subjects t-test, one exception. Rather providing means conditions, single estimate. mean difference score two conditions calculated descriptives diff created get answer.Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMean estimate = \nt() = , p = \nEnter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Mean estimate = Mean estimate = t() = , p = t() = , p = ","code":""},{"path":"t-tests.html","id":"ttest-a15","chapter":"10 t-tests","heading":"10.2.5.1 Activity 15: Write-up","text":"Now go summarising finding sentence using standard APA formatting. hidden version just look go.test stage (M = .59, SD = .18), infants showed significantly longer preferential looking time singer familiar melody shown singer baseline (M = .52, SD = .18), t(31) = 2.42, p = .022, d = .41.Alternatively:test stage, infants showed significantly longer preferential looking time singer familiar melody shown singer baseline (Mean Difference = 0.07, SD = 0.17), t(31) = 2.42, p = .022, d = .41.","code":""},{"path":"t-tests.html","id":"ttest-fin","chapter":"10 t-tests","heading":"10.3 Finished!","text":"long chapter hopefully see really true hardest part set-data wrangling. said , need memorise lines code - just need remember find examples understand bits need change. Play around examples given see changing values . specific Test section chapter make sure check understanding different sections moving .","code":""},{"path":"t-tests.html","id":"ttest-sols","chapter":"10 t-tests","heading":"10.4 Activity solutions","text":"find solutions questions. look giving questions good try trying find help Google Teams issues.","code":""},{"path":"t-tests.html","id":"ttest-a1sol","chapter":"10 t-tests","heading":"10.4.0.1 Activity 1","text":"","code":"\nlibrary(broom)\nlibrary(car)\nlibrary(effectsize)\nlibrary(report)\nlibrary(tidyverse)\nevaluators <- read_csv(\"evaluators.csv\")"},{"path":"t-tests.html","id":"ttest-a2sol","chapter":"10 t-tests","heading":"10.4.0.2 Activity 2","text":"code:summarise give output:","code":"\nevaluators <- evaluators %>%\n  mutate(sex_labels = dplyr::recode(sex, \"1\" = \"male\", \"2\" = \"female\"),\n         sex_labels = as.factor(sex_labels),\n         condition = as.factor(condition))\neval_counts <- group_by(evaluators, sex_labels) %>% count()"},{"path":"t-tests.html","id":"ttest-a6sol","chapter":"10 t-tests","heading":"10.4.0.3 Activity 6","text":"","code":"\nintellect <- filter(ratings2, Category == \"intellect\")\nhire <- filter(ratings2, Category == \"hire\")\nimpression <- filter(ratings2, Category == \"impression\")"},{"path":"t-tests.html","id":"ttest-a8sol","chapter":"10 t-tests","heading":"10.4.0.4 Activity 8","text":"","code":"\nintellect_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = intellect)\nhire_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = hire)\nimpression_d <- cohens_d(Rating ~ condition, \n                      pooled_sd = FALSE, \n                      data = impression)"},{"path":"t-tests.html","id":"ttest-a13sol","chapter":"10 t-tests","heading":"10.4.0.5 Activity 13","text":"plot:descriptives:","code":"ggplot(gaze_tidy, aes(x = time, y = looking)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(aes(fill = time), width = .2, show.legend = FALSE) + \n  stat_summary(geom = \"pointrange\", fun.data = \"mean_cl_normal\") +\n  labs(x = \"Experimental Stage\", \"Preferential Looking Time (Proportion)\ndesc <- gaze_tidy %>% \n  group_by(time) %>% \n  summarise(n = n(), \n            mean_looking = mean(looking), \n            sd_looking = sd(looking))"},{"path":"t-tests.html","id":"ttest-a14sol","chapter":"10 t-tests","heading":"10.4.0.6 Activity 14","text":"t-test:Cohen's D:","code":"\ngaze_test <- t.test(looking ~ time, \n                    paired = TRUE, \n                    data = gaze_tidy) %>% \n  tidy()\ngaze_d <- cohens_d(looking ~ time, \n                   method = \"paired\", \n                   data = gaze_tidy)"},{"path":"t-tests.html","id":"words-from-this-chapter-9","chapter":"10 t-tests","heading":"10.5 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"power-and-effect-sizes.html","id":"power-and-effect-sizes","chapter":"11 Power and Effect Sizes","heading":"11 Power and Effect Sizes","text":"now mainly spent time data-wrangling, understanding probability, visualising data, recently, running inferential tests, .e. t-tests. lectures, however, also started learn additional aspects inferential testing trying reduce certain types error analyses. balance minimising error inferential statisitcs focus today.First thing remember , branch statistics using , Null Hypothesis Significance Testing (NHST), two types hypotheses, trying establish probability null hypothesis accepted. two hypotheses :null hypothesis states compared values equivalent , referring means, written : \\(H_0: \\mu_1 = \\mu_2\\)alternative hypothesis states compared values equivalent , referring means, written : \\(H_1: \\mu_1 \\ne \\mu_2\\).Now, decision hypothesis prone degree error , learn, two main types error worry Psychology :Type error - false positive, probability rejecting null hypothesis rejected (otherwise called alpha \\(\\alpha\\)). words, conclude real \"effect\" fact effect. field standard rate acceptable false positives \\(\\alpha = .05\\) meaning theory 1 20 studies may false positive.Type II error - false negative, probability retaining null hypothesis false (otherwise called beta \\(\\beta\\)). words, conclude real \"effect\" fact one. field standard rate acceptable false negatives \\(\\beta = .2\\) meaning theory 1 5 studies may false negative.Adding ideas hypotheses errors, going look idea power learn long-run probability correctly rejecting null hypothesis fixed effect size fixed sample size; .e. correctly concluding effect real effect detect. Power calculated \\(power = 1-\\beta\\) directly related False Negative rate. field standard False Negatives \\(\\beta = .2\\) field standard power \\(power = 1 - .2 = .8\\), given effect size sample size (though papers, including Registered Reports often required power least \\(power >= .9\\)). , \\(power = .8\\) means majority studies find effect one detect, assuming study maintains rates error power.past number studies fallen short field standard lack power thought key issue replication crisis. makes sense , think , previous studies \\(power = .5\\) .5 probability correctly rejecting null hypothesis. may large number studies null hypothesis rejected ; field becomes noisy point unsure studies replicate. issues like led us redevelop courses really want understand power much possible.","code":""},{"path":"power-and-effect-sizes.html","id":"designing-studies","chapter":"11 Power and Effect Sizes","heading":"11.1 Designing Studies","text":"reiterate, power defined probability correctly rejecting null hypothesis fixed effect size fixed sample size. , power key decision design study, premis higher power planned study, better.Two relationships learn chapter :given sample size \\(\\alpha\\), power study higher effect looking assumed large effect opposed small effect; large effects easier detect., given effect size \\(\\alpha\\), power study higher increase sample size.relationships see , little control size effect trying detect (lives real world control), can instead increase power study increasing size sample (also reducing sources noise measurement error study). , planning study, good researcher consider following four key elements - thank Dr Ian Walker (University Bath) excellent acronym - APES:alpha - commonly thought significance level (.e., p-value); usually set \\(\\alpha = .05\\)power - probability correctly rejecting null hypothesis given effect size sample size, typically set \\(power = .8\\).effect size - size relationship/difference two variablessample size - number observations (usually, participants, sometimes also stimuli) study.beautiful thing know three elements can calculate fourth. two common calculations prior study :determine appropriate sample size required obtain effect size interested . , prior experiment decide interested testing small, medium, large effect sizes, know everything except sample size - many people need run study. Generally, smaller effect size, participants need, assuming power alpha held constant .8 .05 respectively.know alpha, power, effect size want know sample size.determine smallest effect size can reliably detect given sample size. example, know everything except effect size. example, say taking secondary data approach using open dataset, know run 100 participants, add participants, want know minimum effect size reliably detect dataset.know alpha, power, sample size want know smallest effect size can determine.Hopefully gives idea use power determine sample sizes studies - sample size just pulled thin air. approaches described priori power analyses stating power level want (priori means ) study - though second approach determining smallest effect size can determine based known sample size also referred sensitivity power analysis. However, may now thinking though, everything connected, can use effect size study sample size determine power study run ? ! Well, can wrong . actually called Observed Post-Hoc power papers discourage calculating grounds effect size using true effect size population interested ; just effect size sample. indication power analysis misleading. Avoid . can read , , time like: Lakens (2014) Observed Power, editor asks post-hoc power analyses. brief, Observed Power conflates effect size sample effect size within population two . Stick using priori power analyses approaches use determine required sample size achievable reliable effect size.jump bit now start running analyses help understanding alpha, power, effect sizes sample size! start looking effect sizes, moving calculating power.","code":""},{"path":"power-and-effect-sizes.html","id":"effect-size-by-hand","chapter":"11 Power and Effect Sizes","heading":"11.2 Effect Size By Hand","text":"number different \"effect sizes\" can choose calculate common one t-tests, seen previously, Cohen's d: standardised difference two means (units SD) written d = effect-size-value. key point Cohen's d standardised difference, meaning can used compare studies regardless measurement made. Take example height differences men women estimated 5 inches (12.7 cm). effect size, unstandardised effect size every sample test, difference dependent measurement tools, measurement scale, errors contained within (Note: ask Helena time photocopied rulers). using standardised effect size allows make comparisons across studies regardless measurement error. standardised terms, height difference considered medium effect size (d = 0.5) Cohen (1988, cited Schafer Schwarz (2019)) defined representing \"effect likely visible naked eye careful observer\". Cohen (1988) fact stated three sizes Cohen's d people use guide:may wish read paper later different effect sizes psychology - Schafer Schwarz (2019) Meaningfulness Effect Sizes Psychological Research: Differences Sub-Disciplines Impact Potential Biases.thing note formula slightly different depending type t-test used can sometimes change depending read. worksheet, go following formulas:One-sample t-test & paired-sample t-test:\\[d = \\frac{t}{\\sqrt{N}}\\]Independent t-test:\\[d = \\frac{2 \\times t}{\\sqrt{df}}\\]now try calculations. start just looking effect sizes t-tests calculating power later tasks.","code":""},{"path":"power-and-effect-sizes.html","id":"power-a1","chapter":"11 Power and Effect Sizes","heading":"11.2.0.1 Activity 1: Set-up","text":"Open RStudio set working directory chapter folder. Ensure environment clear.\nusing Rserver, avoid number issues restarting session - click Session - Restart R\nusing Rserver, avoid number issues restarting session - click Session - Restart ROpen new R Markdown document save working directory. Call file \"APES\".Delete default R Markdown welcome text insert new code chunk loads following packages, specific order, using library() function. Remember solutions needed.\nLoad packages order, pwr, tidyverse\nused pwr package likely need install using install.packages(). Remember though machine console window. using RServer need install .\nLoad packages order, pwr, tidyversewe used pwr package likely need install using install.packages(). Remember though machine console window. using RServer need install .","code":""},{"path":"power-and-effect-sizes.html","id":"power-a2","chapter":"11 Power and Effect Sizes","heading":"11.2.0.2 Activity 2: Effect size from a one-sample t-test","text":"run one-sample t-test discover significant effect, t(25) = 3.24, p < .05. Using formulas, calculate d determine whether effect size small, medium large.Use appropriate formula one-sample t-tests.given t-value df (degrees freedom), still need determine n calculate d.According Cohen (1988), effect size small (.2 .5), medium (.5 .8) large (> .8).Answering following questions check answers. solutions bottom need :Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test probably considered: smallmediumlarge","code":""},{"path":"power-and-effect-sizes.html","id":"power-a3","chapter":"11 Power and Effect Sizes","heading":"11.2.0.3 Activity 3: Effect size from between-subjects t-test","text":"run -subjects t-test discover significant effect, t(30) = 2.9, p < .05. Calculate d determine whether effect size small, medium large.Use appropriate formula -subjects t-tests.According Cohen (1988), effect size small (.2 .5), medium (.5 .8) large (> .8).Answer following questions check answers. solutions bottom need :Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test probably considered: smallmediumlarge","code":""},{"path":"power-and-effect-sizes.html","id":"power-a4","chapter":"11 Power and Effect Sizes","heading":"11.2.0.4 Activity 4: t-value and effect size for a between-subjects Experiment","text":"run -subjects design study descriptives tell : Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t d -subjects experiment.run -subjects design study descriptives tell : Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t d -subjects experiment.Note: hint contains appropriate t-test formula unsure.Note: hint contains appropriate t-test formula unsure.can calculate d (using appropriate formula -subjects experiment), need first calculate t using formula:t = (Mean1 - Mean2)/sqrt((var1/n1) + (var2/n2))var stands variance formula. Variance standard deviation, right? Variance measured squared units. equation, require variance calculate t standard deviation, need remember var = SD^2.Now t-value, calculating d also need degrees freedom. Think calculate df -subjects experiment, taking n Group 1 Group 2 account.Remember convention people report t d values positive.Answer following questions check answers. solutions bottom need :Enter correct t-value test, rounded two decimal places: Enter correct t-value test, rounded two decimal places: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Based t-value , enter correct value d analysis rounded 2 decimal places: Based t-value , enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test probably described : smallmediumlargeAccording Cohen (1988), effect size t-test probably described : smallmediumlargeExcellent! Now comfortable calculating effect sizes, look using establish sample size required power. One thing realise progress true effect size population something know, need justify one design. clever approach laid Daniel Lakens blog Smallest Effect Size Interest (SESOI) - set smallest effect researcher interested ! can determined theoretical analysis, previous studies, pilot studies, rules thumb like Cohen (1988). However, also keep mind lower effect size, larger sample size need. Everything trade-.","code":""},{"path":"power-and-effect-sizes.html","id":"power-calculations","chapter":"11 Power and Effect Sizes","heading":"11.3 Power Calculations","text":"Today use functions pwr.t.test(), pwr.r.test() pwr.chisq.test package pwr run power calculations t-tests, correlations chi-square.","code":""},{"path":"power-and-effect-sizes.html","id":"t-tests-1","chapter":"11 Power and Effect Sizes","heading":"11.3.1 t-tests","text":"Remember information function, example pwr.t.test(), simply ?pwr.t.test console. can look webpages later get idea (bad ideas spot erroneously calculate post-hoc power!):quick-R summary pwr package - https://www.statmethods.net/stats/power.htmlthe pwr package vignette -  https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.htmlFrom see pwr.t.test() takes series inputs:n - Number observations/participants, per group independent samples version, number subjects matched pairs paired one-sample designs.d - effect size interest (Cohen's d) - difference means divided pooled standard deviationsig.level - significance level (False Positive Rate) \\(\\alpha\\)power - power test (1 minus False Negative Rate) \\(1-\\beta\\)type - type t test : one.sample, two.sample, pairedalternative - type hypothesis; \"two.sided\", \"greater\", \"less\"function works leave one principle. give information returns element missing. , example, say needed know many people per group (n) need detect effect size d = 0.4 power = .8, alpha = .05 two.sample (-subjects) t-test two.sided hypothesis test.","code":""},{"path":"power-and-effect-sizes.html","id":"power-a5","chapter":"11 Power and Effect Sizes","heading":"11.3.1.1 Activity 5: pwr.t.test()","text":"Run code:output tells need 99.0803248 people per condition. get whole people like conservative estimates actually run 100 per condition. lot people!!!make output pwr.t.test() easier work , going amend code just give us exactly number want.pluck() pull value analysis want. e.g. pluck(\"n\") give us sample size pluck(\"d\") give us effect size.ceiling() rounds give us next highest whole numberNote: ceiling() better use round() dealing people always rounds . example, ceiling(1.1) gives 2. round() hand useful rounding effect size, example, two decimal places - e.g. d = round(.4356, 2) give d = 0.44. use ceiling() sample sizes round() effect sizes.","code":"\npwr.t.test(d = .4,\n           power = .8,\n           sig.level = .05,\n           alternative = \"two.sided\",\n           type = \"two.sample\")\npwr.t.test(d = .4,\n           power = .8,\n           sig.level = .05,\n           alternative = \"two.sided\",\n           type = \"two.sample\") %>% \n  pluck(\"n\")\n  ceiling()"},{"path":"power-and-effect-sizes.html","id":"power-a6","chapter":"11 Power and Effect Sizes","heading":"11.3.1.2 Activity 6: Sample size for standard power one-sample t-test","text":"Assuming interested detecting minimum Cohen's d d = 0.23, minimum number participants need one-sample t-test, assuming power = .8, \\(\\alpha\\) = .05, two-sided hypothesis?Using pipeline, store answer single, rounded value called sample_size_t (.e. use pluck() %>% ceiling()).Use list inputs kind check-list clearly determine inputs known unknown. can help enter appropriate values code.structure pwr.t.test() similar one shown except two.sample become one.sampleYou also need use pluck(\"n\") help obtain sample size %>% ceiling() round nearest whole participant.Answer following question check answers. solutions bottom need :Enter minimum number participants need one-sample t-test: ","code":""},{"path":"power-and-effect-sizes.html","id":"activity-7-effect-size-from-a-high-power-between-subjects-t-test","chapter":"11 Power and Effect Sizes","heading":"11.3.1.3 Activity 7: Effect size from a high power between-subjects t-test","text":"Assuming run -subjects t-test 50 participants per group want power .9, minimum effect size can reliably detect? Assume standard \\(\\alpha\\) alternative hypothesis settings.Answer following questions check answers. solutions bottom need :Based information given, set type function? one.sampletwo.sampleBased output, enter minimum effect size can reliably detect test, rounded two decimal places: According Cohen (1988), effect size t-test smallmediumlargeSay run study find effect size determined d = 0.50. Given know power, select statement true: study sufficiently powered analysis indicates can detect effect sizes smaller d = 0.65the study underpowered analysis indicates can detect effect sizes larger d = 0.65","code":""},{"path":"power-and-effect-sizes.html","id":"uneven-groups","chapter":"11 Power and Effect Sizes","heading":"11.3.1.4 Uneven groups","text":"additional function worthwhile knowing called pwr.t2n.test() allows run power analyses t-tests uneven sample sizes two groups. instance, say wanted know minimum effect size determine -subjects t-test 25 participants one group 30 participants second group. additional aspect function instead n =, :n1 = ... number people group 1n2 = ... number people group 2note type argument function two samples.Assuming \\(\\alpha = .05\\), Power = .8, two-tailed test, :Meaning minimum effect size determine d = 0.773.","code":"\npwr.t2n.test(n1 = 25,\n             n2 = 30,\n             power = .8,\n             sig.level = .05,\n             alternative = \"two.sided\") %>%\n  pluck(\"d\") %>%\n  round(3)## [1] 0.773"},{"path":"power-and-effect-sizes.html","id":"correlations-1","chapter":"11 Power and Effect Sizes","heading":"11.3.2 Correlations","text":"Now, going thing correlation analysis using pwr.r.test. structure function similar pwr.t.test() works leave-one-principle:n - Number observationsr - Correlation coefficientsig.level - Significance level (Type error probability)power - Power test (1 minus Type II error probability)alternative - character string specifying alternative hypothesis, must one two.sided (default), greater (positive correlation) less (negative correlation).","code":""},{"path":"power-and-effect-sizes.html","id":"power-a8","chapter":"11 Power and Effect Sizes","heading":"11.3.2.1 Activity 8: Sample size for a correlation","text":"Assuming interested detecting minimum correlation r = .4 (either direction), minimum number participants need correlation analysis, assuming power = .8, \\(\\alpha\\) = .05?Using pipeline, store answer single, rounded value called sample_size_r (.e. use pluck() %>% ceiling()).Enter minimum number participants need correlation: ","code":""},{"path":"power-and-effect-sizes.html","id":"power-a9","chapter":"11 Power and Effect Sizes","heading":"11.3.2.2 Activity 9: Effect size for a correlation analysis","text":"run correlation analysis 50 participants standard power alpha levels hypothesised positive correlation, minimum effect size can reliably detect?\nAnswer following questions check answers. solutions bottom need :run correlation analysis 50 participants standard power alpha levels hypothesised positive correlation, minimum effect size can reliably detect?\nAnswer following questions check answers. solutions bottom need :Based information given, set alternative function? two.sidedgreaterlessBased information given, set alternative function? two.sidedgreaterlessBased output, enter minimum effect size can reliably detect test, rounded two decimal places: Based output, enter minimum effect size can reliably detect test, rounded two decimal places: According Cohen (1988), effect size correlation smallmediumlargeAccording Cohen (1988), effect size correlation smallmediumlargeSay run study find effect size determined d = 0.24. Given know power, select statement true:\n\nstudy sufficiently powered analysis indicates can detect effect sizes smaller d = 0.24the study underpowered analysis indicates can detect effect sizes larger d = 0.34\nSay run study find effect size determined d = 0.24. Given know power, select statement true:","code":""},{"path":"power-and-effect-sizes.html","id":"effect-sizes-in-published-research","chapter":"11 Power and Effect Sizes","heading":"11.3.3 Effect Sizes in Published Research","text":"","code":""},{"path":"power-and-effect-sizes.html","id":"power-a10","chapter":"11 Power and Effect Sizes","heading":"11.3.3.1 Activity 10: Power of published research","text":"Thus far used hypothetical situations - now go look paper Open Stats Lab website called Music Convey Social Information Infants? (used dataset t-test chapter). can download pdf look , determine power significant t-tests reported Experiment 1 Results section Pg489. one-sample t-test paired-samples t-test consider, summarised . Assume testing power = .8, alpha = .05. Based calculations either stated effects underpowered?one-sample: t(31) = 2.96, p = .006, d = 0.52paired t-test: t(31) = 2.42, p = .022, d= 0.43To calculate n: n = df + 1.t-tests believe underpowered? think may ? Additional information can found solution task 8 end activity.One caveat Task 10: keep mind looking single studies using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better can detect smaller effect sizes!","code":""},{"path":"power-and-effect-sizes.html","id":"power-fin","chapter":"11 Power and Effect Sizes","heading":"11.4 Finished!","text":"Great! Hopefully now starting see interaction alpha, power, effect sizes, sample size. always want really high powered studies depending size effect interested (small large), \\(\\alpha\\) level, mean need run less participants make sure study well powered. Points note:Lowering \\(\\alpha\\) level (e.g. .05 .01) reduce power.Lowering effect size (e.g. .8 .2) reduce power.Increasing power (.8 .9) require participants.high-powered study looking detect small effect size low alpha require large number participants!additional functions pwr package types statistical analyses. include calculates part ANOVA regression chapters.want examples power reinforce understanding, go back calculate power t-tests, correlations, chi-squares earlier chapters.","code":""},{"path":"power-and-effect-sizes.html","id":"test-yourself-4","chapter":"11 Power and Effect Sizes","heading":"11.5 Test Yourself","text":"Assuming running -subjects t-test secondary data (\\(\\alpha = .05\\), Power = .8, alternative = two-tailed) secondary data 100 participants groups. smallest effect size, three decimal places, determine data :\n\nd = 0.280d = 0.281d = 0.399d = 0.398\ncode test :Meaning smallest effect size d = 0.39Assuming running -subjects t-test secondary data (\\(\\alpha = .05\\), Power = .8, alternative = two-tailed) secondary data 60 participants Group 1 40 participants Group 2. smallest effect size, three decimal places, determine data :\n\nr = .578d = 0.578d = 0.577r = .577\ncode test :Meaning smallest effect size d = 0.578Assuming ran correlation secondary data (\\(\\alpha = .05\\), Power = .8, alternative = two-tailed) secondary data 50 observations. smallest effect size, three decimal places, determine data :\n\nr = .385r = .384r = .276r = .275\ncode test :Meaning smallest effect size r = .384","code":"\npwr.t.test(n = 100, \n           sig.level = .05, \n           power = .8,\n           type = \"two.sample\",\n           alternative = \"two.sided\") %>% \n  pluck(\"d\") %>% \n  round(3)\npwr.t2n.test(n1 = 60,\n           n2 = 40,\n           sig.level = .05, \n           power = .8,\n           alternative = \"two.sided\") %>% \n  pluck(\"d\") %>% \n  round(3)\npwr.r.test(n = 50,\n           sig.level = .05, \n           power = .8,\n           alternative = \"two.sided\") %>% \n  pluck(\"r\") %>% \n  round(3)"},{"path":"power-and-effect-sizes.html","id":"power-sols","chapter":"11 Power and Effect Sizes","heading":"11.6 Activity solutions","text":"find solutions questions. look giving questions good try trying find help Google Teams issues.","code":""},{"path":"power-and-effect-sizes.html","id":"power-a1sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.1 Activity 1","text":"","code":"\nlibrary(pwr)\nlibrary(broom)\nlibrary(tidyverse)"},{"path":"power-and-effect-sizes.html","id":"power-a2sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.2 Activity 2","text":"","code":"\nd <- 3.24 / sqrt(25 +1)\n\n# effect is medium to large; d = .64"},{"path":"power-and-effect-sizes.html","id":"power-a3sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.3 Activity 3","text":"","code":"\nd <- (2*2.9) / sqrt(30)\n\n# effect is large; d = 1.06"},{"path":"power-and-effect-sizes.html","id":"power-a4sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.4 Activity 4","text":"","code":"\nt = (10 - 11)/sqrt((1.3^2/30) + (1.7^2/30))\n\nd = (2*t)/sqrt((30-1) + (30-1))\n\n# t = 2.56\n# d = .67\n\n# Remember that convention is that people report the t and d as positive."},{"path":"power-and-effect-sizes.html","id":"power-a6sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.5 Activity 6","text":"[1] 151","code":"\nsample_size_t <- pwr.t.test(d = .23,\n                            power = .8, \n                            sig.level = .05, \n                            alternative = \"two.sided\", \n                            type = \"one.sample\") %>% pluck(\"n\") %>% ceiling()\n\nsample_size_t"},{"path":"power-and-effect-sizes.html","id":"power-a7sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.6 Activity 7","text":"[1] 0.655","code":"\npwr.t.test(n = 50,\n           power = .9, \n           sig.level = .05, \n           alternative = \"two.sided\", \n           type = \"two.sample\") %>%\n  pluck(\"d\") %>%\n  round(3)"},{"path":"power-and-effect-sizes.html","id":"power-a8sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.7 Activity 8","text":"","code":"\nsample_size_r <- pwr.r.test(r = .4, \n                            sig.level = .05, \n                            power = .8, \n                            alternative = \"two.sided\") %>%\n  pluck(\"n\") %>% \n  ceiling()"},{"path":"power-and-effect-sizes.html","id":"power-a9sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.8 Activity 9","text":"[1] 0.344","code":"\npwr.r.test(n = 50,\n           sig.level = .05, \n           power = .8, \n           alternative = \"greater\") %>%\n  pluck(\"r\") %>%\n  round(3)"},{"path":"power-and-effect-sizes.html","id":"power-a10sol","chapter":"11 Power and Effect Sizes","heading":"11.6.0.9 Activity 10","text":"Achievable Cohen d Example 1[1] 0.51Giving achievable effect size 0.51 found effect size 0.52.study seems ok authors achieve effect size low .51 found effect size .52Achievable Cohen d Example 2[1] 0.51Giving achievable effect size 0.51 found effect size 0.43.effect might reliable given effect size found much lower achievable effect size. issue researchers established sample size based previous effect size minimum effect size find important. effect size small .4 important powered studies level ran appropriate n ~52 babies (see ). Flipside course obtaining 52 babies isnt easy; hence people consider Many Labs approach good way ahead.ONE CAVEAT making assumption study therefore flawed, keep mind one study using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better.calculate actual sample size required achieve power .8:[1] 52Suggesting sample size n = 52 appropriate.","code":"\npwr.t.test(power = .8, \n           n = 32, \n           type = \"one.sample\", \n           alternative = \"two.sided\", \n           sig.level = .05) %>%\n  pluck(\"d\") %>%\n  round(2)\npwr.t.test(power = .8, \n           n = 32, \n           type = \"paired\", \n           alternative = \"two.sided\", \n           sig.level = .05) %>%\n  pluck(\"d\") %>%\n  round(2)\nsample_size <- pwr.t.test(power = .8,\n                          d = .4, \n                          type = \"paired\", \n                          alternative = \"two.sided\",\n                          sig.level = .05) %>%\n  pluck(\"n\") %>% \n  ceiling()\n\nsample_size"},{"path":"power-and-effect-sizes.html","id":"words-from-this-chapter-10","chapter":"11 Power and Effect Sizes","heading":"11.7 Words from this Chapter","text":"find list words used chapter might new case helps somewhere refer back mean. links table take entry words PsyTeachR Glossary. Note Glossary written numerous members team may use slightly different terminology shown chapter.","code":""},{"path":"power-and-effect-sizes.html","id":"additional-information","chapter":"11 Power and Effect Sizes","heading":"11.8 Additional Information","text":"","code":""},{"path":"power-and-effect-sizes.html","id":"a-blog-on-how-to-choose-an-effect-size-of-interest","chapter":"11 Power and Effect Sizes","heading":"11.8.1 A blog on how to choose an effect size of interest","text":"really quick analogy Ian Walker's \"Research Methods statistics\", say test stats test telescope. say telescope specifically designed spotting animals size elephants larger (similar saying cohens d .8 greater example - big effect). telescope can reliably detect something size elephant look see something smaller think might mouse, say \"object\"\" definitely mouse enough power telescope - blurry. likewise rule mouse something know sure - true telescope designed spot things size elephant larger. bought telescope able spot elephants interested . interested spotting mice bought powerful telescope. point Lakens' SESOI (Smallest Effect Size Interest) blog mentioned start - power minimum effect size (minimum object size) interested . imperative decide study effect interested - can base previous literature theory.","code":""},{"path":"power-and-effect-sizes.html","id":"a-blog-on-interpreting-and-writing-up-power","chapter":"11 Power and Effect Sizes","heading":"11.8.2 A blog on interpreting and writing up power","text":"points interpreting power consolidate things bit. Firstly, great now thinking power effect sizes first place. important becomes second nature thinking design study future years future studies first question ask designing study/secondary analysis size APES - Alpha, Power, Effect Size Sample. remember priori power analysis way ahead. power alpha determined advance study using determine effect size sample size.Power stated commonly papers now start notice Methods Results sections. see something along lines \"Based power =..... alpha =...., given X voices sample, power analysis (pwr package citation) revealed d = ...... minimum effect sizes reliably determine.\"interpret study terms power? Well, lets say run power analysis t-test (correlation), set smallest effect size interest d = .4 (equivalent r-value). run analysis find d = .6 effect significant, study enough power determine effect. effect found bigger effect found. can confidence reliable effect given power alpha values. However, say instead d = .6 found significant effect effect size just .4, say d = .3 - effect size found smaller smallest effect reliably find. case cautious still unclear whether actually effect whether found effect chance due study enough power reliably detect effect size small. say sure effect effect. need consider stances write . Remember though sampled population, representative sample population also influence validity power. sample give slightly different effect size.Alternatively, probably quite likely many degree projects due time constraints, say find non-significant effect effect size smaller predicted; say find non-significant effect effect size d = .2 power analysis said reliably detect effect small d = .4. issue determine solely based study ) non-significant effect powered b) non-significant effect actually effect first place. discussion need consider stances. can however say effect looking bigger d = 0.4. still useful information. Ok know small effect really , can rule effect size bigger original d-value. turn helps future researchers plan studies better can guide better knowing many participants run. See useful published null findings!Basically, test finds effect size smaller can detect, know know - sure mouse know elephant. Instead use previous findings support object mouse caveat conclusion suggestion test really sensitive finding mouse. Similar finding effect size smaller can detect. can use previous literature support effect rule sure. might actually found effect powerful test. Just like might able determine mouse powerful telescope.Taking bit studies really enough power (terms N - say study 25000 participants) find flea proverbial mouse, nevertheless non-significant finding. case fortunate situation well-powered study can say degree confidence hypothesis design unlikely ever produce replicable significant result. probably certain can get science close can get \"fact\", rare precious thing. However, incredibly high powered studies, lots participants, tend able find difference significant difference. within-subjects design 10000 participants (Power = .8, \\(\\alpha = .05\\)) can determine reliably detect incredibly small effect size d = 0.04. question stage whether effect real world significance meaning.take-home message discussion always consider result relation hypothesis, integrating previous research theory, additional issue power, discussion also consider result relation whether can truly determine effect might resolved (e.g. re-assessing effect size, changing design (within powerful), low sample, power high (e.g. .9), alpha low (e.g. .01)). issue power probably small part generalisability/limitation section.finally, n can swap effect relationship, d r, analyses accordingly.end chapter. sure look anything unsure make notes help develop knowledge skills. good write questions unsure see can answer later speak someone . Good work today!","code":""},{"path":"screening-data.html","id":"screening-data","chapter":"12 Screening Data","heading":"12 Screening Data","text":"chapter going focus screen datasets potential issues reinforce concept tidy data. far, given complete datasets work , however, find real data often much messier , example, participants may answer items questionnaire may errors implausible values dataset. also going show different function make calculating descriptive statistics easier.","code":""},{"path":"screening-data.html","id":"screening-a1","chapter":"12 Screening Data","heading":"12.1 Activity 1: Set-up","text":"following.Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Screening Data\".Download messy.csv save Screening Data folder. Make sure change file name .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads tidyverse psych packages using library() function loads data object named messy using read_csv()","code":""},{"path":"screening-data.html","id":"screening-a2","chapter":"12 Screening Data","heading":"12.2 Activity 2: Look at the data","text":"messy simulated data experiment looking effect note-taking test performance whether affected native speaker. Participants first given pre-test judge baseline knowledge, watch lecture take notes. Immediately lecture finished take another test. Finally, tested week delay. maximum score test 30. Participants lose marks incorrect answers minus scores also possible. dataset six variables:id = participant ID numberage = age participantspeaker = participant native non-native English speakergender = participant male, female, non-binarypre = pre-test score notes takenpost = post-test score immediately lecturedelay = test score one week delay","code":""},{"path":"screening-data.html","id":"missing-data","chapter":"12 Screening Data","heading":"12.3 Missing data","text":"first issue cover missing data. Data can missing participants accidentally fill question, can missing intentionally want answer, turn final testing session, something wrong whilst setting questionnaire/experiment save. Real data frequently contains missing values important know identify missing data can .","code":""},{"path":"screening-data.html","id":"screening-a3","chapter":"12 Screening Data","heading":"12.4 Activity 3: summary()","text":"good way get sense many missing data points use summary(). speaker gender text rather numbers, order see many values missing first need convert factors.Run codeAs can see, 20 data points missing (NAs) speaker, gender, delay (importantly, just 20 participants).several different approaches dealing missing data. cover common.","code":"\nmessy <- messy %>%\n  mutate(speaker = as.factor(speaker), \n         gender = as.factor(gender))\n\nsummary(messy)"},{"path":"screening-data.html","id":"screening-a4","chapter":"12 Screening Data","heading":"12.5 Activity 4: Listwise deletion","text":"One method dealing missing data listwise deletion. approach removes participant single missing value. missing data columns dataset, participant removed left complete datasets. can achieve using drop_naRun code view object.can see messy_listwise now contains data participants complete set data. might seem like good thing, sometimes appropriate option, however, couple important points consider.First, gender part experiment - one IVs, just demographic information. include gender analyses listwise deletion deleted experimental data participant missing gender. related second problem using full listwise deletion may result loss lot data. Look environment pane - original dataset 200 participants, using drop_na() 143 lost 25% data . real data also want check missing values coming one particular group (.e., non-random attrition).One option amend use drop_na() include gender can using code using select().Run code. many observations messy_listwise2 ? ","code":"\nmessy_listwise <- drop_na(messy)\nmessy_listwise2 <- drop_na(messy, -gender)"},{"path":"screening-data.html","id":"pairwise-deletion","chapter":"12 Screening Data","heading":"12.6 Pairwise deletion","text":"alternative listwise deletion pairwise deletion cases removed depending upon analysis. example, calculate correlations pre, post, delay without removing participants missing data delay condition, R use different numbers participants correlation depending missing data can see Sample Sizes section.","code":"## \n## CORRELATIONS\n## ============\n## - correlation type:  pearson \n## - correlations shown only when both variables are numeric\n## \n##         pre     post    delay   \n## pre       .    0.448*** 0.512***\n## post  0.448***     .    0.548***\n## delay 0.512*** 0.548***     .   \n## \n## ---\n## Signif. codes: . = p < .1, * = p<.05, ** = p<.01, *** = p<.001\n## \n## \n## p-VALUES\n## ========\n## - total number of tests run:  3 \n## - correction for multiple testing:  holm \n## \n##         pre  post delay\n## pre       . 0.000 0.000\n## post  0.000     . 0.000\n## delay 0.000 0.000     .\n## \n## \n## SAMPLE SIZES\n## ============\n## \n##       pre post delay\n## pre   200  200   180\n## post  200  200   180\n## delay 180  180   180"},{"path":"screening-data.html","id":"screening-a5","chapter":"12 Screening Data","heading":"12.7 Activity 5: na.rm = TRUE","text":"running inferential tests like correlations t-tests, R usually know ignore missing values. However, calculating descriptive statistics want calculate average score number different items, need explicitly tell R ignore missing values.Run code calculate mean score testing condition.mean score delay shows NA. R trying calculate average dataset including missing value creates logical problem (take average nothing?). order calculate mean tell R ignore missing values adding na.rm = TRUE code. can read \"remove NAs? Yes\".Run code. mean score delay condition 2 decimal places? \nreally important think whether want calculate descriptives participants missing data. example, calculating average reaction time hundreds trials, missing data points affect validity mean. However, using standardised questionnaire validated using complete responses participants answer 3/10 questions, may appropriate calculate mean score remaining data.\n","code":"\nsummarise(messy, \n          pre_mean = mean(pre),\n          post_mean = mean(post),\n          delay_mean = mean(delay)\n          )"},{"path":"screening-data.html","id":"screening-a6","chapter":"12 Screening Data","heading":"12.8 Activity 6: Implausible values","text":"crucial step data screening checking implausible values. implausible depends data collected! summary() can also help looking minimum maximum values.Run summary(messy) look minimum maximum values variable.Run summary(messy) look minimum maximum values variable.min max values age look plausible? YesNoDo min max values age look plausible? YesNoDo min max values pre look plausible? YesNoDo min max values pre look plausible? YesNoDo min max values post look plausible? YesNoDo min max values post look plausible? YesNoDo min max values delay look plausible? YesNoDo min max values delay look plausible? YesNoThe maximum value age 470, unlikely correct!maximum value pre, post, delay 30, described start chapter. However, post, maximum value 33 something wrong. important check data, just raw data calculated total score.","code":""},{"path":"screening-data.html","id":"screening-a7","chapter":"12 Screening Data","heading":"12.9 Activity 7: Visualising implausible values","text":"Whilst summary() can useful, another key step visualise data check implausible values.depend data, preferences. produce violin-boxplots data points top check distributions\nFigure 7.5: Data screening plots\nalso use histograms:\nFigure 12.1: Histograms data screening\n\nFigure 12.2: Histograms data screening\nWhatever method choose, make sure look data trying work know advance range values take (example, Likert scale 1-7, score 8, reaction times, 50ms unlikely reflect real response).","code":"\nmessy %>%\n  pivot_longer(cols = c(\"pre\", \"post\", \"delay\"), \n               names_to = \"test\", \n               values_to = \"score\") %>%\n  ggplot(aes(x = test, y = score)) +\n  geom_violin() +\n  geom_boxplot() +\n  geom_jitter(width = .2)\nggplot(messy, aes(x = age)) +\n  geom_histogram()\nmessy %>%\n  pivot_longer(cols = c(\"pre\", \"post\", \"delay\"), \n               names_to = \"test\", \n               values_to = \"score\") %>%\n  ggplot(aes(x = score)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~test)"},{"path":"screening-data.html","id":"dealing-with-implausible-values-or-missing-data","chapter":"12 Screening Data","heading":"12.10 Dealing with implausible values or missing data","text":"remove implausible values can use replace mutate.age, know one specific data point implausible, age 470 can specify just replace one value NA.post, multiple missing values specify replace data point maximum plausible value (30) NA.hard fast rule missing data. review missing data see patterns, example, missing data one condition? single participant lot missing data removed.One method dealing implausible data impute data, .e., replace missing data substituted values. many methods , example, can replace missing values mean. go method choose chapter information available online various options interested. code imputing missing data relatively simple uses mutate() replace_na().can read code \"create new variable named post_impute replaces values post NA mean values post.","code":"\nmessy_screen <-  messy %>% \n  mutate(age = replace(age, age == 470, NA),\n         post = replace(post, post > 30, NA))\nmessy_impute <- messy_screen %>%\n  mutate(post_impute = replace_na(post, \n                                  mean(post, na.rm = TRUE)))"},{"path":"screening-data.html","id":"alternative-descriptive-statistics","chapter":"12 Screening Data","heading":"12.11 Alternative descriptive statistics","text":"far book, calculated descriptive statistics using summarise() tidyverse. good reason done - output summarise() works well ggplot() code flexible. However, options producing descriptive statistics helpful know .psych package contains many functions useful psychology research. One functions psych describe().Run codedescribe() produces full set descriptive statistics, including skew, kurtosis standard error entire dataset! Run ?describe see full explanation statistics calculates.may noticed ran code received number error messages. describe() know deal data id numbers letters.Additionally, see id, speaker gender star next name. star signifies variables factors, really appropriate calculate statistics, asked apply describe entire dataset done asked.describe() can used conjunction select() remove variables.variant describe() describeBy works much like using summarise() group_by() together.look environment see descriptives3 saved List 3. means table descriptives gender saved separate table, one female, one male, one non-binary. get access individually, need use object$variable notation.output describe() little harder work terms manipulating table using data subsequent plots analyses, still strongly recommend use summarise() group_by() operations, however, getting comprehensive overview data, describe() good function know .","code":"\ndescriptives <- describe(messy)\ndescriptives\ndescriptives2 <- messy %>%\n  select(-id, -speaker, -gender) %>%\n  describe()\n\ndescriptives2\ndescriptives3 <- messy %>%\n  select(-id, -speaker) %>%\n  describeBy(group = \"gender\")\n\ndescriptives3## \n##  Descriptive statistics by group \n## gender: female\n##         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis\n## age        1 84 38.10 48.64     31   32.96 11.86  18 470   452  8.32    71.04\n## gender*    2 84  1.00  0.00      1    1.00  0.00   1   1     0   NaN      NaN\n## pre        3 84 10.38  5.06      9   10.28  4.45  -1  23    24  0.24    -0.42\n## post       4 84 18.20  6.99     17   17.96  5.93   3  36    33  0.37    -0.08\n## delay      5 78 13.18  5.17     13   13.33  5.93  -3  24    27 -0.31     0.15\n##           se\n## age     5.31\n## gender* 0.00\n## pre     0.55\n## post    0.76\n## delay   0.59\n## ------------------------------------------------------------ \n## gender: male\n##         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis\n## age        1 68 34.96 10.03     35   35.09 14.83  18  50    32 -0.05    -1.30\n## gender*    2 68  2.00  0.00      2    2.00  0.00   2   2     0   NaN      NaN\n## pre        3 68 10.04  4.85     11   10.11  4.45  -5  26    31 -0.05     1.33\n## post       4 68 16.28  5.41     16   16.23  5.19   4  33    29  0.25     0.46\n## delay      5 59 14.02  5.07     14   14.04  4.45   1  29    28  0.04     0.62\n##           se\n## age     1.22\n## gender* 0.00\n## pre     0.59\n## post    0.66\n## delay   0.66\n## ------------------------------------------------------------ \n## gender: nonbinary\n##         vars  n  mean   sd median trimmed   mad min max range  skew kurtosis\n## age        1 28 34.96 9.25   35.5   34.96 11.86  20  50    30 -0.03    -1.30\n## gender*    2 28  3.00 0.00    3.0    3.00  0.00   3   3     0   NaN      NaN\n## pre        3 28  9.29 5.36   10.0    9.54  4.45  -4  19    23 -0.48    -0.18\n## post       4 28 16.86 5.10   16.0   16.79  4.45   8  26    18  0.35    -0.85\n## delay      5 25 12.84 4.67   13.0   12.90  4.45   3  25    22  0.07     0.43\n##           se\n## age     1.75\n## gender* 0.00\n## pre     1.01\n## post    0.96\n## delay   0.93\ndescriptives3$male\ndescriptives3$female\ndescriptives3$nonbinary"},{"path":"screening-data.html","id":"screening-fin","chapter":"12 Screening Data","heading":"12.12 Finished!","text":"done! comprehensive tutorial every type dataset come across concept tidy data take practice hopefully give good starting point real, messy data.","code":""},{"path":"screening-data.html","id":"screening-sols","chapter":"12 Screening Data","heading":"12.13 Activity solutions","text":"","code":""},{"path":"screening-data.html","id":"screening-a1sol","chapter":"12 Screening Data","heading":"12.13.1 Activity 1","text":"click tab see solution\n","code":"\nlibrary(\"tidyverse\")\nlibrary(\"psych\")\nmessy <- read_csv(\"messy.csv\")"},{"path":"visualisation.html","id":"visualisation","chapter":"13 Visualisation","heading":"13 Visualisation","text":"chapter going focus visualising data using ggplot2. already created number different plots including bar charts, scatterplots, histograms, qq-plots, violin-boxplots, now show customise plots give better idea range flexibility visualising data R.chapter, asked write code , give example code. Instead, play arguments, change TRUE FALSE vice-versa, change values colours. help learn bit .activities chapter going use data Experiment 3 Zhang, T., Kim, T., Brooks, . W., Gino, F., & Norton, M. . (2014). \"present\" future: unexpected value rediscovery. Psychological Science, 25, 1851-1860..help understand data visualising, abstract:Although documenting everyday activities may seem trivial, four studies reveal creating records present generates unexpected benefits allowing future rediscoveries. Study 1, used time-capsule paradigm show individuals underestimate extent rediscovering experiences past curiosity provoking interesting future. Studies 2 3, found people particularly likely underestimate pleasure rediscovering ordinary, mundane experiences, opposed extraordinary experiences. Finally, Study 4 demonstrates underestimating pleasure rediscovery leads time-inconsistent choices: Individuals forgo opportunities document present prefer rediscovering moments future engaging alternative fun activity. Underestimating value rediscovery linked people’s erroneous faith memory everyday events. documenting present, people provide opportunity rediscover mundane moments may otherwise forgotten.","code":""},{"path":"visualisation.html","id":"viz-a1","chapter":"13 Visualisation","heading":"13.1 Activity 1: Set-up Visualisation","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Visualisation\".Download Zhang et al. 2014 Study 3.csv save chapter folder. Make sure change file name .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads package tidyverse using library() function.Run code load wrangle data tidy data.","code":"\nlibrary(tidyverse)\nzhang_data <- read_csv(\"Zhang et al. 2014 Study 3.csv\")%>%\n  select(Gender, Age,Condition, T1_Predicted_Interest_Composite, T2_Actual_Interest_Composite)%>%\n  mutate(subject = row_number())%>%\n  pivot_longer(names_to = \"time\",values_to = \"interest\",\n               cols = T1_Predicted_Interest_Composite:T2_Actual_Interest_Composite)%>%\n  mutate(Condition = recode(Condition, \"1\" = \"Ordinary\", \"2\" = \"Extraordinary\"))%>%\n  mutate(time = recode(time, \"T1_Predicted_Interest_Composite\" = \"time1_interest\", \"T2_Actual_Interest_Composite\" = \"time2_interest\"),\n         Gender = recode(Gender, \"1\" = \"male\", \"2\" = \"female\")) %>%\n  filter(Gender %in% c(\"male\", \"female\"))"},{"path":"visualisation.html","id":"viz-a2","chapter":"13 Visualisation","heading":"13.2 Activity 2: Histograms","text":"First, create histograms interest check distribution.\nfirst line code creates ggplot() object specifies dataset used, represented x y-axis. histogram, need specify variable x-axis y always frequency","code":""},{"path":"visualisation.html","id":"basic-histogram","chapter":"13 Visualisation","heading":"13.2.1 Basic histogram","text":"code create simple histogram default appearance customisation. use graph paper, just want quickly check distributions, e.g., normality, code might enough.\nFigure 13.1: Basic histogram\n","code":"\nggplot(zhang_data, aes(interest))+ \n  geom_histogram()"},{"path":"visualisation.html","id":"colour-and-fill","chapter":"13 Visualisation","heading":"13.2.2 Colour and fill","text":"next section code change appearance. Plots ggplot2 highly customisable - R Data Science excellent chapter ggplot like additional information.Adding binwidth geom_histogram() changes bins histogram, .e., wide bars . default 30. Sometimes may appropriate often want change binwidth. value give depend upon data.colour() changes colour line around bars. fill() changes fill bars.\nFigure 13.2: Histogram colour changes\n","code":"\nggplot(zhang_data, aes(x = interest))+ \n  geom_histogram(binwidth = .3, \n                 colour = \"black\",  \n                 fill = \"grey\") "},{"path":"visualisation.html","id":"axis-labels","chapter":"13 Visualisation","heading":"13.2.3 Axis labels","text":"next section code changes labels graphs. Note labels additional layer (.e., comes +, rather argument geom_histogram()).function use depend data, common scale_x/y_continuous scale_x/y_discrete depending whether displaying continuous categorical data. , axis separate layer.scale functions control information axis, label breaks, minimum maximum values. information use help documentation.labelling purposes, two main arguments:name() controls main name axislabels() controls name breaksFor histogram just change main axis labels.\nFigure 13.3: Histogram label changes\n","code":"\nggplot(zhang_data, aes(x = interest))+ \n  geom_histogram(binwidth = .3, \n                 colour = \"black\",  \n                 fill = \"grey\") + \n  scale_x_continuous(name = \"Mean interest score (1-7)\") +\n  scale_y_continuous(name = \"Count\") "},{"path":"visualisation.html","id":"density-curve","chapter":"13 Visualisation","heading":"13.2.4 Density curve","text":"following section adds normal density curve histogram, can useful checking assumption normality.add line must change geom_histogram() use density y-axis (default count) add stat_function() layer draws line.\nFigure 13.4: Histogram normal density curve\n","code":"\nggplot(zhang_data, aes(interest))+ \n  geom_histogram(binwidth = .3, \n                 colour = \"black\", \n                 fill = \"grey\",\n                 aes(y = ..density..))+ # change y-axis to density\n  scale_x_continuous(name = \"Mean interest score (1-7)\") +\n  scale_y_continuous(name = \"Count\") +\n  stat_function(fun = dnorm, # this adds a normal density function curve\n                colour = \"red\", # this makes it red\n                args = list(mean = mean(zhang_data$interest, na.rm = TRUE),\n                           sd = sd(zhang_data$interest, na.rm = TRUE)))"},{"path":"visualisation.html","id":"viz-a3","chapter":"13 Visualisation","heading":"13.3 Activity 3: Scatterplots","text":"","code":""},{"path":"visualisation.html","id":"basic-scatterplot","chapter":"13 Visualisation","heading":"13.3.1 Basic scatterplot","text":"Now make scatterplot plotting Age interest see relationship two. need specify x y-axis variables. following code produce simple scatterplot. , use graph paper, eye-balling data suffice.\nFigure 13.5: Basic scatterplot\n","code":"\nggplot(zhang_data, aes(x = interest,y = Age))+\n       geom_point()"},{"path":"visualisation.html","id":"axis-labels-1","chapter":"13 Visualisation","heading":"13.3.2 Axis labels","text":"plot look like much relationship age interest ratings. can now change labels using scale functions .\nFigure 13.6: Scatterplot label changes\n","code":"\nggplot(zhang_data, aes(x = interest,y = Age))+\n       geom_point()+\n  scale_x_continuous(name = \"Mean interest score (1-7)\") + \n  scale_y_continuous(name = \"Age\")"},{"path":"visualisation.html","id":"adding-a-regression-line","chapter":"13 Visualisation","heading":"13.3.3 Adding a regression line","text":"often useful add regression line line best fit scatterplot. regression line added geom_smooth() default also provide 95% confidence interval. can specify type line want draw, often need method = lm, .e., linear model straight line. Look help documentation geom_smooth() see methods can use.\nFigure 13.7: Scatterplot regression line\n","code":"\nggplot(zhang_data, aes(x = interest,y = Age))+\n  geom_point()+\n  scale_x_continuous(name = \"Mean interest score (1-7)\") + \n  scale_y_continuous(name = \"Age\")+\n  geom_smooth(method=lm) # if you don't want the shaded CI, add se = FALSE to this"},{"path":"visualisation.html","id":"grouped-scatterplots","chapter":"13 Visualisation","heading":"13.3.4 Grouped scatterplots","text":"can use ggplot show relationship might differ different populations within data. adding colour() aes() setting whatever variable like distinguish . case, see relationship age interest differs male female participants. participants missing gender first filter .\nFigure 13.8: Grouped scatterplot\nplot labels tidied . Notice use scale_color_discrete() adjust labels Gender.\nchange labels, R simply overwrite names dataset. wanted actually change order categories (e.g., male red line) need change order factor. later, now, just sure changing name right category (.e., female comes first))\n\nFigure 13.9: Grouped scatterplot adjusted labels\n","code":"\nzhang_data %>%\n  filter(Gender %in% c(\"male\", \"female\")) %>%\n           ggplot(aes(x = interest,y = Age, colour = Gender))+\n  geom_point()+\n  scale_x_continuous(name = \"Mean interest score (1-7)\") + \n  scale_y_continuous(name = \"Age\")+\n  geom_smooth(method=lm)\nggplot(zhang_data, aes(x = interest,y = Age, colour = Gender))+\n  geom_point()+\n  scale_x_continuous(name = \"Mean interest score (1-7)\") + \n  scale_y_continuous(name = \"Age\")+\n  geom_smooth(method=lm)+\n  scale_color_discrete(name = \"Gender\",\n                       labels = c(\"Female\", \"Male\"))"},{"path":"visualisation.html","id":"viz-a4","chapter":"13 Visualisation","heading":"13.4 Activity 4: Boxplots","text":"","code":""},{"path":"visualisation.html","id":"basic-boxplot","chapter":"13 Visualisation","heading":"13.4.1 Basic boxplot","text":"following code produce simple boxplot eye-balling data.\nFigure 13.10: Basic boxplot\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_boxplot()"},{"path":"visualisation.html","id":"adding-data-points","chapter":"13 Visualisation","heading":"13.4.2 Adding data points","text":"add another layer geom_point() can add raw data points boxplots make informative.\nFigure 13.11: Boxplot overplotting\nHowever, plot suffers -plotting, , multiple data points top . can change using geom_jitter(), adds layer points jittered one visible.height width affect much point jittered. Play around values see affects data points.\nFigure 13.12: Boxplot jittered data\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_boxplot()+\n  geom_point()\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_boxplot()+\n  geom_jitter(height = 0, width = .1)"},{"path":"visualisation.html","id":"adding-colour","chapter":"13 Visualisation","heading":"13.4.3 Adding colour","text":"may want add colour graph (consistency, sort labels). adding 'fill' argument ggplot aesthetic specifying variable colour fill organised .\nFigure 13.13: Boxplot colour\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = Condition))+\n  geom_boxplot()+\n  geom_jitter(height = 0, width = .1)+\n  scale_x_discrete(name = \"Condition\") + # note the x-axis is discrete\n  scale_y_continuous(name = \"Mean interest rating (1-7)\")+\n  scale_fill_discrete(guide = FALSE) # this suppresses the legend because we don't need it"},{"path":"visualisation.html","id":"boxplots-for-multiple-factors","chapter":"13 Visualisation","heading":"13.4.4 Boxplots for multiple factors","text":"one IV, using fill command change colour little redundant, colours add additional information. makes sense use colour represent additional IV.example, use Condition time IVs. fill() now specifies second IV, rather repeating IV x-axis previous plot.multiple IVs command overlay raw data points changes data points also need dodged (try running code previous geom_jitter function see happens)\nFigure 13.14: Boxplot two factors\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = time))+\n  geom_boxplot()+\n  geom_point(position=position_jitterdodge(jitter.width = .1))"},{"path":"visualisation.html","id":"colour-blind-friendly-options","chapter":"13 Visualisation","heading":"13.4.5 Colour-blind friendly options","text":"one fill option can use. Rather specifying scale_fill_discrete(), can use scale_fill_viridis_d(). function exactly thing uses colour-blind friendly palette (also prints black white). 5 different options colours can see changing option , B, C, D E. Personally like option E alpha = .6 (control transparency) official School position.\nFigure 7.3: Boxplots friendly colours\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = time))+\n  geom_boxplot(alpha = .6)+\n  geom_point(position=position_jitterdodge(jitter.width = .1)) +\n  scale_fill_viridis_d(option = \"E\")"},{"path":"visualisation.html","id":"viz-a5","chapter":"13 Visualisation","heading":"13.5 Activity 5: Reordering factors","text":"R orders categorical variables alphabetically. gender really matter whether male female represented first time 1 2 makes sense order may want change order Condition (mind makes sense Ordinary come first, may just ).can use mutate() fct_level() change factor levels order want.Now can re-run boxplot. better.\nFigure 13.15: Boxplot reordered factors\n","code":"\nzhang_data <- zhang_data %>%\n  mutate(Condition = fct_relevel(Condition, c(\"Ordinary\", \"Extraordinary\")))\nggplot(zhang_data, aes(x = Condition, y = interest, fill = time))+\n  geom_boxplot(alpha = .6)+\n  geom_point(position=position_jitterdodge(jitter.width = .1)) +\n  scale_fill_viridis_d(option = \"E\")"},{"path":"visualisation.html","id":"viz-a6","chapter":"13 Visualisation","heading":"13.6 Activity 6: Bar Charts","text":"","code":""},{"path":"visualisation.html","id":"basic-bar-chart","chapter":"13 Visualisation","heading":"13.6.1 Basic bar chart","text":"Bar charts used counts can distort understanding data use represent means (see great example.First, bar chart count male females sample.\nFigure 13.16: Basic bar chart\n","code":"\nggplot(zhang_data, aes(x=Gender))+\n  geom_bar()"},{"path":"visualisation.html","id":"bar-charts-with-two-factors","chapter":"13 Visualisation","heading":"13.6.2 Bar charts with two factors","text":"can also use fill() separate gender Condition\nFigure 13.17: Bar chart two factors\n","code":"\nggplot(zhang_data, aes(x=Gender, fill = Condition))+\n  geom_bar(position = \"dodge\", alpha = .6) + # the position argument places the bars next to each other, rather than on top of each other, try removing this\n  scale_fill_viridis_d(option = \"E\")"},{"path":"visualisation.html","id":"viz-a7","chapter":"13 Visualisation","heading":"13.7 Activity 7: Violin plots","text":"","code":""},{"path":"visualisation.html","id":"basic-violin-plot","chapter":"13 Visualisation","heading":"13.7.1 Basic violin plot","text":"Violin plots -called normal distribution shape look something like violin. show density, .e., fatter violin data points value.\nFigure 13.18: Basic violin plot\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_violin()"},{"path":"visualisation.html","id":"violin-plots-with-raw-data-points","chapter":"13 Visualisation","heading":"13.7.2 Violin plots with raw data points","text":"Like boxplot, can also add raw data points violin plot, making sure use jitter avoid -plotting.\nFigure 13.19: Violin plot data points\n\nimportant remember R literal. ggplot2 works system layers. add new geoms top existing ones stop think whether good idea. Try running code put geom_jitter() first add geom_violin(). order layers matters.\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_violin()+\n  geom_jitter(height = 0, width = .1)"},{"path":"visualisation.html","id":"viz-a8","chapter":"13 Visualisation","heading":"13.8 Activity 8: Violin-boxplots","text":"One increasingly common graph violin + boxplot + summary plot shows huge amount information data single plot.code uses two calls stat_summary() introduced t-test chapter. first draws point represent mean, second draws errorbar represents standard error (mean_se).guides new function can used adjust whether legends displayed. effect specifying show.legend = FALSE geom_violin() geom_boxplot() uses less code .fatten = NULL removes median line boxplots. can useful running test comparing means makes easier see point range.may get warning messages telling R removed rows containing missing values, need worry .\nFigure 13.20: Violin-boxplot summary data\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = Condition))+\n  geom_violin(alpha = .6, trim = FALSE)+\n  geom_boxplot(width = .2, alpha = .7, fatten = NULL)+\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_fill_viridis_d(option = \"E\", label = c(\"Ordinary\", \"Extraordinary\"))+\n  scale_y_continuous(name = \"Mean interest rating (1-7)\") +\n  guides(fill = FALSE)"},{"path":"visualisation.html","id":"viz-a9","chapter":"13 Visualisation","heading":"13.9 Activity 9: Faceting","text":"ggplot2 contains facet function produces different plots level grouping variable can useful two factors, example, three-way ANOVA. following code displays produces violin-boxplots Condition ~ interest, separately male female participants.code adds extra argument position = position_dodge(.9) align layers violin plots. Try removing argument layer see happens, also try adjusting value .9 another number.\nFigure 13.21: Violin-boxplot facetted gender\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = time))+\n  geom_violin(alpha = .6, trim = FALSE)+\n  geom_boxplot(width = .2, \n               alpha = .6, \n               fatten = NULL,\n               position = position_dodge(.9))+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(.9)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(.9))+\n  scale_fill_viridis_d(option = \"E\") +\n  facet_wrap(~Gender)"},{"path":"visualisation.html","id":"facet-labelling","chapter":"13 Visualisation","heading":"13.9.1 Facet labelling","text":"Finally, changing labels within facets little complicated - additional scale layer, instead, adjust inside facet_wrap() using labeller. always felt unintuitive look every single time worry confusing - just remember look example.\nFigure 13.22: Facetted plot updated labels\n","code":"\nggplot(zhang_data, aes(x = Condition, y = interest, fill = time))+\n  geom_violin(alpha = .6, trim = FALSE)+\n  geom_boxplot(width = .2, \n               alpha = .6, \n               fatten = NULL,\n               position = position_dodge(.9))+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(.9)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(.9))+\n  scale_fill_viridis_d(option = \"E\") +\n  facet_wrap(~Gender, labeller = labeller(Gender = (c(female = \"Female\", male = \"Male\"))))"},{"path":"visualisation.html","id":"viz-a10","chapter":"13 Visualisation","heading":"13.10 Activity 10: Split-violins and raincloud plots","text":"Finally, going something bit snazzy. well functions included packages, anyone can also write custom functions share code. One custom function allows us create raincloud plots highly informative pretty. See information creation function (cite use publication report).order use custom function code need install plyr package, although crucially, load like normally using library(). custom function code just use one specific function, load entire package risk creating function conflict.","code":"\ninstall.packages(\"plyr\")"},{"path":"visualisation.html","id":"split-violin-plots","chapter":"13 Visualisation","heading":"13.10.1 Split-violin plots","text":"functions need exist package can load, need create . Copy paste code without changing anything. need understand code. certainly . run , see geom_split_violin appear Environment pane Functions.split-violin version violin-boxplot good visualising interactions. look faceted graph made, actually quite lot unnecessary space used need half violin see distribution - half just repeating information.\nFigure 13.23: Split-violin plot\n","code":"\nGeomSplitViolin <- ggproto(\n  \"GeomSplitViolin\", \n  GeomViolin, \n  draw_group = function(self, data, ..., draw_quantiles = NULL) {\n    data <- transform(data, \n                      xminv = x - violinwidth * (x - xmin), \n                      xmaxv = x + violinwidth * (xmax - x))\n    grp <- data[1,'group']\n    newdata <- plyr::arrange(\n      transform(data, x = if(grp%%2==1) xminv else xmaxv), \n      if(grp%%2==1) y else -y\n    )\n    newdata <- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])\n    newdata[c(1,nrow(newdata)-1,nrow(newdata)), 'x'] <- round(newdata[1, 'x']) \n    if (length(draw_quantiles) > 0 & !scales::zero_range(range(data$y))) {\n      stopifnot(all(draw_quantiles >= 0), all(draw_quantiles <= 1))\n      quantiles <- ggplot2:::create_quantile_segment_frame(data, draw_quantiles)\n      aesthetics <- data[rep(1, nrow(quantiles)), setdiff(names(data), c(\"x\", \"y\")), drop = FALSE]\n      aesthetics$alpha <- rep(1, nrow(quantiles))\n      both <- cbind(quantiles, aesthetics)\n      quantile_grob <- GeomPath$draw_panel(both, ...)\n      ggplot2:::ggname(\"geom_split_violin\", \n                       grid::grobTree(GeomPolygon$draw_panel(newdata, ...), quantile_grob))\n    } else {\n      ggplot2:::ggname(\"geom_split_violin\", GeomPolygon$draw_panel(newdata, ...))\n    }\n  }\n)\n\ngeom_split_violin <- function (mapping = NULL, \n                               data = NULL, \n                               stat = \"ydensity\", \n                               position = \"identity\", ..., \n                               draw_quantiles = NULL, \n                               trim = TRUE, \n                               scale = \"area\", \n                               na.rm = FALSE, \n                               show.legend = NA, \n                               inherit.aes = TRUE) {\n  layer(data = data, \n        mapping = mapping, \n        stat = stat, \n        geom = GeomSplitViolin, \n        position = position, \n        show.legend = show.legend, \n        inherit.aes = inherit.aes, \n        params = list(trim = trim, \n                      scale = scale, \n                      draw_quantiles = draw_quantiles, \n                      na.rm = na.rm, ...)\n  )\n}\nggplot(zhang_data, aes(x = Condition, y = interest, fill = Gender))+\n  geom_split_violin(trim = FALSE, alpha = .4)+\n  geom_boxplot(width = .2, alpha = .6,\n               position = position_dodge(.25))+\n  scale_fill_viridis_d(option = \"E\") +\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(width = 0.25)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(width = 0.25))"},{"path":"visualisation.html","id":"raincloud-plots","chapter":"13 Visualisation","heading":"13.10.2 Raincloud plots","text":"second custom function geom_flat_violin. Copy paste code see appear Environment pane.plot similar split-violin, adds raw data points looks bit like raincloud result.First, run plot just one variable, Condition. , try changing arguments (adjust numbers change FALSE TRUE) see can control different aspects plot, particular, try removing coord_flip() see happens.\nFigure 13.24: Raincloud plot one factor\n","code":"\n\"%||%\" <- function(a, b) {\n  if (!is.null(a)) a else b\n}\n\ngeom_flat_violin <- function(mapping = NULL, data = NULL, stat = \"ydensity\",\n                             position = \"dodge\", trim = TRUE, scale = \"area\",\n                             show.legend = NA, inherit.aes = TRUE, ...) {\n  layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomFlatViolin,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = list(\n      trim = trim,\n      scale = scale,\n      ...\n    )\n  )\n}\n\nGeomFlatViolin <-\n  ggproto(\"Violinist\", Geom,\n          setup_data = function(data, params) {\n            data$width <- data$width %||%\n              params$width %||% (resolution(data$x, FALSE) * 0.9)\n            \n            # ymin, ymax, xmin, and xmax define the bounding rectangle for each group\n            data %>%\n              group_by(group) %>%\n              mutate(ymin = min(y),\n                     ymax = max(y),\n                     xmin = x,\n                     xmax = x + width / 2)\n            \n          },\n          \n          draw_group = function(data, panel_scales, coord) {\n            # Find the points for the line to go all the way around\n            data <- transform(data, xminv = x,\n                              xmaxv = x + violinwidth * (xmax - x))\n            \n            # Make sure it's sorted properly to draw the outline\n            newdata <- rbind(plyr::arrange(transform(data, x = xminv), y),\n                             plyr::arrange(transform(data, x = xmaxv), -y))\n            \n            # Close the polygon: set first and last point the same\n            # Needed for coord_polar and such\n            newdata <- rbind(newdata, newdata[1,])\n            \n            ggplot2:::ggname(\"geom_flat_violin\", GeomPolygon$draw_panel(newdata, panel_scales, coord))\n          },\n          \n          draw_key = draw_key_polygon,\n          \n          default_aes = aes(weight = 1, colour = \"grey20\", fill = \"white\", size = 0.5,\n                            alpha = NA, linetype = \"solid\"),\n          \n          required_aes = c(\"x\", \"y\")\n  )\nggplot(zhang_data, aes(x = Condition, y = interest))+\n  geom_flat_violin(position = position_nudge(x = .25, y = 0), \n                   trim=FALSE, alpha = 0.75) +\n  geom_jitter(aes(color = Condition), \n             width = .2, size = .5, alpha = .75, show.legend = FALSE)+\n  geom_boxplot(width = .1, alpha = 0.5, fatten = NULL)+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(width = 0.25)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(width = 0.3)) +\n  coord_flip()"},{"path":"visualisation.html","id":"raincloud-plots-with-multiple-factors","chapter":"13 Visualisation","heading":"13.10.3 Raincloud plots with multiple factors","text":"Now can run code 2 x 2 plot, adding Gender fill argument. quite complicated plot, worry struggling understand code remember, just need understand bits change.\nFigure 13.25: Raincloud plot two factors\n","code":"\nggplot(zhang_data, \n       aes(x = Condition, y = interest, fill = Gender))+\n  geom_flat_violin(position = position_nudge(x = .25, y = 0), \n                   trim=FALSE, \n                   alpha = 0.6) +\n  geom_point(position = position_jitter(width = .05, height = 0.05), \n             size = .5, \n             alpha = .7, \n             show.legend = FALSE, \n             aes(colour = Gender))+\n  geom_boxplot(width = .3, \n               alpha = 0.5, \n               position = \"dodge\")+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(width = 0.3)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(width = 0.3)) +\n  scale_fill_viridis_d(option = \"E\") +\n  scale_colour_viridis_d(option = \"E\")"},{"path":"visualisation.html","id":"viz-fin","chapter":"13 Visualisation","heading":"13.10.4 Finished!","text":"done! said throughout chapter, need remember code, just need remember possible find examples can modify.","code":""},{"path":"one-way-anova.html","id":"one-way-anova","chapter":"14 One-way ANOVA","heading":"14 One-way ANOVA","text":"","code":""},{"path":"one-way-anova.html","id":"background-intrusive-memories","chapter":"14 One-way ANOVA","heading":"14.0.1 Background: Intrusive memories","text":"lecture worked calculating ANOVA hand order gain conceptual understanding. However, run ANOVA, typically computer calculations . chapter show run one-factor factorial ANOVA using afex package post-hoc tests using package called emmeans.example using data experiment 2 James, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, . L., & Holmes, E. . (2015). Computer game play reduces intrusive memories experimental trauma via reconsolidation-update mechanisms. Psychological Science, 26, 1201-1215.abstract paper follows:Memory traumatic event becomes consolidated within hours. Intrusive memories can flash back repeatedly mind's eye cause distress. investigated whether reconsolidation - process memories become malleable recalled - can blocked using cognitive task whether approach can reduce unbidden intrusions. predicted reconsolidation reactivated visual memory experimental trauma disrupted engaging visuospatial task compete visual working memory resources. showed intrusive memories virtually abolished playing computer game Tetris following memory-reactivation task 24 hr initial exposure experimental trauma. Furthermore, memory reactivation playing Tetris required reduce subsequent intrusions (Experiment 2), consistent reconsolidation-update mechanisms. simple, non-invasive cognitive-task procedure administered emotional memory already consolidated (.e., > 24 hours exposure experimental trauma) may prevent recurrence intrusive memories emotional events.","code":""},{"path":"one-way-anova.html","id":"anova-a1","chapter":"14 One-way ANOVA","heading":"14.0.2 Activity 1: Set-up","text":"following:Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"One-way ANOVA\".Download James Holmes_Expt 2_DATA.csv save chapter folder.server, avoid number issues restarting session - click Session - Restart RIn new code chunk, type run code loads pwr, lsr, car, broom, afex, emmeans, performance tidyverse using library() function loads data object named dat using read_csv(). working machine may need install afex emmeans always install packages university machines.Add (hint: mutate) column dat called subjectthat equals row_number() act participant ID currently missing data set.","code":""},{"path":"one-way-anova.html","id":"anova-a2","chapter":"14 One-way ANOVA","heading":"14.0.3 Activity 2: Data wrangling","text":"lot columns data set need analysis names variable also long difficult work .Create new object called dat2 just three columns need - use select() select columns subject, Condition, Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_DiaryUse rename() rename Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary intrusionsSee can one pipelineHint: new_name = old_name","code":""},{"path":"one-way-anova.html","id":"anova-a3","chapter":"14 One-way ANOVA","heading":"14.0.4 Activity 3: Numbers and factors","text":"addition names variables long, levels Condition named 1,2,3,4 R think number rather category. going overwrite column Condition column recodes numbers factor. Copy paste code Markdown run .really important step. forget recode variables factors R treats numbers, lot things work. Trust us, spent lot time trying figure wrong forgot step!","code":"\ndat2 <- dat2 %>%\n  mutate(Condition = as.factor(Condition))"},{"path":"one-way-anova.html","id":"anova-a4","chapter":"14 One-way ANOVA","heading":"14.0.5 Activity 4: Create summary statistics","text":"Next want calculate descriptive statistics. really interested scores experimental group rather overall.Create object called sum_datthat contains mean, standard deviation standard error number intrusions grouped ConditionUse pipe achieve one pipelineYour table four columns, Condition, mean, sd, se.","code":"## \n## \n## * Use group_by(some_grouping_variable) %>% summarise(...)\n## * standard error = sd/sqrt(n) =  sd/sqrt(length(some_variable_name)"},{"path":"one-way-anova.html","id":"anova-a5","chapter":"14 One-way ANOVA","heading":"14.0.6 Activity 5: Visualisation","text":"Now can visualise data. original paper use bar plot, reproduce later now use better plot gives us information data.Create violin-boxplot number intrusions y-axis condition x-axis (see Visualisation chapter info).Change labels x-axis something informative (hint: scale_x_discrete(labels = c(\"label names\"))\nFigure 14.1: Number intrusions condition\ncan see plot outliers groups. information present bar plot, good idea use reproduce anyway.code shows produce bar plot presented paper. Try figure bit code plot (remember use help documentation function) see happens change values argument.\nFigure 14.2: Bar plot instrusions condition\n","code":"\nggplot(sum_dat, aes(x = Condition, y = mean, fill = Condition))+\n  stat_summary(fun = \"mean\", geom = \"bar\", show.legend = FALSE)+\n  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.25)+\n  scale_y_continuous(limits = c(0,7), \n                     breaks = c(0,1,2,3,4,5,6,7), \n                     name = \"Intrusive-Memory Frequency (Mean for the Week\")+\n  scale_x_discrete(labels = c(\"No-task control\", \"Reactivation plus Tetris\", \"Tetris only\",\n                                \"Reactivation only\"))"},{"path":"one-way-anova.html","id":"anova-a6","chapter":"14 One-way ANOVA","heading":"14.0.7 Activity 6: One-way ANOVA","text":"Now can run one-way ANOVA using aov_ez afex package save object mod. well running ANOVA, aov_ez function also conducts Levene's test homogeneity variance can test final assumption.aov_ez() likely produce messages look like errors, worry , just letting know done.Copy paste code run view results ANOVA using anova(mod).Just like t-tests correlations, can use tidy() make output easier work .Run code transform output. worry warning message, just telling know automatically rename columns keep original names.term = IVnum.Df = degrees freedom effectden.Df = degrees freedom residualsMSE = Mean-squared errorsstatistic = F-statisticges = effect sizep.value = p.valueYou refer lecture information variable means calculated.overall effect Condition significant? YesNoWhat F-statistics 2 decimal places? According rules thumb, effect size SmallMediumLarge","code":"\nmod <- aov_ez(id = \"subject\", # the column containing the subject IDs\n              dv = \"intrusions\", # the DV \n              between = \"Condition\", # the between-subject variable\n              es = \"pes\", # sets effect size to partial eta-squared\n              type = 3, # this affects how the sum of squares is calculated, set this to 3\n              include_aov = TRUE,\n              data = dat2)\n\nanova(mod)\nmod_output <- (mod$anova_table) %>% tidy()## Warning in tidy.anova(.): The following column names in ANOVA output were not\n## recognized or transformed: num.Df, den.Df, MSE, ges"},{"path":"one-way-anova.html","id":"anova-a7","chapter":"14 One-way ANOVA","heading":"14.0.8 Activity 7: Assumption checking","text":"may wondering yet checked assumptions. Well, unlike t-tests correlations, order test assumptions need use model created aov_ez(), assess point. one-way independent ANOVA, assumptions Student's t-test:DV interval ratio dataThe observations independentThe residuals normally distributedThere homogeneity variance groupsWe know 1 2 met design study. test 3, done can look QQ-plot residuals test normality Shapiro-Wilk test. residuals stored one components mod. access specify mod$aov$residuals.\nFigure 14.3: qq-plot model residuals\nthings note assumption test results. First, look p-value Shapiro-Wilk test - 4.252e-06. Whenever see e end number means R using scientific notation. Scientific notation way writing large small numbers. number e negative means number divided 10 power six. Put simply, move decimal place six places left get standard number. reporting p-values results section, use scientific notation, instead round 3 decimal places.value 4.252e-06? .00425242.52.000004252If want R round make easier read, use code save object, tidy round p.value. Just remember APA style never write \"p = 0\", instead, write \"p < .001\" (p never equal actual zero, can just , , small).second thing note qq-plot Shapiro-Wilk test clear assumption normality met. problem? Well, Field et al. (2009) say sample sizes group equal ANOVA robust violations normality homogeneity variance. also good discussion bit technical. can check many participants condition using count():Thankfully sample sizes equal OK proceed ANOVA. clear whether normality checked original paper.last assumption, can test homogeneity variance Levene's test function test_levene() afex. code simple, just need supply ANOVA model created earlier mod.results Levene's test show assumption homogeneity variance also met. paper indicate might case specifies ANOVAs assume equal variance, however, results ANOVA reported identical results correction made although post-hoc tests Welch tests (can tell degrees freedom adjusted whole numbers).Whilst might seem confusing - imagine might wondering point assumption testing given seems ignored - showing three reasons:reassure sometimes data can fail meet assumptions still ok use test. put statistical terms, many tests robust mild deviations normality unequal variance, particularly equal sample sizes.critical thinking point, remind just piece research published mean perfect always evaluate whether methods used appropriate.reinforce importance pre-registration decisions made advance, /open data code analyses can reproduced exactly avoid ambiguity exactly done. example, given equal sample sizes difference variance groups extreme, looks like still appropriate use ANOVA decisions justification decisions transparent.","code":"\nqqPlot(mod$aov$residuals)\nshapiro.test(mod$aov$residuals)## [1] 11 60\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$aov$residuals\n## W = 0.87739, p-value = 4.252e-06\nshapiro <- shapiro.test(mod$aov$residuals) %>% #run the test\n  tidy() %>% # tidy the output\n  mutate(p.value = round(p.value, digits = 3)) # overwrite the p-value with one rounded to 3 decimal places\ndat2 %>% count(Condition)\ntest_levene(mod)## Warning: Functionality has moved to the 'performance' package.\n## Calling 'performance::check_homogeneity()'.## Warning: Variances differ between groups (Levene's Test, p = 0.039)."},{"path":"one-way-anova.html","id":"anova-a8","chapter":"14 One-way ANOVA","heading":"14.0.9 Activity 8: Post-hoc tests","text":"post-hoc comparisons, mentioned, paper appears computed Welch t-tests mention multiple comparison correction. reproduce results using t.test contrasts.example, compare condition 1 (control group) condition 2 (reactivation plus tetris group) run:\nCondition four levels, can’t just specify intrustion ~ Condition t-test compares two groups wouldn’t know four compare first filter data use new function droplevels(). important remember comes R two things consider, data can see underlying structure data. code use filter() select conditions 1 2 can compare . However, change fact R \"knows\" Condition four levels - matter two levels observations , underlying structure still says four groups. droplevels() tells R remove unused levels factor. Try running code without droplevels() see happens.\nHowever, quicker better way allows apply correction multiple comparisons easily use emmeans() computes possible pairwise comparison t-tests applies correction p-value.First, use emmeans() run comparisons can pull contrasts use tidy() make easier work .Run code . conditions significantly different ? comparisons different ones reported paper now correction multiple comparisons applied?\ninquisitive amongst may noticed mod list 5 seemingly contains thing three times: anova_table, aov Anova. reasons behind differences complex go detail course (see info) simple version anova_table Anovause one method calculating results (type 3 sum squares) aov uses different method (type 1 sum squares). important purposes need use anova_table view overall results (replicate results papers) aovto run follow-tests get access residuals (lm() factorial ANOVA). always, precision attention detail key.\n","code":"\ndat2 %>%\n  filter(Condition %in% c(\"1\", \"2\")) %>%\n  droplevels() %>% \n  t.test(intrusions ~ Condition, data = .)\nmod_pairwise <-emmeans(mod, pairwise ~ Condition, adjust = \"bonferroni\")\nmod_contrasts <- mod_pairwise$contrasts %>% tidy()"},{"path":"one-way-anova.html","id":"anova-a9","chapter":"14 One-way ANOVA","heading":"14.0.10 Activity 9: Power and effect size","text":"Finally, can replicate power analysis using pwr.anova.test.basis effect size d = 1.14 Experiment 1, assumed large effect size f = 0.4. sample size 18 per condition required order ensure 80% power detect difference 5% significance level.already got effect size overall ANOVA, however, also really calculate Cohen's D using cohensD lsr pairwise comparisons. code little complicated need separately comparison, bind together add mod_contrasts - just make sure understand bits code need change run different data.\noptions data meet assumptions really appropriate continue regular one-way ANOVA? always, multiple options judgement call.\n\nrun non-parametric test, Kruskal-Wallis -subject designs Friedman test within-subject designs.\n\nnormality problem, try transforming data. Field et al. (2009) good section data transformation.\n\nuse bootstrapping, something cover course . , Field et al. (2009) covers although little complicated.\n","code":"\npwr.anova.test(k = 4, f = .4, sig.level = .05, power = .8)## \n##      Balanced one-way analysis of variance power calculation \n## \n##               k = 4\n##               n = 18.04262\n##               f = 0.4\n##       sig.level = 0.05\n##           power = 0.8\n## \n## NOTE: n is number in each group\nd_1_2 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,2)) %>% \n                   droplevels())\n\nd_1_3 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,3)) %>%\n                   droplevels()) \n\nd_1_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,4)) %>%\n                   droplevels())\n\nd_2_3 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(2,3)) %>% \n                   droplevels())\n\nd_2_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(2,4)) %>% \n                   droplevels())\n\nd_3_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(3,4)) %>%\n                   droplevels())\n\npairwise_ds <- c(d_1_2,d_1_3,d_1_4,d_2_3,d_2_4,d_3_4)\n\nmod_contrasts <- mod_contrasts %>%\n  mutate(eff_size = pairwise_ds)"},{"path":"one-way-anova.html","id":"anova-a10","chapter":"14 One-way ANOVA","heading":"14.0.11 Activity 10: Write-up","text":"code replicates write-paper, although changed Welch t-test pairwise comparisons emmeans.Second, critically, 7-day diary postintervention, significant difference groups overall intrusion frequency daily life, F(3, 68) = 3.79, p = 0.014, ηp2 = .0.14. Planned comparisons demonstrated relative -task control group, reactivation-plus-Tetris group, t(68) = 3.04, p = 0.02, d = 1, experienced significantly fewer intrusive memories; finding replicated Experiment 1. Critically, predicted reconsolidation theory, reactivation-plus-Tetris group significantly fewer intrusive memories Tetris-group, t(68) = -1.89, p = 0.38, d = 0.84, well reactivation-group, t(68) = -2.78, p = 0.04, d = 1.11. , significant differences -task control group reactivation-group, t(68) = 0.26, p = 1, -task control group Tetris-group, t(68) = 1.15, p = 1","code":"Second, and critically, for the 7-day diary postintervention, there was a significant difference between groups in overall intrusion frequency in daily life, F(`r mod_output$num.Df`, `r mod_output$den.Df`) = `r mod_output$statistic %>% round(2)`, p = `r mod_output$p.value %>% round(3)`, ηp2 = .`r mod_output$ges %>% round(2)`. Planned comparisons demonstrated that relative to the no-task control group, only those in the reactivation-plus-Tetris group, t(`r mod_contrasts$df[1]`) = `r mod_contrasts$statistic[1] %>% round(2)`, p = `r mod_contrasts$adj.p.value[1] %>% round(2)`, d = `r mod_contrasts$eff_size[1] %>% round(2)`, experienced significantly fewer intrusive memories; this finding replicated Experiment 1. The reactivation-plus-Tetris group had significantly fewer intrusive thoughts than the reactivation-only group, t(`r mod_contrasts$df[5]`) = `r mod_contrasts$statistic[5] %>% round(2)`, p = `r mod_contrasts$adj.p.value[5] %>% round(2)`, d = `r mod_contrasts$eff_size[5] %>% round(2)`. Further, there were no significant differences between the reactivation-plus-Tetris group and the Tetris-only group, t(`r mod_contrasts$df[4]`) = `r mod_contrasts$statistic[4] %>% round(2)`, p = `r mod_contrasts$adj.p.value[4] %>% round(2)`, d = `r mod_contrasts$eff_size[4] %>% round(2)`, the no-task control group and the reactivation-only group, t(`r mod_contrasts$df[3]`) = `r mod_contrasts$statistic[3] %>% round(2)`, p = `r mod_contrasts$adj.p.value[3] %>% round(2)`, or between the no-task control group and the Tetris-only group, t(`r mod_contrasts$df[2]`) = `r mod_contrasts$statistic[2] %>% round(2)`, p = `r mod_contrasts$adj.p.value[2] %>% round(2)`"},{"path":"one-way-anova.html","id":"anova-sols","chapter":"14 One-way ANOVA","heading":"14.0.12 Activity solutions","text":"line find solutions tasks. look giving tasks good try !","code":""},{"path":"one-way-anova.html","id":"anova-1sol","chapter":"14 One-way ANOVA","heading":"14.0.12.1 Activity 1","text":"** Click tab see solution **","code":"library(\"pwr\")\nlibrary(\"lsr\")\nlibrary(\"car\")\nlibrary(\"broom\")\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibr\\ry(\"performance\")\nlibrary(\"tidyverse\")\n\ndat <- read_csv(\"James Holmes_Expt 2_DATA.csv\")%>%\n  mutate(subject = row_number())"},{"path":"one-way-anova.html","id":"anova-a2sol","chapter":"14 One-way ANOVA","heading":"14.0.12.2 Activity 2","text":"** Click tab see solution **","code":"\ndat2 <- dat%>%\n  select(subject,Condition,Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)%>%\n  rename(intrusions = Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)"},{"path":"one-way-anova.html","id":"anova-a4sol","chapter":"14 One-way ANOVA","heading":"14.0.12.3 Activity 4","text":"** Click tab see solution **","code":"\nsum_dat<-dat2%>%\n  group_by(Condition)%>%\n  summarise(mean = mean(intrusions),\n            sd = sd(intrusions),\n            se = sd/sqrt(length(intrusions)))"},{"path":"one-way-anova.html","id":"anova-a5sol","chapter":"14 One-way ANOVA","heading":"14.0.12.4 Activity 5","text":"** Click tab see solution **","code":"\nggplot(dat2, aes(x = Condition, y = intrusions))+\n  geom_violin(trim = FALSE)+\n  geom_boxplot(width = .2)"},{"path":"factorial-anova.html","id":"factorial-anova","chapter":"15 Factorial ANOVA","heading":"15 Factorial ANOVA","text":"second week ANOVA going look example factorial ANOVA. learn interpreting lectures, now, just focus code.going reproduce analysis Experiment 3 Zhang, T., Kim, T., Brooks, . W., Gino, F., & Norton, M. . (2014). \"present\" future: unexpected value rediscovery. Psychological Science, 25, 1851-1860.. may remember study Chapter Visualisation chapter.experiment 2 x 2 mixed design:first IV time (time1, time2) within-subjectsThe second IV type event (ordinary vs. extraordinary) -subjects factorThe DV use interest","code":""},{"path":"factorial-anova.html","id":"factorial-a1","chapter":"15 Factorial ANOVA","heading":"15.0.1 Activity 1: Set-up","text":"Open R Studio set working directory chpter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Factorial ANOVA\".Download Zhang et al. 2014 Study 3.csv extract files Chapter 15 folder.server, avoid number issues restarting session - click Session - Restart RIf working computer, install package rcompanion. Remember install packages university computers, already installed.Type run code loads pwr, rcompanion, lsr, car, broom, afex, emmeans tidyverse using library() function.Run code load data wrangle format need. need write code make sure can understand line - good way code uses pipes (%>%) highlight run line progressively can see builds . Line--line code:Reads data fileSelect three columns needAdds column subject IDsTidies dataRecodes values Condition numeric text labelsRecodes values time easier read/write","code":"\nfactorial <- read_csv(\"Zhang et al. 2014 Study 3.csv\")%>%\n  select(Condition, T1_Predicted_Interest_Composite, T2_Actual_Interest_Composite)%>%\n  mutate(subject = row_number())%>%\n  pivot_longer(names_to = \"time\",values_to = \"interest\", cols =       c(\"T1_Predicted_Interest_Composite\",\"T2_Actual_Interest_Composite\"))%>%\n  mutate(Condition = dplyr::recode(Condition, \"1\" = \"Ordinary\", \"2\" = \"Extraordinary\"))%>%\n  mutate(time = dplyr::recode(time, \"T1_Predicted_Interest_Composite\" = \"time1_interest\",\n                       \"T2_Actual_Interest_Composite\" = \"time2_interest\")) %>%\n  mutate(Condition = as.factor(Condition))"},{"path":"factorial-anova.html","id":"factorial-a2","chapter":"15 Factorial ANOVA","heading":"15.0.2 Activity 2: Descriptive statistics","text":"Calculate descriptive statistics (mean SD) interest Condition time (hint: need group_by() two variables) store object named sum_dat_factorial. known cells means.","code":""},{"path":"factorial-anova.html","id":"factorial-a3","chapter":"15 Factorial ANOVA","heading":"15.0.3 Activity 3: Violin-boxplots","text":"going produce two kinds plots visualise data. First, produce violin-boxplots can see distribution data.Write code produces violin-boxplots scores group.\nHint 1: need add second IV first call ggplot fill argument (aes(x,y,fill)).\nHint 2: need add position = position_dodge(.9) geom_boxplot get plots align.\nHint 1: need add second IV first call ggplot fill argument (aes(x,y,fill)).Hint 2: need add position = position_dodge(.9) geom_boxplot get plots align.need replicate exact colour scheme used , see can play around settings whatever colour scheme think works best.\nFigure 15.1: Violin-boxplot condition time\n","code":""},{"path":"factorial-anova.html","id":"factorial-a4","chapter":"15 Factorial ANOVA","heading":"15.0.4 Activity 4: Interaction plots","text":"Now going produce interaction plot makes easier see IVs interacting, requires ggplot2 functions come across yet. Rather using raw data dat_factorial, use means produced sum_dat_factorial. type plot requires two geoms, one draw points, one draw lines connect .plot reproduces plot used paper.Run code play around looks changing arguments e.g., colour, line-type, theme.\nFigure 10.1: Interaction plot\n","code":"\nggplot(sum_dat_factorial, aes(x = time, y = mean, group = Condition, shape = Condition)) +\n  geom_point(size = 3) +\n  geom_line(aes(linetype = Condition))+\n  scale_x_discrete(labels = c(\"Time 1\", \"Time 2\"))+\n  theme_classic()"},{"path":"factorial-anova.html","id":"factorial-a5","chapter":"15 Factorial ANOVA","heading":"15.0.5 Activity 5: ANOVA","text":"Complete code run factorial ANOVA. Remember need specify IVs one -subjects one within-subjects. Look help documentation aov_ez find .Complete code run factorial ANOVA. Remember need specify IVs one -subjects one within-subjects. Look help documentation aov_ez find .Save ANOVA model object called mod_factorialSave ANOVA model object called mod_factorialPull anova table, can either mod_factorial$anova_table anova(mod_factorial) result. Save object named factorial_output make sure used tidy().Pull anova table, can either mod_factorial$anova_table anova(mod_factorial) result. Save object named factorial_output make sure used tidy().Look results. Remember pre-class information read p-values scientific notation.main effect condition significant? YesNoIs main effect time significant? YesNoIs two-way interaction significant? YesNo","code":"\nmod_factorial <- aov_ez(id = \"NULL\",\n               data = NULL, \n               between = \"NULL\", \n               within = \"NULL\",\n               dv = \"NULL\", \n               type = 3,\n               es = \"NULL\") \n\nfactorial_output <- NULL"},{"path":"factorial-anova.html","id":"factorial-a6","chapter":"15 Factorial ANOVA","heading":"15.0.6 Activity 6: Assumption checking","text":"assumptions factorial ANOVA one-way ANOVA.DV interval ratio dataThe observations independentThe residuals normally distributedThere homogeneity variance groupsAs , know assumption 2 met design study. Assumption 1 throws interesting issue problem ordinal data. Ordinal data kind data comes Likert scales , common psychology. problem ordinal data interval ratio data, fixed number values can take (values Likert scale) claim distance values equal (difference strongly agree agree difference agree neutral?).Technically, use ANOVA analyse ordinal data - almost everyone . people argue multiple Likert scale items averaged (case study) averaged data normally distributed, problem. minority (actually correct) argue use non-parametric methods complicated tests ordinal regression type data. Whichever route choose, understand data able justify decision.test assumption 3, extract residuals model (mod_factorial$lm$residuals), create qq-plot conduct Shapiro-Wilk test.test assumption 3, extract residuals model (mod_factorial$lm$residuals), create qq-plot conduct Shapiro-Wilk test.residuals normally distributed? YesNoNo, given sample probably acceptable proceedAre residuals normally distributed? YesNoNo, given sample probably acceptable proceedFor final assumption, can use test_levene() test homogeneity variance.Conduct Levene's test. assumption 4 met? YesNo","code":""},{"path":"factorial-anova.html","id":"factorial-a7","chapter":"15 Factorial ANOVA","heading":"15.0.7 Activity 7: Post-hoc tests","text":"interaction significant, follow post-hoc tests using emmeans() determine comparisons significant. overall interaction significant, conduct additional tests.emmeans() requires specify aov object, factors want contrast. interaction, use notation pairwise ~ IV1 | IV2 specify multiple comparison correction want apply. Finally, can use tidy() tidy output contrasts save tibble.Run code view results.Note two factors, also reverse order IVs. , get results contrasting time 1 time 2 condition. Instead, look difference ordinary extraordinary events time point.Run code look output contrast_factorial contrasts_factorial2 carefully making sure understand interpret results. find useful refer interaction plot made earlier.main effects (condition time) two levels, need post-hoc tests determine conditions differ , however, one factors three levels use emmeans() calculate contrast main effects, like one-way ANOVA.Finally, calculate effect size pairwise comparisons need individually using 'cohensD()fromlsr`.Run code add effect sizes contrasts_factorial contrasts_factorial2.","code":"\n# run the tests\nposthoc_factorial <- emmeans(mod_factorial, \n                             pairwise ~ time| Condition, \n                             adjust = \"bonferroni\")\n\n# tidy up the output of the tests\ncontrasts_factorial <- posthoc_factorial$contrasts %>%\n  tidy()\nposthoc_factorial2 <- emmeans(mod_factorial, \n                             pairwise ~ Condition| time, \n                             adjust = \"bonferroni\") \n\ncontrasts_factorial2 <- posthoc_factorial2$contrasts %>%\n  tidy()\nd_extra_t1_t2 <- cohensD(interest ~ time, \n                         data = (filter(factorial, Condition == \"Extraordinary\") %>% droplevels())) \n\nd_ord_t1_t2 <- cohensD(interest ~ time, \n                         data = (filter(factorial, Condition == \"Ordinary\") %>% droplevels())) \n\n\nCondition_ds <- c(d_extra_t1_t2, d_ord_t1_t2)\n\ncontrasts_factorial <- contrasts_factorial %>%\n  mutate(eff_size = Condition_ds)\n\nd_time1_extra_ord <- cohensD(interest ~ Condition, \n                         data = (filter(factorial, time == \"time1_interest\") %>% droplevels())) \n\nd_time2_extra_ord <- cohensD(interest ~ Condition, \n                         data = (filter(factorial, time == \"time2_interest\") %>% droplevels()))\n\n\ntime_ds <- c(d_time1_extra_ord, d_time2_extra_ord)\n\ncontrasts_factorial2 <- contrasts_factorial2 %>%\n  mutate(eff_size = time_ds)"},{"path":"factorial-anova.html","id":"factorial-a8","chapter":"15 Factorial ANOVA","heading":"15.0.8 Activity 8: Write-up","text":"p-values < .001 entered manually. way get R produce formatting overly complicated purposes. want push , look papaja package.values partial eta-squared match analysis reported paper. figured yet - know, please get touch!replaced simple effects main paper pairwise comparisons.First need calculate descriptives main effect time earlier.Copy paste white-space.conducted repeated measures ANOVA interest dependent measure found main effect time, F(1, 128) = 25.88, p < .001, ηp2 = 0.044; anticipated interest Time 1 (M = 4.2), SD = 1.12)) lower actual interest Time 2 (M = 4.69, SD = 1.19).also observed interaction time type experience, F(1, 128) = 4.445, p = 0.04, ηp2 = 0.008. Pairwise comparisons revealed ordinary events, predicted interest Time 1 (M = 4.04, SD = 1.09) lower experienced interest Time 2 (M = 4.73, SD = 1.24), t(128) = -5.05, p < .001, d = 0.59. Although predicted interest extraordinary events Time 1 (M = 4.36, SD = 1.13) lower experienced interest Time 2 (M = 4.65, SD = 1.14), t(128) = -2.12, p < .001, d = 0.25 , magnitude underestimation smaller ordinary events.","code":"\ntime_descrip <- factorial %>% \n  group_by(time) %>%\n  summarise(mean_interest = mean(interest, na.rm = TRUE),\n            sd_interest = sd(interest, na.rm = TRUE),\n            min = mean(interest) - qnorm(0.975)*sd(interest)/sqrt(n()),\n            max = mean(interest) + qnorm(0.975)*sd(interest)/sqrt(n()))We conducted the same repeated measures ANOVA with interest as the dependent measure and again found a main effect of time, F(`r factorial_output$num.Df[2]`, `r factorial_output$den.Df[2]`) = `r factorial_output$statistic[2] %>% round(2)`, p < .001, ηp2 = `r factorial_output$ges[2] %>% round(3)`; anticipated interest at Time 1 (M = `r time_descrip$mean_interest[1] %>% round(2)`), SD = `r time_descrip$sd_interest[1]%>% round(2)`)) was lower than actual interest at Time 2 (M = `r time_descrip$mean_interest[2]%>% round(2)`, SD = `r time_descrip$sd_interest[2]%>% round(2)`).We also observed an interaction between time and type of experience, F(`r factorial_output$num.Df[3]`, `r factorial_output$den.Df[3]`) = `r factorial_output$statistic[3] %>% round(3)`, p = `r factorial_output$p.value[3] %>% round(2)`, ηp2 = `r factorial_output$ges[3] %>% round(3)`. Pairwise comparisons revealed that for ordinary events, predicted interest at Time 1 (M = `r sum_dat_factorial$mean[3]%>% round(2)`, SD = `r sum_dat_factorial$sd[3]%>% round(2)`) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[4]%>% round(2)`, SD = `r sum_dat_factorial$sd[4]%>% round(2)`), t(`r contrasts_factorial$df[2]%>% round(2)`) = `r contrasts_factorial$statistic[2]%>% round(2)`, p < .001, d = `r contrasts_factorial$eff_size[2]%>% round(2)`. Although predicted interest for extraordinary events at Time 1 (M = `r sum_dat_factorial$mean[1]%>% round(2)`, SD = `r sum_dat_factorial$sd[1]%>% round(2)`) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[2]%>% round(2)`, SD = `r sum_dat_factorial$sd[2]%>% round(2)`), t(`r contrasts_factorial$df[1]%>% round(2)`) = `r contrasts_factorial$statistic[1]%>% round(2)`, p < .001, d = `r contrasts_factorial$eff_size[1]%>% round(2)` , the magnitude of underestimation was smaller than for ordinary events."},{"path":"factorial-anova.html","id":"factorial-a9","chapter":"15 Factorial ANOVA","heading":"15.0.9 Activity 9: Transforming data","text":"chapter decided violation assumption normality ok replicate results paper. happy violation extreme? One option deal normality transform data. want information consult Appendix chapter data transformation.various options can transform data going use Tukeys Ladder Powers transformation. finds power transformation makes data fit normal distribution closely possible type transformation.Run code. use mutate() add new variable data-set, interest_tukey going transformed DV. function transformTukey() rcompanion package. Setting plotit = TRUE automatically create qqPlots histograms can immediately visualise new variable.Now transformed DV can re-run ANOVA new variable.Notice changed pattern ANOVA results, p-values main effects interactions slightly different overall conclusions remain . likely violations normality quite mild large sample size, however, transformation can confident results may always case transformed ANOVA violations extreme.","code":"\nfactorial <- factorial %>%\n  mutate(interest_tukey = transformTukey(interest, plotit=TRUE))\ntukey_factorial <- aov_ez(id = \"subject\",\n               data = factorial, \n               between = \"Condition\", \n               within = \"time\",\n               dv = \"interest_tukey\", \n               type = 3)\n\ntukey_factorial"},{"path":"factorial-anova.html","id":"factorial-fin","chapter":"15 Factorial ANOVA","heading":"15.0.9.1 Finished!","text":"","code":""},{"path":"factorial-anova.html","id":"factorial-sols","chapter":"15 Factorial ANOVA","heading":"15.0.10 Activity solutions","text":"","code":""},{"path":"factorial-anova.html","id":"factorial-a1sol","chapter":"15 Factorial ANOVA","heading":"15.0.10.1 Activity 1","text":"** Click tab see solution **","code":"\nlibrary(\"pwr\")\nlibrary(\"rcompanion\")\nlibrary(\"car\")\nlibrary(\"lsr\")\nlibrary(\"broom\")\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")"},{"path":"factorial-anova.html","id":"factorial-a2sol","chapter":"15 Factorial ANOVA","heading":"15.0.10.2 Activity 2","text":"** Click tab see solution **","code":"\nsum_dat_factorial<-factorial%>%\n  group_by(Condition, time)%>%\n  summarise(mean = mean(interest, na.rm = TRUE),\n            sd = sd(interest, na.rm = TRUE)\n            )"},{"path":"factorial-anova.html","id":"factorial-a3sol","chapter":"15 Factorial ANOVA","heading":"15.0.10.3 Activity 3","text":"** Click tab see solution **","code":"\nggplot(factorial, \n       aes(x = time , y = interest, fill = Condition))+\n  geom_violin(trim = FALSE, \n              alpha = .4)+\n  geom_boxplot(position = position_dodge(.9), \n               width = .2, \n               alpha = .6)+\n  scale_x_discrete(labels = c(\"Time 1\", \"Time 2\"))+\n  scale_fill_viridis_d(option = \"E\")+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(width = 0.9)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(width = 0.9)) +\n  theme_minimal()"},{"path":"factorial-anova.html","id":"factorial-a5sol","chapter":"15 Factorial ANOVA","heading":"15.0.10.4 Activity 5","text":"** Click tab see solution **","code":"\nmod_factorial <- aov_ez(id = \"subject\",\n               data = factorial, \n               between = \"Condition\", \n               within = \"time\",\n               dv = \"interest\", \n               type = 3) \n\nfactorial_output <- anova(mod_factorial) %>% tidy()\n\n# OR\n\nfactorial_output <- mod_factorial$anova_table %>% tidy()"},{"path":"factorial-anova.html","id":"factorial-a6sol","chapter":"15 Factorial ANOVA","heading":"15.0.10.5 Activity 6","text":"** Click tab see solution **","code":"\n# normality testing\nqqPlot(mod_factorial$lm$residuals)\nshapiro.test(mod_factorial$lm$residuals)\n\n# levene's test\ntest_levene(mod_factorial)"},{"path":"regression.html","id":"regression","chapter":"16 Regression","heading":"16 Regression","text":"activity, working real data using regression explore question whether relationship statistics anxiety engagement course activities. hypothesis students anxious statistics less likely engage course-related activities. avoidance behaviour ultimately responsible lower performance students (although examining assessment scores activity).going analyse data STARS Statistics Anxiety Survey, administered students third-year statistics course Psychology University Glasgow. responses anonymised associating responses student arbitrary ID number (integer).STARS survey (Cruise, Cash, & Bolton, 1985) 51-item questionnaire, response 1 5 scale, higher numbers indicating greater anxiety.Cruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development validation instrument measure statistical anxiety. Proceedings American Statistical Association, Section Statistical Education, Las Vegas, NV.Example items STARS survey (Cruise, Cash, & Bolton, 1985)measure engagement course, use data Moodle usage analytics. course term, eight optional weekly -line sessions students attend extra support. variable n_weeks psess.csv file tells many (eight) given student attended.hypothesis greater anxiety reflected lower engagement. Answer following question.hypothesis correct positivenoa negative correlation students' mean anxiety levels n_weeks.","code":""},{"path":"regression.html","id":"regression-a1","chapter":"16 Regression","heading":"16.1 Activity 1: Setup","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Regression\".Download L3_stars.csv psess.csv save chapter folder. Make sure change file name .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, broom, see, performance, report tidyverse using library() function.Load two CSV datasets variables called stars engage using read_csv().","code":""},{"path":"regression.html","id":"regression-a2","chapter":"16 Regression","heading":"16.2 Activity 2: Tidy the data","text":"Take look datasets loaded .next thing need calculate mean anxiety score student (recall individual students identified ID variable).Recall difference wide tidy data. wide data, row represents individual case, observations case separate columns; tidy data, row represents single observation, observations grouped together cases based value variable (data, ID variable).STARS data currently widetidy format.calculate means, need use pivot_longer() restructure STARS data appropriate \"tidy\" format; .e., looks like table .Write run code tidy STARS data, store resulting table stars2.","code":""},{"path":"regression.html","id":"regression-a3","chapter":"16 Regression","heading":"16.3 Activity 3: Calculate mean anxiety for each student","text":"Now got data tidy format, use summarise() group_by() calculate mean anxiety scores (mean_anxiety) student (ID). Store resulting table variable named stars_means.","code":""},{"path":"regression.html","id":"regression-a4","chapter":"16 Regression","heading":"16.4 Activity 4: Join the datasets together","text":"order perform regression analysis, combine data stars_means engage using inner_join(). Call resulting table joined. look like :","code":""},{"path":"regression.html","id":"regression-a5","chapter":"16 Regression","heading":"16.5 Activity 5: Calculate descriptives for the variables overall","text":"also useful calculate descriptives statistics sample overall can check sample scores expecting (e.g., comparable previous studies samples?). also useful write-.Run code. Read line ensure understand calculated.","code":"\ndescriptives <- joined %>%\n  summarise(mean_anx = mean(mean_anxiety, na.rm = TRUE),\n            sd_anx = sd(mean_anxiety, na.rm = TRUE),\n            mean_weeks = mean(n_weeks, na.rm = TRUE),\n            sd_weeks = sd(n_weeks, na.rm = TRUE))"},{"path":"regression.html","id":"regression-a6","chapter":"16 Regression","heading":"16.6 Activity 6: Visualisations","text":"Now variables one place, write code reproduce exact scatterplot (using ggplot2).\nFigure 16.1: Scatteplot mean anxiety attendance\nAccording scatterplot, apparent relationshipas anxiety increases, engagement decreasesas anxiety increases, engagement increases","code":""},{"path":"regression.html","id":"regression-a7","chapter":"16 Regression","heading":"16.7 Activity 7: Run the regression","text":"lm() function Base R main function estimate Linear Model (hence function name lm). lm() uses formula syntax seen , .e., DV ~ predictor.Use lm() function predict n_weeks (DV) mean_anxiety (predictor). Store result call lm() variable mod. see results, use summary(mod).Answer following questions model. may wish refer lecture notes help answer questions.estimate y-intercept model, rounded three decimal places, three decimal places, GLM model \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\), \\(\\beta_1\\) three decimal places, unit increase anxiety, n_weeks decreases two decimal places, overall F-ratio model? overall model significant? YesNoWhat proportion variance model explain? summary table, estimate intercept.summary table, estimate mean_anxiety, .e., slope.summary table, also estimate mean_anxiety, slope much decreases just remove - sign.summary table, F-ratio noted F-statistic.overall model p.value .001428 less .05, therefore significant.variance explained determined R-squared, simply multiple 100 get percent. always use adjusted R-squared value.","code":"\nmod <- lm(n_weeks ~ mean_anxiety, joined)\nmod_summary <- summary(mod)"},{"path":"regression.html","id":"regression-a8","chapter":"16 Regression","heading":"16.8 Activity 8: Assumption checking","text":"Just like ANOVA, check assumptions run regression now check whether anything concerned . covered lecture, assumptions regression little bit involved ANOVA.outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Assumptions 1-3 nice easy. know data design study. Assumption 4 simply means spread data - example, point running regression age variable participants 20 years old. can check using scatterplot created Activity 4 can see assumption met, indeed spread scores.rest assumptions, going use functions packages see performance make life whole lot easier.First, can use check_model() produce range assumption test visualisations. Helpfully, function also provides brief explanation looking plot - functions R helpful!get error message Failed error:  ‘package called ‘qqplotr’’, install package qqplotr, need load using library(), check_model() uses background.\n(#fig:check_model)Visual assumption checks\nAssumption 5, linearity, plot suggests perfect looks pretty good.already noted, good visualise assumption checks just relying statistics can problematic, can sensitive small large sample sizes. However, can also reassuring statistical test back intuitions plot.Assumption 6, normality residuals, plot suggest residuals might normal, can check check_normality() runs Shapiro-Wilk test.result confirms residuals normally distributed, something likely exacerbated relatively small sample size. feeling confident, can see might resolve , core aims chapter conclude sample continue.multiple ways can transform data deal non-normality, can find information data transformation Appendix .First, need get sense issue dependent variable, case n_weeks. simple histogram shows DV normal distribution, instead, looks like uniform distribution.important remember assumptions regression residuals normally distributed, raw data, however, transforming DV can help.transform uniform distribution normal distribution, going use unif2norm function faux package (may need install).code uses mutate() create new variable n_weeks_transformed result transformation.notice histogram transformed variable still look amazing, remember residuals, raw data matters. re-run regression transformed data check model , things looking much better.worth saying point transformation use, whether works, can bit trial--error.homoscedasticity, plot looks mostly fine, can double check check_heteroscedasticity() result confirms data met assumption.","code":"\ncheck_model(mod)\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p = 0.008).\nggplot(joined, aes(x = n_weeks)) +\n  geom_histogram(binwidth = 1)\nlibrary(faux)\njoined <- mutate(joined, \n                 n_weeks_transformed = unif2norm(n_weeks))\n\nggplot(joined, aes(x = n_weeks_transformed)) +\n  geom_histogram()\nmod_transformed <- lm(n_weeks_transformed ~ mean_anxiety, joined)\ncheck_normality(mod_transformed)\ncheck_model(mod_transformed)## OK: residuals appear as normally distributed (p = 0.107).\ncheck_heteroscedasticity(mod)## OK: Error variance appears to be homoscedastic (p = 0.542)."},{"path":"regression.html","id":"regression-a9","chapter":"16 Regression","heading":"16.9 Activity 9: Power and effect size","text":"First can calculate minimum effect size able detect given sample size design study using pwr.f2.test(). usual, fill information set effect size argument, case f2, NULL.Based power analysis, minimum effect size able detect rounded 2 decimal places? According Cohen's guidelines, SmallMediumLarge effect.formula calculate observed f2, must manually using formula lecture.observed effect size larger minimum effect size detect? Yes, study sufficiently poweredNo, study underpowered","code":"\npwr.f2.test(u = 1, v = 35, f2 = NULL, sig.level = .05, power = .8)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"regression.html","id":"regression-a10","chapter":"16 Regression","heading":"16.10 Activity 10: Write-up","text":"two ways can use R help write-. first inline coding like done chapters, second use report package. one use entirely nice options.need manually calculate p-value inline coding extract lm() model. Run code .Now, copy paste code white-space knit document.simple linear regression performed engagement (M = 4.54, SD = 0.56) outcome variable statistics anxiety (M = 2.08, SD = 0.56) predictor variable. results regression indicated model significantly predicted course engagement (F(1, 35) = 11.99, p < .001, Adjusted R2 = 0.23, f2 = .63), accounting 23% variance. Anxiety significant positive predictor (β = -2.17, p < 0.001.\n)second option uses report. Just like t-test, output functions tend useable without editing particularly first learning write-stats can useful kind template (also see different ways reporting stats).Running report() output summary results copy past Word document.","code":"\nf <-mod_summary$fstatistic\nmod_p <- pf(f[1], f[2], f[3], lower=FALSE) A simple linear regression was performed with engagment (M = `r descriptives$mean_weeks %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the outcome variable and statistics anxiety (M = `r descriptives$mean_anx %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3]`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f2 = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Anxiety was a significant positive predictor (β = `r mod$coefficients[2] %>% round(2)`, p < `r mod_p %>% round(3)`.\n)\nreport(mod)## 'interpret_d()' is now deprecated. Please use 'interpret_cohens_d()'.\n## 'interpret_d()' is now deprecated. Please use 'interpret_cohens_d()'.## We fitted a linear model (estimated using OLS) to predict n_weeks with mean_anxiety (formula: n_weeks ~ mean_anxiety). The model explains a statistically significant and moderate proportion of variance (R2 = 0.26, F(1, 35) = 11.99, p = 0.001, adj. R2 = 0.23). The model's intercept, corresponding to mean_anxiety = 0, is at 9.06 (95% CI [6.32, 11.80], t(35) = 6.71, p < .001). Within this model:\n## \n##   - The effect of mean anxiety is statistically significant and negative (beta = -2.17, 95% CI [-3.45, -0.90], t(35) = -3.46, p = 0.001; Std. beta = -0.51, 95% CI [-0.80, -0.21])\n## \n## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."},{"path":"regression.html","id":"regression-sols","chapter":"16 Regression","heading":"16.11 Activity solutions","text":"","code":""},{"path":"regression.html","id":"regression-a1sol","chapter":"16 Regression","heading":"16.11.1 Activity 1","text":"** Click tab see solution **","code":"\nlibrary(\"pwr\")\nlibrary(\"broom\")\nlibrary(\"see\")\nlibrary(\"performance\")\nlibrary(\"report\")\nlibrary(\"tidyverse\")\n\nstars <- read_csv(\"L3_stars.csv\")\nengage <- read_csv(\"psess.csv\")"},{"path":"regression.html","id":"regression-a2sol","chapter":"16 Regression","heading":"16.11.2 Activity 2","text":"** Click tab see solution **","code":"\nstars2 <- pivot_longer(data = stars, names_to = \"Question\", values_to = \"Score\",cols = Q01:Q51) %>%\n  arrange(ID)"},{"path":"regression.html","id":"regression-a3sol","chapter":"16 Regression","heading":"16.11.3 Activity 3","text":"** Click tab see solution **","code":"\nstars_means <- stars2 %>%\n  group_by(ID) %>%\n  summarise(mean_anxiety = mean(Score, na.rm = TRUE),\n            min = min(Score), \n            max = min(Score),\n            sd = sd(Score))"},{"path":"regression.html","id":"regression-a4sol","chapter":"16 Regression","heading":"16.11.4 Activity 4","text":"** Click tab see solution **","code":"\njoined <- inner_join(stars_means, engage, \"ID\")"},{"path":"regression.html","id":"regression-a5sol","chapter":"16 Regression","heading":"16.11.5 Activity 5","text":"** Click tab see solution **","code":"\nggplot(joined, aes(mean_anxiety, n_weeks)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"17 Multiple regression","heading":"17 Multiple regression","text":"currently much debate (hype) surrounding smartphones effects well-, especially regard children teenagers. looking data recent study English adolescents:Przybylski, . & Weinstein, N. (2017). Large-Scale Test Goldilocks Hypothesis. Psychological Science, 28, 204--215.large-scale study found support \"Goldilocks\" hypothesis among adolescents: \"just right\" amount screen time, amount less amount associated lower well-. huge survey study: data contain responses 120,000 participants!Fortunately, authors made data study openly available, allows us dig deeper results. exercise, look whether relationship screen time well-modulated participants' (self-reported) gender.dependent measure used study Warwick-Edinburgh Mental Well-Scale (WEMWBS). 14-item scale 5 response categories, summed together form single score ranging 14-70.Przybylski & Weinstein's page study Open Science Framework, can find participant survey asks large number additional questions (see page 14 WEMWBS questions pages 4-5 questions screen time). Within page can also find raw data; however, purpose exercise, using local pre-processed copies data provide.Przybylski Weinstein looked multiple measures screen time, focusing smartphone use. found decrements well-started appear respondents reported one hour weekly smartphone use. question: negative association hours use well-(beyond one-hour point) differ boys girls?Note analysis, :continuous\\(^*\\) DV, well-;continuous\\(^*\\) DV, well-;continuous\\(^*\\) predictor, screen time;continuous\\(^*\\) predictor, screen time;categorical predictor, gender.categorical predictor, gender.\\(^*\\)variables quasi-continuous, inasmuch discrete values possible. However, sufficient number discrete categories can treat effectively continuous.want estimate two slopes relating screen time well-, one girls one boys, statistically compare slopes. problem seems simultaneously like situation run regression (estimate slopes) also one need t-test (compare two groups).expressive power regression allows us within single model. Bishop blog showed, independent groups t-test just special case ordinary regression single categorical predictor; ANOVA just special case regression predictors categorical. although can express ANOVA design using regression, converse true: express every regression design ANOVA. Regression allows us combination continuous categorical predictors model. inconvenience running ANOVA models regression models take care numerically code categorical predictors.","code":""},{"path":"multiple-regression.html","id":"mulregression-a1","chapter":"17 Multiple regression","heading":"17.1 Activity 1: Set-up","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Multiple Regression\".Download wellbeing.csv, participant_info.csv screen_time.csv save Chapter folder. Make sure change file names .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, see, performance, report, tidyverse using library() function.Load CSV datasets variables called pinfo, wellbeing screen using read_csv().","code":""},{"path":"multiple-regression.html","id":"mulregression-a2","chapter":"17 Multiple regression","heading":"17.2 Activity 2: Look at the data","text":"Take look resulting tibbles pinfo, wellbeing, screen. wellbeing tibble information WEMWBS questionnaire; screen information screen time use weekends (variables ending ) weekdays (variables ending wk) four types activities: using computer (variables starting Comph; Q10 survey), playing video games (variables starting Comp; Q9 survey), using smartphone (variables starting Smart; Q11 survey) watching TV (variables starting Watch; Q8 survey). want information variables, look items 8-11 pages 4-5 PDF version survey OSF website.variable corresponding gender located table named pinfowellbeingscreen variable called .variable corresponding gender located table named pinfowellbeingscreen variable called .WEMWBS data longwide format, contains observations  participants  items.WEMWBS data longwide format, contains observations  participants  items.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Run summary() three data-sets. missing data points? YesNoRun summary() three data-sets. missing data points? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a3","chapter":"17 Multiple regression","heading":"17.3 Activity 3: Compute the well-being score for each respondent","text":"WEMWBS well-score simply sum items.Write code create new table called wemwbs, two variables: Serial (participant ID), tot_wellbeing, total WEMWBS score.\"pivot\" table wide longgroup_by(); summarise(tot_wellbeing = ...)Sanity check: Verify scores fall 14-70 range. Przybylski Weinstein reported mean 47.52 standard deviation 9.55. Can reproduce values?summarise(), min(), max()Now visualise distribution tot_wellbeing histogram using ggplot2.geom_histogram()distribution well-scores symmetricnegatively skewedpositively skewed.","code":"\nggplot(wemwbs, aes(tot_wellbeing)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"multiple-regression.html","id":"mulregression-a4","chapter":"17 Multiple regression","heading":"17.4 Activity 4: Visualise the relationship","text":"take quick look relationship screen time (four different technologies) measures well-. code .Run code try explain words line code (remember, pronounce %>% \"\"). may find easier look tables produced.\nFigure 17.1: Relationship wellbeing screentime usage technology weekday\ngraph makes evident smartphone use 1 hour per day associated increasingly negative well-. Note combined tables using inner_join(), include data observations across wemwbs screen2 tables.next step, going focus smartphone/well-relationship.","code":"\nscreen_long <- screen %>%\n  pivot_longer(names_to = \"var\", values_to = \"hours\", -Serial) %>%\n  separate(var, c(\"variable\", \"day\"), \"_\")\n\nscreen2 <- screen_long %>%\n  mutate(variable = dplyr::recode(variable,\n               \"Watch\" = \"Watching TV\",\n               \"Comp\" = \"Playing Video Games\",\n               \"Comph\" = \"Using Computers\",\n               \"Smart\" = \"Using Smartphone\"),\n     day = dplyr::recode(day,\n              \"wk\" = \"Weekday\",\n              \"we\" = \"Weekend\"))\n\ndat_means <- inner_join(wemwbs, screen2, \"Serial\") %>%\n  group_by(variable, day, hours) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\nggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2)"},{"path":"multiple-regression.html","id":"mulregression-a5","chapter":"17 Multiple regression","heading":"17.5 Activity 5: Smartphone and well-being for boys and girls","text":"analysis, going collapse weekday weekend use smartphones.Create new table, smarttot, mean number hours per day smartphone use participant, averaged weekends/weekdays.need filter dataset include smartphone use technologies.also need group results participant ID (.e., serial).final data-set two variables: Serial (participant) tothours.need use data-set screen2 .filter() group_by() summarise()Next, create new tibble called smart_wb includes (filters) participants smarttot used smartphone one hour per day week, combine (join) table information wemwbs pinfo.**filter() inner_join() another inner_join()","code":""},{"path":"multiple-regression.html","id":"mulregression-a6","chapter":"17 Multiple regression","heading":"17.6 Activity 6: Mean-centering variables","text":"discussed lecture, continuous variables regression, often sensible transform mean centering. mean center predictor X simply subtracting mean (X_centered = X - mean(X)). two useful consequences:model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).categorical predictors two levels, become coded -.5 .5 (mean two values 0).Use mutate add two new variables smart_wb: tothours_c, calculated mean-centered version tothours predictor; male_c, recoded -.5 female .5 male.create male_c need use if_else(male == 1, .5, -.5) can read code \"variable male equals 1, recode .5, , recode -.5\".Finally, recode male male_c factors, R knows treat real numbers.","code":""},{"path":"multiple-regression.html","id":"mulregression-a7","chapter":"17 Multiple regression","heading":"17.7 Activity 7: Visualise the relationship","text":"Reverse-engineer plot. Calculate mean well-scores combination male tothours, create scatterplot plot includes separate regression lines gender.may find useful refer Visualisation chapter.group_by() variables summarise()colour = variable_you_want_different_colours_for\nFigure 17.2: Relationship mean wellbeing smartphone use gender\nWrite interpretation plot plain English.Girls show lower overall well-compared boys. addition, slope girls appears negative boys; one boys appears relatively flat. suggests negative association well-smartphone use stronger girls.","code":""},{"path":"multiple-regression.html","id":"mulregression-a8","chapter":"17 Multiple regression","heading":"17.8 Activity 8: Running the regression","text":"Now going see statistical support interpretation graph.data smart_wb, use lm() function calculate multiple regression model:\\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\)\\(Y_i\\) well-score participant \\(\\);\\(X_{1i}\\) mean-centered smartphone use variable participant \\(\\);\\(X_{2i}\\) gender (-.5 = female, .5 = male);\\(X_{3i}\\) interaction smartphone use gender (\\(= X_{1i} \\times X_{2i}\\))use summary() view results store object called mod_summary().R formulas look like : y ~ + b + :b :b means interactionThe interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.2 decimal places, proportion variance well-scores overall model explain? 2 decimal places, proportion variance well-scores overall model explain? p-value overall model fit < 2.2e-16. significant? YesNoThe p-value overall model fit < 2.2e-16. significant? YesNoWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boysWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boys","code":""},{"path":"multiple-regression.html","id":"mulregression-a9","chapter":"17 Multiple regression","heading":"17.9 Activity 9: Assumption checking","text":"Now time test pesky assumptions. assumptions multiple regression simple regression one additional assumption, multicollinearity, idea predictor variables highly correlated.outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Multicollinearity: predictor variables highly correlatedFrom work done far know assumptions 1 - 4 met can use functions performance package check rest, like simple linear regression chapter.One difference used check_model() previously rather just letting run tests wants, going specify tests, stop throwing error. word warning - assumptions tests take longer usual run, big dataset. first line code run assumption tests save object, calling object name display plots.\nFigure 8.1: Assumption plots\nassumption 5, linearity, already know looking scatterplot relationship linear, residual plot also confirms .assumption 6, normality residuals, residuals look good plots provides excellent example often better visualise rely statistics use check_normality() calls Shapiro-Wilk test:tells us residuals normal, despite fact plots look almost perfect. large sample sizes, deviation perfect normality can flagged non-normal.assumption 7, homoscedasticity, plot missing reference line - fun fact, took us several days lives asking help Twitter figure . reason line dataset large creates memory issue need create plot using code developers package see provided us Twitter. default code try draw confidence intervals around line causes memory issue, code removes se = FALSE.Please note datasets extra step, good example comes programming, matter long , always problem come across asking help part process.\nFigure 7.4: Adjusted homogeneity plot produce reference line\nlike normality, plot perfect pretty good another example visualisation better running statistical tests see significant result run:assumption 8, linearity, plot looks fine, also used grouped scatterplots look .Finally, assumption 9, multicollinearity, plot also indicates issues can also test statistically using check_collinearity().Essentially, function estimates much variance coefficient “inflated” linear dependence predictors, .e., predictor actually adding unique variance model, just really strongly related predictors. can read . Thankfully, VIF affected large samples like tests.various rules thumb, converge VIF 2 - 2.5 one predictor problematic.","code":"\nassumptions <- check_model(mod, check = c(\"vif\", \"qq\", \"normality\", \"linearity\", \"homogeneity\"))\n\nassumptions\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p < .001).\nggplot(assumptions$HOMOGENEITY, aes(x, y)) +\n    geom_point2() +\n    stat_smooth(\n      method = \"loess\",\n      se = FALSE,\n      formula = y ~ x,\n    ) +\n    labs(\n      title = \"Homogeneity of Variance\",\n      subtitle = \"Reference line should be flat and horizontal\",\n      y = expression(sqrt(\"|Std. residuals|\")),\n      x = \"Fitted values\"\n    ) \ncheck_homogeneity(mod)## Warning: Variances differ between groups (Bartlett Test, p = 0.000).\ncheck_collinearity(mod)"},{"path":"multiple-regression.html","id":"mulregression-a10","chapter":"17 Multiple regression","heading":"17.10 Activity 10: Power and effect size","text":"Finally, calculate power effect size usual.Using code Power Effect Size calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places Using code Power Effect Size calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places observed effect size study 2 decimal places? observed effect size study 2 decimal places? study sufficiently powered? YesNoIs study sufficiently powered? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a11","chapter":"17 Multiple regression","heading":"17.11 Activity 11: Write-up","text":"simple regression, can use inline coding report() function help write-. First, copy paste code white-space knit document. Note p-values entered manually APA p < .001 formatting.continuous predictors mean-centered deviation coding used categorical predictors. results regression indicated model significantly predicted course engagement (F(3, 7.1029^{4}) = 2450.89, p < .001, Adjusted R2 = 0.09, f2 = .63), accounting 9% variance. Total screen time significant negative predictor well-scores (β = -0.77, p < .001, gender (β = 5.14, p < .001, girls lower well-scores boys. Importantly, significant interaction screen time gender (β = 0.45, p < .001), smartphone use negatively associated well-girls boys.Now, can use report() produce automated summary. , need editing may useful aid interpretation reporting.","code":"All continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3] %>% round(2)`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f^2^ = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Total screen time was a significant negative predictor of wellbeing scores (β = `r mod$coefficients[2] %>% round(2)`, p < .001, as was gender (β = `r mod$coefficients[3] %>% round(2)`, p < .001, with girls having lower wellbeing scores than boys. Importantly, there was a significant interaction between screentime and gender (β = `r mod$coefficients[4] %>% round(2)`, p < .001), smartphone use was more negatively associated with wellbeing for girls than for boys. \nreport(mod)## 'interpret_d()' is now deprecated. Please use 'interpret_cohens_d()'.\n## 'interpret_d()' is now deprecated. Please use 'interpret_cohens_d()'.## We fitted a linear model (estimated using OLS) to predict tot_wellbeing with thours_c and male_c (formula: tot_wellbeing ~ thours_c * male_c). The model explains a statistically significant and weak proportion of variance (R2 = 0.09, F(3, 71029) = 2450.89, p < .001, adj. R2 = 0.09). The model's intercept, corresponding to thours_c = 0 and male_c = -0.5, is at 44.87 (95% CI [44.78, 44.96], t(71029) = 1001.87, p < .001). Within this model:\n## \n##   - The effect of thours c is statistically significant and negative (beta = -0.77, 95% CI [-0.82, -0.73], t(71029) = -32.96, p < .001; Std. beta = -0.15, 95% CI [-0.16, -0.15])\n##   - The effect of male c [0 5] is statistically significant and positive (beta = 5.14, 95% CI [5.00, 5.28], t(71029) = 72.25, p < .001; Std. beta = 0.54, 95% CI [0.52, 0.55])\n##   - The interaction effect of male c [0 5] on thours c is statistically significant and positive (beta = 0.45, 95% CI [0.38, 0.52], t(71029) = 12.24, p < .001; Std. beta = 0.09, 95% CI [0.08, 0.11])\n## \n## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."},{"path":"multiple-regression.html","id":"mulregression-fin","chapter":"17 Multiple regression","heading":"17.12 Finished!","text":"done! just week R component RM2! progress made truly astonishing. Even struggled R quite understood every single line code shown, capable data wrangling visualisation alone makes highly competitive psychology graduates world.Regardless whether continue quantitative methods using R, remember important critical skills learned part process. next time see dataset see data talked news, think work put getting data final format. importantly, think decisions researcher needed make along way might affected outcome.","code":""},{"path":"multiple-regression.html","id":"mulregression-sols","chapter":"17 Multiple regression","heading":"17.13 Activity solutions","text":"","code":""},{"path":"multiple-regression.html","id":"mulregression-a3sol","chapter":"17 Multiple regression","heading":"17.13.1 Activity 3","text":"","code":"\nwemwbs <- wellbeing %>%\n  pivot_longer(names_to = \"var\", values_to = \"score\", -Serial) %>%\n  group_by(Serial) %>%\n  summarise(tot_wellbeing = sum(score))\n\n# sanity check values\n\nwemwbs %>% summarise(mean = mean(tot_wellbeing),\n                     sd = sd(tot_wellbeing),\n                     min = min(tot_wellbeing), \n                     max = max(tot_wellbeing))"},{"path":"multiple-regression.html","id":"mulregression-a5sol","chapter":"17 Multiple regression","heading":"17.13.2 Activity 5","text":"","code":"\nsmarttot <- screen2 %>%\n  filter(variable == \"Using Smartphone\") %>%\n  group_by(Serial) %>%\n  summarise(tothours = mean(hours))\n\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") "},{"path":"multiple-regression.html","id":"mulregression-a6sol","chapter":"17 Multiple regression","heading":"17.13.3 Activity 6","text":"","code":"\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") %>%\n  mutate(thours_c = tothours - mean(tothours),\n         male_c = ifelse(male == 1, .5, -.5),\n         male_c = as.factor(male_c),\n         male = as.factor(male))"},{"path":"multiple-regression.html","id":"mulregression-a7sol","chapter":"17 Multiple regression","heading":"17.13.4 Activity 7","text":"","code":"\nsmart_wb_gen <- smart_wb %>%\n  group_by(tothours, male) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\nggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = male)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\")"},{"path":"multiple-regression.html","id":"mulregression-a8sol","chapter":"17 Multiple regression","heading":"17.13.5 Activity 8","text":"","code":"\nmod <- lm(tot_wellbeing ~ thours_c * male_c, smart_wb)\n# alternatively: \n# mod <- lm(tot_wellbeing ~ thours_c + male_c + thours_c:male_c, smart_wb)\n\nmod_summary <- summary(mod)"},{"path":"multiple-regression.html","id":"mulregression-a9sol","chapter":"17 Multiple regression","heading":"17.13.6 Activity 9","text":"","code":"\nqqPlot(mod$residuals)"},{"path":"multiple-regression.html","id":"mulregression-a10sol","chapter":"17 Multiple regression","heading":"17.13.7 Activity 10","text":"","code":"\npwr.f2.test(u = 3, v = 71029, f2 = NULL, sig.level = .05, power = .99)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video .","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). install R, also install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page. install R, also install RTools; use \"recommended\" version highlighted near top list.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.3 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. Foe example, Lisa prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.4 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. course require make PDFs. generate PDF reports, additionally need install tinytex (Xie, 2021) run following code:","code":"\ntinytex::install_tinytex()"},{"path":"updating-packages.html","id":"updating-packages","chapter":"B Updating packages","heading":"B Updating packages","text":"Package developers occasionally release updates packages. typically add new functions package, fix amend existing functions. aware package updates may cause previous code stop working. tend happen minor updates packages, occasionally major updates, can serious issues developer made fundamental changes code works. reason, recommend updating packages beginning academic year (semester) - assessment deadline just case!update individual package, easiest way use install.packages() function, always installs recent version package.update multiple packages, indeed packages, RStudio provides helpful tools. Click Tools - Check Package Updates. dialogue box appear can select packages wish update. aware select packages, may take time unable use R whilst process completes.\nFigure B.1: Updating packages RStudio\nOccasionally, might problem packages seemingly refuse update, , rlang vctrs cause end trouble. packages likely ever explicitly load, required beneath surface R things like knit Markdown files etc.try update existing package get error message says something like Warning install.packages : installation package ‘vctrs’ non-zero exit status perhaps Error loadNamespace(, c(lib.loc, .libPaths()), versionCheck = vI[[]]) :  namespace 'rlang' 0.4.9 loaded, >= 0.4.10 required one solution found manually uninstall package, restart R, install package new, rather trying update existing version. installr package also useful function uninstalling packages.","code":"\ninstall.packages(\"tidyverse\")\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")"},{"path":"updating-packages.html","id":"updating-r","chapter":"B Updating packages","heading":"B.1 Updating R","text":"Finally, may also wish update R . key thing aware update R, just download latest version website, lose packages. easiest way update R cause huge headache use installr package. use updateR() function, series dialogue boxes appear. fairly self-explanatory full step--step guide available use installr, important bit select \"Yes\" asked like copy packages older version R.always, issues, please ask Teams book GTA session.","code":"\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Load installr\nlibrary(installr)\n\n# Run the update function\nupdateR()"},{"path":"additional-resources.html","id":"additional-resources","chapter":"C Additional Resources","heading":"C Additional Resources","text":"\nFigure C.1: truth programming\nlike additional practice, can check UofG PsyTeachR course books.Level 1 - Intro R (overlaps Msc Conv book), data wrangling, data viz, descriptive statisticsLevel 2 - second-year undergraduate course introduces statistical concepts permutation tests,t-tests, NHST, alpha, power, effect size, sample size. Semester 2 focusses correlations general linear model.Level 3: third-year undergraduate course teaches students specify, estimate, interpret statistical models corresponding various study designs, using General Linear Models approach.MSc Data Skills: course provides overview skills needed reproducible research open science using statistical programming language R. Students learn data visualisation, data tidying wrangling, archiving, iteration functions, probability data simulations, general linear models, reproducible workflows.also highly recommend following, help practice data wrangling skills also great options enjoying R want stretch :Open Stats Lab - wonderful resource gives practice running statistical tests providing datasets published papers.R Data Science - written authors tidyverse, great resource additional data wrangling practice depth many tidyverse functions.Text Mining R - Shows use R work text. something cover course, uses data wrangling skills useful additional skill .make BBC style graphics - Ever wondered BBC News makes data visualisation? Well, now can make !Data Vizualisation - entire book data visualisation goes detail take ggplot limits.","code":""},{"path":"citing-r-rstudio.html","id":"citing-r-rstudio","chapter":"D Citing R and RStudio","heading":"D Citing R and RStudio","text":"cite R RStudioYou may way writing scientific report cite reference R, however, time comes important give people built (free!) credit. provide separate citations R, RStudio, packages use.get citation version R using, simply run citation() function always provide recent citation.generate citation packages using, can also use citation() function name package wish cite.generate citation version RStudio using, can use RStudio.Vesion() function:Finally, example might look write-method section:Analysis conducted using R (R Core Team, 2020), RStudio (Rstudio Team, 2020), tidyverse package (Wickham, 2017).noted, may , come back important give open-source community credit work.","code":"\ncitation()## \n## To cite R in publications use:\n## \n##   R Core Team (2021). R: A language and environment for statistical\n##   computing. R Foundation for Statistical Computing, Vienna, Austria.\n##   URL https://www.R-project.org/.\n## \n## A BibTeX entry for LaTeX users is\n## \n##   @Manual{,\n##     title = {R: A Language and Environment for Statistical Computing},\n##     author = {{R Core Team}},\n##     organization = {R Foundation for Statistical Computing},\n##     address = {Vienna, Austria},\n##     year = {2021},\n##     url = {https://www.R-project.org/},\n##   }\n## \n## We have invested a lot of time and effort in creating R, please cite it\n## when using it for data analysis. See also 'citation(\"pkgname\")' for\n## citing R packages.\ncitation(\"tidyverse\")## \n##   Wickham et al., (2019). Welcome to the tidyverse. Journal of Open\n##   Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\n## \n## A BibTeX entry for LaTeX users is\n## \n##   @Article{,\n##     title = {Welcome to the {tidyverse}},\n##     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n##     year = {2019},\n##     journal = {Journal of Open Source Software},\n##     volume = {4},\n##     number = {43},\n##     pages = {1686},\n##     doi = {10.21105/joss.01686},\n##   }\nRStudio.Version()"},{"path":"symbols.html","id":"symbols","chapter":"E Symbols","heading":"E Symbols","text":"\nFigure E.1: Image James Chapman/Soundimals\n","code":""},{"path":"conventions.html","id":"conventions","chapter":"F Conventions","heading":"F Conventions","text":"book use following conventions:Generic code: list(number = 1, letter = \"\")Highlighted code: dplyr::slice_max()File paths: data/sales.csvR Packages: tidyverseFunctions: paste()Strings: \"psyTeachR\"Numbers: 100, 3.14Logical values: TRUE, FALSEGlossary items: ordinalCitations: Wickham (2021)Internal links: Chapter ??External links: R Data ScienceMenu/interface options: New File...Quiz question: going learn lot: TRUEFALSEYou found !Informational asides.Notes warn something.Notes things cause serious errors.Try .","code":"\n# code chunks\npaste(\"Applied\", \"Data\", \"Skills\", 1, sep = \" \")## [1] \"Applied Data Skills 1\"```{r setup, message = FALSE}\n# code chunks with visible r headers\nlibrary(tidyverse)```"},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit, provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
